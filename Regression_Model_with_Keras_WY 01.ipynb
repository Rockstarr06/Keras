{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the neccessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics as stats\n",
    "import os\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime\n",
    "\n",
    "# Forcing keras to use CPU.\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n",
      "0   540.0                 0.0      0.0  162.0               2.5   \n",
      "1   540.0                 0.0      0.0  162.0               2.5   \n",
      "2   332.5               142.5      0.0  228.0               0.0   \n",
      "3   332.5               142.5      0.0  228.0               0.0   \n",
      "4   198.6               132.4      0.0  192.0               0.0   \n",
      "\n",
      "   Coarse Aggregate  Fine Aggregate  Age  Strength  \n",
      "0            1040.0           676.0   28     79.99  \n",
      "1            1055.0           676.0   28     61.89  \n",
      "2             932.0           594.0  270     40.27  \n",
      "3             932.0           594.0  365     41.05  \n",
      "4             978.4           825.5  360     44.30  \n",
      "\n",
      "Shape of dataframe :  (1030, 9)\n"
     ]
    }
   ],
   "source": [
    "# Reading the Data and storing it in a dataframe\n",
    "\n",
    "path = r'C:\\concrete_data.csv' # the path to the concrete_data.csv file\n",
    "df = pd.read_csv(path) # read the data into dataframe\n",
    "\n",
    "print(df.head()) # display first 5 entries in the dataframe\n",
    "print('\\nShape of dataframe : ',df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features or the predictors (X) are :        Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n",
      "0      540.0                 0.0      0.0  162.0               2.5   \n",
      "1      540.0                 0.0      0.0  162.0               2.5   \n",
      "2      332.5               142.5      0.0  228.0               0.0   \n",
      "3      332.5               142.5      0.0  228.0               0.0   \n",
      "4      198.6               132.4      0.0  192.0               0.0   \n",
      "...      ...                 ...      ...    ...               ...   \n",
      "1025   276.4               116.0     90.3  179.6               8.9   \n",
      "1026   322.2                 0.0    115.6  196.0              10.4   \n",
      "1027   148.5               139.4    108.6  192.7               6.1   \n",
      "1028   159.1               186.7      0.0  175.6              11.3   \n",
      "1029   260.9               100.5     78.3  200.6               8.6   \n",
      "\n",
      "      Coarse Aggregate  Fine Aggregate  Age  \n",
      "0               1040.0           676.0   28  \n",
      "1               1055.0           676.0   28  \n",
      "2                932.0           594.0  270  \n",
      "3                932.0           594.0  365  \n",
      "4                978.4           825.5  360  \n",
      "...                ...             ...  ...  \n",
      "1025             870.1           768.3   28  \n",
      "1026             817.9           813.4   28  \n",
      "1027             892.4           780.0   28  \n",
      "1028             989.6           788.9   28  \n",
      "1029             864.5           761.5   28  \n",
      "\n",
      "[1030 rows x 8 columns] \n",
      "\n",
      "\n",
      "The target (Y) is :  0       79.99\n",
      "1       61.89\n",
      "2       40.27\n",
      "3       41.05\n",
      "4       44.30\n",
      "        ...  \n",
      "1025    44.28\n",
      "1026    31.18\n",
      "1027    23.70\n",
      "1028    32.77\n",
      "1029    32.40\n",
      "Name: Strength, Length: 1030, dtype: float64 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating dataframes of features (X) and target (Y)\n",
    "X = df.iloc[:, 0:8]\n",
    "Y = df.iloc[:,8]\n",
    "\n",
    "# Printing the dataframes X and Y to ensure we have created the dataframes with the correct columns\n",
    "print('The features or the predictors (X) are : ', X, '\\n\\n') \n",
    "print('The target (Y) is : ', Y, '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_model() :\n",
    "    \n",
    "    # Create the model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu', input_shape=(X.shape[1],)))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split() :\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\n",
    "    \n",
    "    # Create a list containing X_train, X_test, Y_train, Y_test and return the list\n",
    "    splits = [X_train, X_test, Y_train, Y_test] \n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict() :\n",
    "    return model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mse() :\n",
    "    return mean_squared_error(Y_test,Y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = blue> PART A : BUILDING A BASELINE MODEL </font>\n",
    "\n",
    "\n",
    "<b>The baseline model consists of the following : </b>\n",
    "    <ul>\n",
    "        <li> Input layer with 10 nodes </li>\n",
    "        <li> A single hidden layer with 10 nodes and ReLU activation function </li>\n",
    "        <li> Adam optimizer and mean squared error loss function </li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = darkorange>Task 1 : Train and Test the Baseline Model</font>\n",
    "\n",
    "In order to train and test the the baseline model, the following steps are performed :\n",
    "<ol>\n",
    "    <li>Randomly split the data into <i>X_train, X_test, Y_train, Y_test</i> sets</li>\n",
    "    <li>Train the model using <i>X_train, Y_train</i> using <b>50</b> epochs</li>\n",
    "    <li>Get the predictions on the <i>X_test</i> set </li>\n",
    "    <li>Compute the <b>mean squared error</b> on the test set using sklearn</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #2980B9> Step 1 : Randomly split the data into <i>X_train, X_test, Y_train, Y_test </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into X_train, X_test, Y_train, Y_test\n",
    "X_train, X_test, Y_train, Y_test = data_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #2980B9> Step 2 : Train the model using <i>X_train, Y_train</i> using 50 epochs </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1789.0752 - val_loss: 1000.5565\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 686.4020 - val_loss: 548.3293\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 432.1407 - val_loss: 466.4672\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 367.7163 - val_loss: 421.0092\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 332.9849 - val_loss: 390.3380\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 300.8669 - val_loss: 354.7792\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 257.6034 - val_loss: 305.5390\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.0364 - val_loss: 260.8717\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.4263 - val_loss: 236.9267\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 184.7013 - val_loss: 228.4767\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.0560 - val_loss: 222.6192\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.1468 - val_loss: 217.1437\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 171.4547 - val_loss: 211.0342\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 167.5648 - val_loss: 206.1974\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.2977 - val_loss: 202.5846\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.8441 - val_loss: 199.0047\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.7718 - val_loss: 198.0122\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.6227 - val_loss: 192.7256\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.8606 - val_loss: 190.1353\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.8847 - val_loss: 187.5159\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.8074 - val_loss: 185.7462\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.1112 - val_loss: 183.6933\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.6187 - val_loss: 181.7012\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.2673 - val_loss: 181.8573\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.4150 - val_loss: 178.5572\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.2323 - val_loss: 175.1765\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.6697 - val_loss: 173.8234\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.5792 - val_loss: 170.9779\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.7084 - val_loss: 167.6789\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.3810 - val_loss: 164.1684\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.8643 - val_loss: 163.7628\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.0013 - val_loss: 160.5824\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.7558 - val_loss: 159.3516\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.1282 - val_loss: 160.1735\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.3824 - val_loss: 155.6940\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.0061 - val_loss: 155.3021\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.0923 - val_loss: 153.6517\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.5879 - val_loss: 152.7404\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.5107 - val_loss: 157.5325\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.5597 - val_loss: 150.6056\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.8752 - val_loss: 148.8084\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.2405 - val_loss: 147.9618\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.2118 - val_loss: 151.0377\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.7706 - val_loss: 146.3653\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.3998 - val_loss: 146.9753\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.4102 - val_loss: 148.5120\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.1452 - val_loss: 146.1113\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.2275 - val_loss: 145.2150\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.0584 - val_loss: 143.1136\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.3775 - val_loss: 146.6239\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f8b2c19cf0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the model\n",
    "model = regression_model()\n",
    "\n",
    "# Fit the model on the train set\n",
    "model.fit(X_train, Y_train, validation_split=0.3, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #2980B9> Step 3 : Get the predictions on the X_test </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 665us/step\n"
     ]
    }
   ],
   "source": [
    "# Store the predictions in a variable Y_Predicted\n",
    "Y_predicted = predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note </b> : Y_test or the original values are also sometimes refered to as Y_true and this is the notation used in the examples found on the [mean square error page of sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html). However, in this notebook the Y_test notations is used for original values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #2980B9> Step 4 : Compute the <i>mean squared error</i> on the test set using sklearn </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error (MSE) of the Baseline Model is :  130.8598788433242\n"
     ]
    }
   ],
   "source": [
    "# Calculate the mean square error\n",
    "mse = calculate_mse()\n",
    "print('Mean Square Error (MSE) of the Baseline Model is : ' , mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = darkorange>Task 2 : Create 50 Models and Calculate the <i>µ</i> & <i>σ</i> of their MSE Errors</font>\n",
    "\n",
    "In order to train 50 models and calulate the mean (µ) and standard deviation (σ) of their mean square errors (MSE)  :\n",
    "<ol>\n",
    "    <li>Create an empty list <code>list_of_mse</code> to store the mean square error of each of the models</li>\n",
    "    <li>Define a for loop and perform each of the following steps in the loop</li>\n",
    "        <ol>\n",
    "        <li>Randomly split the data into <i>X_train, X_test, Y_train, Y_test</i> sets</li>\n",
    "        <li>Train the model using <i>X_train, Y_train</i> using <b>50</b> epochs</li>\n",
    "        <li>Evaluate the model using <i>X_test, Y_test</i></li>\n",
    "        <li>Get the predictions on the <i>X_test</i> set </li>\n",
    "        <li>Compute the <b>mean squared error</b> on the test set using sklearn</li>\n",
    "    </ol>\n",
    "</ol>\n",
    "\n",
    "<b>Note</b> : To calcuate the mean and standard deviation of the mean square errors (mse) of 50 models which are stored in <code>list_of_means</code>, I will be using the python library <code>statistics</code> which has builtin functions to help caluclate the mean and standard deviation of a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #2980B9> Step 1 : Creating the <i>list_of_mse</i> list</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the empty lists\n",
    "list_of_mse = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #2980B9> Step 2 : Creating 50 Models, Calculating The MSE for Each and <i>µ</i> & <i>σ</i> of the 50 MSE Values in <i>list_of_mse</i> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Training Model #  1 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 57325.3398 - val_loss: 28950.9727\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 20686.6895 - val_loss: 9186.0498\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 7565.0703 - val_loss: 3596.5237\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3605.1389 - val_loss: 1936.5596\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2234.9946 - val_loss: 1235.1655\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1573.2444 - val_loss: 844.9540\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1109.3850 - val_loss: 563.2899\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 773.1603 - val_loss: 441.4553\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 561.2615 - val_loss: 373.2502\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 440.1154 - val_loss: 325.5760\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 361.3653 - val_loss: 295.8565\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 309.9269 - val_loss: 284.7177\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 277.1777 - val_loss: 281.1161\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.7950 - val_loss: 281.0185\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 251.4449 - val_loss: 279.0914\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.1044 - val_loss: 276.8341\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.4648 - val_loss: 275.2389\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.5083 - val_loss: 272.6676\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 233.9719 - val_loss: 271.4198\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 230.2124 - val_loss: 266.2706\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 226.4416 - val_loss: 263.6532\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 223.9513 - val_loss: 260.4286\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 220.4393 - val_loss: 257.7760\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.0631 - val_loss: 254.7494\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 214.1940 - val_loss: 251.3456\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 211.5759 - val_loss: 247.8404\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 209.2968 - val_loss: 244.6146\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 205.3591 - val_loss: 241.8462\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.4737 - val_loss: 239.3327\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.6567 - val_loss: 235.5913\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 197.9759 - val_loss: 232.6098\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.3360 - val_loss: 229.7030\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 191.4410 - val_loss: 226.5846\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 188.4603 - val_loss: 223.8762\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 186.4712 - val_loss: 219.9922\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 183.3181 - val_loss: 217.1045\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.0791 - val_loss: 214.7925\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 178.5337 - val_loss: 212.0607\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.3353 - val_loss: 209.3218\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.7580 - val_loss: 206.7484\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.6333 - val_loss: 205.5026\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.5763 - val_loss: 202.0728\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 166.6795 - val_loss: 199.9677\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.4822 - val_loss: 197.6791\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.2327 - val_loss: 195.6179\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.2741 - val_loss: 194.0395\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.1877 - val_loss: 191.8231\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.4377 - val_loss: 189.8231\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.8782 - val_loss: 188.4361\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.9726 - val_loss: 185.9833\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "Mean Squared Error for Training Model #  1  :  181.61412558966873\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  2 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 625.4216 - val_loss: 543.1278\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 452.7003 - val_loss: 432.1681\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 352.0912 - val_loss: 405.7480\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 302.2485 - val_loss: 332.4082\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 259.4090 - val_loss: 293.3318\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.9501 - val_loss: 272.2611\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 215.9440 - val_loss: 254.5885\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 201.2142 - val_loss: 244.3217\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.0706 - val_loss: 236.6381\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 184.6340 - val_loss: 231.5938\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.7367 - val_loss: 227.7147\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.1069 - val_loss: 221.5976\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.6191 - val_loss: 240.6308\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.0082 - val_loss: 212.2148\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.8588 - val_loss: 215.6683\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 161.8085 - val_loss: 205.5505\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.3787 - val_loss: 197.7512\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.2183 - val_loss: 193.0747\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.9389 - val_loss: 186.8564\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.8853 - val_loss: 182.7146\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.2847 - val_loss: 175.5444\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.2660 - val_loss: 171.6012\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.5269 - val_loss: 173.0132\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.7251 - val_loss: 162.7307\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.6082 - val_loss: 159.6327\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.1392 - val_loss: 165.7579\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.4326 - val_loss: 153.7292\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.3933 - val_loss: 154.6153\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.5387 - val_loss: 154.2381\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.9031 - val_loss: 150.0377\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.4002 - val_loss: 145.4017\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.5543 - val_loss: 147.3119\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.8013 - val_loss: 145.8721\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.6944 - val_loss: 139.7469\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.6217 - val_loss: 135.4305\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.5593 - val_loss: 133.5493\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.2169 - val_loss: 131.7674\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.5001 - val_loss: 131.6985\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.6511 - val_loss: 135.8074\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.1269 - val_loss: 126.8215\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.2501 - val_loss: 125.1790\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.7877 - val_loss: 129.9327\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.9789 - val_loss: 125.2765\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.0481 - val_loss: 120.6568\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.3739 - val_loss: 122.1883\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.6128 - val_loss: 118.2020\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.0028 - val_loss: 116.6444\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.6787 - val_loss: 122.5490\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.5974 - val_loss: 114.8814\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.8449 - val_loss: 129.9314\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "Mean Squared Error for Training Model #  2  :  115.15899304489986\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  3 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 14553.6084 - val_loss: 7430.8833\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4395.3882 - val_loss: 1713.3164\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 872.2078 - val_loss: 354.0000\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 272.7941 - val_loss: 306.8101\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 263.5387 - val_loss: 299.7832\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 249.6453 - val_loss: 283.6115\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 242.8383 - val_loss: 279.8828\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.4224 - val_loss: 275.7088\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.5016 - val_loss: 272.9311\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 231.6651 - val_loss: 270.1659\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 228.3625 - val_loss: 267.5623\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 225.4600 - val_loss: 265.3258\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.4882 - val_loss: 261.9973\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 219.2394 - val_loss: 260.0859\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 216.2669 - val_loss: 258.2342\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 213.6038 - val_loss: 255.3648\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 210.7415 - val_loss: 252.8884\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.5762 - val_loss: 250.6848\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 205.5657 - val_loss: 249.3395\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.1088 - val_loss: 246.9197\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 201.6233 - val_loss: 244.0580\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.3210 - val_loss: 244.7278\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 197.2374 - val_loss: 240.3969\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.1989 - val_loss: 239.6451\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.6616 - val_loss: 237.1872\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 190.3583 - val_loss: 235.6864\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 188.8026 - val_loss: 233.8034\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 186.8648 - val_loss: 233.1096\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.9006 - val_loss: 230.5429\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 183.4301 - val_loss: 229.5880\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 182.1439 - val_loss: 228.5635\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.0921 - val_loss: 226.4857\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 178.8344 - val_loss: 225.3347\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.0815 - val_loss: 223.9714\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.1599 - val_loss: 222.9891\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.4263 - val_loss: 221.2358\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.4148 - val_loss: 221.4755\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 172.0357 - val_loss: 218.5303\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.6338 - val_loss: 217.7708\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.2777 - val_loss: 215.5518\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 167.2688 - val_loss: 214.3683\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 166.3464 - val_loss: 212.9043\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.5924 - val_loss: 211.3451\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.3158 - val_loss: 210.6224\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.4705 - val_loss: 208.3478\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.3035 - val_loss: 206.7037\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.9287 - val_loss: 205.6327\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.6426 - val_loss: 203.6902\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.8791 - val_loss: 204.5238\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.0766 - val_loss: 200.9009\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 554us/step\n",
      "Mean Squared Error for Training Model #  3  :  195.07496659187984\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  4 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 162320.0781 - val_loss: 104171.0234\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75104.1484 - val_loss: 45921.1445\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 32045.8535 - val_loss: 18457.7207\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 12365.1338 - val_loss: 6705.1030\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4488.7847 - val_loss: 2416.4077\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1861.8458 - val_loss: 1197.9012\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1151.5564 - val_loss: 946.4800\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 998.0882 - val_loss: 887.8394\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 939.4722 - val_loss: 836.4095\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 862.3121 - val_loss: 775.8663\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 796.6733 - val_loss: 735.3879\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 744.2779 - val_loss: 694.7899\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 692.4387 - val_loss: 653.3010\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 644.2088 - val_loss: 615.4186\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 596.2302 - val_loss: 575.4660\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 553.4670 - val_loss: 539.4406\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 514.4152 - val_loss: 513.3812\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 480.3573 - val_loss: 479.7477\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 447.2952 - val_loss: 462.2675\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 412.2608 - val_loss: 420.8280\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 373.9430 - val_loss: 395.3065\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 352.0124 - val_loss: 382.7184\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 331.2211 - val_loss: 361.3848\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 315.7727 - val_loss: 346.0779\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 300.7849 - val_loss: 336.6795\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 290.3535 - val_loss: 329.0482\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 280.5821 - val_loss: 321.8980\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 273.2249 - val_loss: 313.6331\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 269.9536 - val_loss: 309.6110\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.3012 - val_loss: 302.1800\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 261.4196 - val_loss: 303.5309\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 259.5078 - val_loss: 313.2919\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 246.4242 - val_loss: 293.5021\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.3675 - val_loss: 287.6026\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.2747 - val_loss: 283.2913\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.0014 - val_loss: 277.4299\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 228.3116 - val_loss: 274.1056\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 234.5258 - val_loss: 283.8680\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 220.7689 - val_loss: 271.0036\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 215.5666 - val_loss: 265.7611\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 212.7823 - val_loss: 274.7219\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 213.2350 - val_loss: 261.4774\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 205.3249 - val_loss: 261.5659\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 201.7782 - val_loss: 252.0329\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.0383 - val_loss: 263.5336\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 200.2909 - val_loss: 253.8737\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.0430 - val_loss: 243.3295\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 188.6907 - val_loss: 238.6479\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.0386 - val_loss: 240.5958\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.7050 - val_loss: 233.4930\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "Mean Squared Error for Training Model #  4  :  228.2280242103762\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  5 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 152231.8750 - val_loss: 107452.1484\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83199.7266 - val_loss: 57510.5156\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 44106.9258 - val_loss: 29901.1113\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 22564.0352 - val_loss: 14772.6904\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 10806.9082 - val_loss: 6618.8262\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4618.4155 - val_loss: 2578.5476\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1730.7322 - val_loss: 928.3851\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 665.4056 - val_loss: 429.5424\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 368.5373 - val_loss: 341.4340\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 323.7460 - val_loss: 333.7928\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 316.0020 - val_loss: 331.7051\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 311.5821 - val_loss: 327.5159\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 307.5560 - val_loss: 323.5020\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 302.9391 - val_loss: 320.4808\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 298.6816 - val_loss: 317.1271\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 294.6349 - val_loss: 313.6553\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 291.3584 - val_loss: 309.2521\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 286.8387 - val_loss: 306.3294\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 282.3134 - val_loss: 303.1316\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 279.2575 - val_loss: 299.6991\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 274.4941 - val_loss: 296.6687\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 270.2873 - val_loss: 293.1761\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 266.3495 - val_loss: 289.8509\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.6562 - val_loss: 287.1298\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 258.8407 - val_loss: 284.2767\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.0854 - val_loss: 281.4922\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 250.8368 - val_loss: 278.0645\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 247.3108 - val_loss: 275.4850\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.4518 - val_loss: 273.4912\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 240.1184 - val_loss: 271.4613\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.5729 - val_loss: 269.3293\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 232.8267 - val_loss: 265.5586\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 229.4617 - val_loss: 263.5807\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 225.9276 - val_loss: 262.1890\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 223.2850 - val_loss: 260.0102\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 219.9894 - val_loss: 257.4384\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.6324 - val_loss: 255.1258\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 213.5390 - val_loss: 254.1232\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 211.0459 - val_loss: 251.6857\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 207.7273 - val_loss: 249.0575\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 205.1432 - val_loss: 246.8235\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 202.6075 - val_loss: 243.9796\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.5851 - val_loss: 241.3305\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.5232 - val_loss: 239.7549\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.2645 - val_loss: 234.8811\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 191.3146 - val_loss: 232.8940\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.4145 - val_loss: 229.1108\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 183.5417 - val_loss: 226.6844\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.7903 - val_loss: 224.2157\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.1176 - val_loss: 220.8645\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 655us/step\n",
      "Mean Squared Error for Training Model #  5  :  182.29196903810458\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  6 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 12460.6426 - val_loss: 4742.7051\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4606.3423 - val_loss: 3987.0869\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3415.1897 - val_loss: 3347.5708\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2727.8630 - val_loss: 2646.1309\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2190.9927 - val_loss: 2154.1589\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1781.9832 - val_loss: 1808.3424\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1452.4221 - val_loss: 1460.4849\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1172.0298 - val_loss: 1228.7948\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 958.0608 - val_loss: 1016.2192\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 784.1174 - val_loss: 843.1998\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 648.4312 - val_loss: 723.4363\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 544.2695 - val_loss: 607.6251\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 463.5401 - val_loss: 526.4464\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 396.2055 - val_loss: 464.4854\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 349.0226 - val_loss: 409.7873\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 313.3746 - val_loss: 374.1905\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 284.2725 - val_loss: 343.0407\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 265.2508 - val_loss: 314.9649\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.4213 - val_loss: 295.7650\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 230.8044 - val_loss: 277.7792\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 219.5409 - val_loss: 262.7738\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.2141 - val_loss: 249.1989\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.0203 - val_loss: 240.1695\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 190.7930 - val_loss: 228.0354\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 183.3320 - val_loss: 222.1731\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 178.2509 - val_loss: 214.2037\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.3053 - val_loss: 205.0168\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.4566 - val_loss: 200.5617\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.4062 - val_loss: 192.2580\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.3161 - val_loss: 187.0932\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.9648 - val_loss: 181.7333\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.2405 - val_loss: 177.8325\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.7157 - val_loss: 172.3187\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.3555 - val_loss: 167.1347\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.2389 - val_loss: 166.0699\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.0258 - val_loss: 160.1933\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.0605 - val_loss: 156.5086\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.8078 - val_loss: 153.3102\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.3357 - val_loss: 151.7111\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.2646 - val_loss: 147.2205\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.0718 - val_loss: 145.5736\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.9421 - val_loss: 141.7120\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 112.5062 - val_loss: 140.0842\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.4402 - val_loss: 137.5998\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.6420 - val_loss: 135.0623\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.4406 - val_loss: 134.8324\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.7109 - val_loss: 131.4167\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.5339 - val_loss: 129.1544\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.0804 - val_loss: 127.6868\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.4833 - val_loss: 126.0908\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 668us/step\n",
      "Mean Squared Error for Training Model #  6  :  122.48041392129545\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  7 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 30728.2070 - val_loss: 13307.9668\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 7185.2197 - val_loss: 2713.6677\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1749.2192 - val_loss: 1004.7844\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 948.9954 - val_loss: 797.4182\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 830.3797 - val_loss: 763.4006\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 801.9859 - val_loss: 746.7821\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 782.9544 - val_loss: 729.8769\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 763.0759 - val_loss: 711.3390\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 742.9844 - val_loss: 692.0870\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 720.1472 - val_loss: 672.6299\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 697.6316 - val_loss: 653.4395\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 668.3835 - val_loss: 629.5515\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 639.0692 - val_loss: 603.9700\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 606.1190 - val_loss: 576.2275\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 576.4164 - val_loss: 545.4266\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 545.5058 - val_loss: 514.1293\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 505.0456 - val_loss: 454.2699\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 444.4201 - val_loss: 392.1324\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 350.7037 - val_loss: 311.9491\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 291.3777 - val_loss: 268.5659\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 254.9306 - val_loss: 250.3722\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.5874 - val_loss: 243.4986\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 229.2041 - val_loss: 238.6764\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 220.2266 - val_loss: 228.5077\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 211.2538 - val_loss: 227.6142\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 201.8243 - val_loss: 216.8534\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.7114 - val_loss: 212.8675\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.1698 - val_loss: 206.8961\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.1017 - val_loss: 212.3488\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 175.4446 - val_loss: 196.1855\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 169.9486 - val_loss: 194.8502\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 167.6034 - val_loss: 190.3828\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.9890 - val_loss: 188.1470\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.0495 - val_loss: 185.2899\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.0475 - val_loss: 188.5731\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.5774 - val_loss: 177.4619\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.6273 - val_loss: 175.1871\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.8060 - val_loss: 171.9688\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.0734 - val_loss: 171.9341\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.7239 - val_loss: 167.9230\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.0464 - val_loss: 166.3982\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.2457 - val_loss: 164.0468\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.3559 - val_loss: 173.6430\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.4810 - val_loss: 161.2588\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.0304 - val_loss: 161.0086\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.0203 - val_loss: 163.4516\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.6488 - val_loss: 161.7188\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.2597 - val_loss: 156.3722\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.0442 - val_loss: 155.5108\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.5877 - val_loss: 154.6395\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "Mean Squared Error for Training Model #  7  :  137.6861904905765\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  8 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 550.4107 - val_loss: 422.5899\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 424.2454 - val_loss: 404.4533\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 398.5211 - val_loss: 395.8524\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 376.1232 - val_loss: 380.1162\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 363.7343 - val_loss: 368.6931\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 351.2775 - val_loss: 358.6652\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 338.9191 - val_loss: 349.5677\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 327.5184 - val_loss: 340.7969\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 318.3445 - val_loss: 332.3282\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 309.2791 - val_loss: 324.1943\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 300.0742 - val_loss: 317.4732\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 294.8020 - val_loss: 311.2980\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 286.7384 - val_loss: 304.0267\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 278.1733 - val_loss: 298.2824\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 272.1081 - val_loss: 293.6855\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 266.1882 - val_loss: 287.6214\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.7542 - val_loss: 282.9116\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 254.7856 - val_loss: 277.8747\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 250.1533 - val_loss: 273.4189\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.2466 - val_loss: 270.2033\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 240.5186 - val_loss: 264.7803\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.4756 - val_loss: 260.3737\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 233.8236 - val_loss: 256.0187\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 228.4998 - val_loss: 253.2252\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 224.1073 - val_loss: 248.2362\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 220.1637 - val_loss: 245.2224\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.3521 - val_loss: 241.9241\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 210.9109 - val_loss: 236.1463\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 206.8473 - val_loss: 232.4201\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.1048 - val_loss: 229.0032\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 197.3795 - val_loss: 224.3454\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.6469 - val_loss: 220.3861\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 189.2473 - val_loss: 218.1857\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.6982 - val_loss: 212.6116\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.8690 - val_loss: 207.2166\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 172.0672 - val_loss: 200.6343\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.4817 - val_loss: 199.0268\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.4807 - val_loss: 190.9557\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.4932 - val_loss: 185.8258\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.6241 - val_loss: 180.4719\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.8995 - val_loss: 175.7359\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.4802 - val_loss: 176.2885\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.4192 - val_loss: 168.7838\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.1894 - val_loss: 164.6544\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.4539 - val_loss: 164.2260\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.3053 - val_loss: 160.4527\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.4898 - val_loss: 159.3687\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.5721 - val_loss: 155.4116\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.4780 - val_loss: 153.9652\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.8768 - val_loss: 152.7513\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 668us/step\n",
      "Mean Squared Error for Training Model #  8  :  137.8341505015061\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  9 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 43897.1133 - val_loss: 24863.2773\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 15114.7646 - val_loss: 6992.6055\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4247.2012 - val_loss: 2307.6130\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2229.6699 - val_loss: 2029.1899\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2135.5344 - val_loss: 1860.7098\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1937.4647 - val_loss: 1721.1483\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1789.6191 - val_loss: 1610.9049\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1664.0464 - val_loss: 1504.7581\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1546.2169 - val_loss: 1410.7131\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1441.1261 - val_loss: 1330.6852\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1355.7035 - val_loss: 1254.9406\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1269.4524 - val_loss: 1191.4805\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1194.2874 - val_loss: 1130.2081\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1126.9479 - val_loss: 1074.2091\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1061.4315 - val_loss: 1027.0110\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1007.3701 - val_loss: 977.3404\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 954.0308 - val_loss: 934.5680\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 905.6500 - val_loss: 888.7533\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 863.1962 - val_loss: 849.2377\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 821.9166 - val_loss: 813.0870\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 786.8301 - val_loss: 781.5991\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 751.7752 - val_loss: 750.3506\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 719.7023 - val_loss: 720.4258\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 692.1755 - val_loss: 693.0522\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 661.4910 - val_loss: 667.0345\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 634.4711 - val_loss: 643.0044\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 610.0160 - val_loss: 618.2709\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 587.6761 - val_loss: 597.9125\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 563.6985 - val_loss: 577.6414\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 544.1251 - val_loss: 558.6912\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 521.1852 - val_loss: 540.1994\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 504.2589 - val_loss: 523.5305\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 485.6397 - val_loss: 506.9541\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 467.2215 - val_loss: 490.8069\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 451.5548 - val_loss: 475.1039\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 435.0959 - val_loss: 460.2222\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 420.1497 - val_loss: 446.7741\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 404.3308 - val_loss: 433.2957\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 391.4466 - val_loss: 420.0675\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 379.6904 - val_loss: 406.3485\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 366.1280 - val_loss: 396.2920\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 354.1522 - val_loss: 384.1161\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 343.7856 - val_loss: 374.1615\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 331.8386 - val_loss: 361.5745\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 320.0737 - val_loss: 352.0992\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 308.6722 - val_loss: 342.1852\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 299.2881 - val_loss: 332.8428\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 290.1046 - val_loss: 323.1964\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 280.7754 - val_loss: 314.0107\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 271.7772 - val_loss: 305.5613\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "Mean Squared Error for Training Model #  9  :  284.2867851096251\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  10 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7164.1250 - val_loss: 4718.6514\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4424.3892 - val_loss: 3191.0051\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3064.8335 - val_loss: 2345.8298\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2256.4663 - val_loss: 1804.0425\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1709.5676 - val_loss: 1412.7037\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1307.0981 - val_loss: 1125.8312\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1044.8333 - val_loss: 892.9455\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 806.8787 - val_loss: 738.1245\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 641.8463 - val_loss: 590.2615\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 563.3891 - val_loss: 487.8294\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 438.8104 - val_loss: 412.7454\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 347.8138 - val_loss: 369.3450\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 295.6026 - val_loss: 320.7861\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 252.7154 - val_loss: 302.1208\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 221.6406 - val_loss: 248.9720\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 195.9753 - val_loss: 226.1765\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.3011 - val_loss: 212.3247\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 169.0007 - val_loss: 208.7337\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.7535 - val_loss: 205.4429\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.0461 - val_loss: 189.9220\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.8526 - val_loss: 191.3284\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.8245 - val_loss: 189.8228\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.3478 - val_loss: 185.4280\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.3023 - val_loss: 177.3838\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.3474 - val_loss: 169.1494\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.7749 - val_loss: 165.7190\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.7525 - val_loss: 170.8452\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.1716 - val_loss: 169.1119\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.1601 - val_loss: 165.8310\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.9450 - val_loss: 159.7162\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.9417 - val_loss: 159.8214\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.2657 - val_loss: 154.1633\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.3477 - val_loss: 154.2206\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.7647 - val_loss: 151.9806\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.9817 - val_loss: 155.0667\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.0135 - val_loss: 147.9477\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.3758 - val_loss: 146.6535\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.7970 - val_loss: 146.4841\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.7752 - val_loss: 144.5666\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.2415 - val_loss: 143.3653\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.6305 - val_loss: 141.5983\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.4654 - val_loss: 144.9191\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.4642 - val_loss: 139.8196\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.6189 - val_loss: 148.5966\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.6218 - val_loss: 159.3262\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.9015 - val_loss: 136.2874\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.9687 - val_loss: 151.4112\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.7733 - val_loss: 135.8436\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.2956 - val_loss: 133.6070\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.2349 - val_loss: 133.3787\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "Mean Squared Error for Training Model #  10  :  119.17886560830317\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  11 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2880.7876 - val_loss: 1341.0334\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1300.5162 - val_loss: 957.2164\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 924.3795 - val_loss: 719.7012\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 717.5643 - val_loss: 597.6917\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 595.7603 - val_loss: 527.9310\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 526.3409 - val_loss: 489.0395\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 473.5900 - val_loss: 452.1583\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 434.9009 - val_loss: 427.3180\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 402.8334 - val_loss: 405.2564\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 371.3153 - val_loss: 385.1950\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 345.8745 - val_loss: 365.0060\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 318.0277 - val_loss: 353.1701\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 301.2464 - val_loss: 340.6833\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 289.7946 - val_loss: 334.6156\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 278.8970 - val_loss: 331.5397\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 271.3734 - val_loss: 326.0703\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 264.7031 - val_loss: 324.2295\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 258.9437 - val_loss: 321.0418\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 254.2248 - val_loss: 317.7506\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 251.1339 - val_loss: 315.2982\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 247.5574 - val_loss: 313.1816\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.9914 - val_loss: 310.5406\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.2216 - val_loss: 307.9908\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 242.8680 - val_loss: 307.4734\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 240.1960 - val_loss: 307.8612\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 234.7850 - val_loss: 301.4489\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 231.7986 - val_loss: 301.5455\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 228.8477 - val_loss: 296.3963\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 227.6933 - val_loss: 295.1242\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 225.4512 - val_loss: 294.8034\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 225.4313 - val_loss: 288.9813\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 221.3503 - val_loss: 288.9189\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 218.0166 - val_loss: 284.0971\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.3698 - val_loss: 281.4695\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 216.6089 - val_loss: 278.6754\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 211.9308 - val_loss: 275.1791\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 210.5953 - val_loss: 271.8020\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.1507 - val_loss: 271.3765\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 204.1493 - val_loss: 265.5096\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.8115 - val_loss: 262.9112\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 200.6239 - val_loss: 260.2731\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 195.3362 - val_loss: 257.0480\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.1528 - val_loss: 257.6320\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 191.5707 - val_loss: 249.2003\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.1993 - val_loss: 245.6864\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.8161 - val_loss: 243.3087\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.9471 - val_loss: 237.3219\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.0087 - val_loss: 233.1779\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.0178 - val_loss: 230.7950\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.4058 - val_loss: 226.2583\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 668us/step\n",
      "Mean Squared Error for Training Model #  11  :  226.4017769102234\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  12 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 8039.1240 - val_loss: 4291.3774\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2749.3584 - val_loss: 1570.4744\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1305.8854 - val_loss: 1140.0095\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1155.2976 - val_loss: 1073.1630\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1061.8214 - val_loss: 998.0527\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 997.5687 - val_loss: 946.2883\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 937.5742 - val_loss: 890.3977\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 884.3613 - val_loss: 843.2009\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 836.3933 - val_loss: 801.9492\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 793.2433 - val_loss: 762.3320\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 752.7911 - val_loss: 728.7231\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 718.2255 - val_loss: 697.1658\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 684.8237 - val_loss: 668.8262\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 654.2346 - val_loss: 644.2842\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 627.2085 - val_loss: 618.9595\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 600.2577 - val_loss: 596.7054\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 574.5112 - val_loss: 575.5893\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 552.0085 - val_loss: 556.2322\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 530.4273 - val_loss: 539.3367\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 510.7117 - val_loss: 520.8027\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 490.6541 - val_loss: 505.5486\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 471.9839 - val_loss: 489.0123\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 454.9315 - val_loss: 475.2357\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 437.0690 - val_loss: 460.1011\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 421.6614 - val_loss: 446.5729\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 405.6646 - val_loss: 434.6296\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 391.2113 - val_loss: 422.5582\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 377.0246 - val_loss: 411.2975\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 364.3592 - val_loss: 398.9546\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 352.3229 - val_loss: 388.6304\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 340.0540 - val_loss: 377.3500\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 328.8188 - val_loss: 368.8939\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 319.4726 - val_loss: 357.9810\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 308.2395 - val_loss: 348.4031\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 297.9991 - val_loss: 340.2176\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 290.1909 - val_loss: 331.0907\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 280.0220 - val_loss: 324.1050\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 272.3092 - val_loss: 314.5558\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 264.1180 - val_loss: 308.0367\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 256.4171 - val_loss: 300.3437\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 248.2269 - val_loss: 293.0476\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.6180 - val_loss: 286.7841\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 234.5224 - val_loss: 280.6266\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 228.3194 - val_loss: 273.0110\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 220.8792 - val_loss: 266.3081\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 215.2259 - val_loss: 260.1816\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 211.4945 - val_loss: 253.9683\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 204.5651 - val_loss: 249.7757\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.7056 - val_loss: 244.9584\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.7692 - val_loss: 236.9989\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "Mean Squared Error for Training Model #  12  :  224.76990434648908\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  13 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 6993.1611 - val_loss: 1604.9578\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1557.3428 - val_loss: 1287.5996\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 964.8317 - val_loss: 669.0039\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 608.0458 - val_loss: 506.2571\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 451.1216 - val_loss: 413.5286\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 381.5684 - val_loss: 358.9371\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 337.5467 - val_loss: 324.2657\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 296.4929 - val_loss: 299.2521\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 268.3914 - val_loss: 280.2512\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.0287 - val_loss: 265.2188\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 228.7434 - val_loss: 252.6005\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 216.1118 - val_loss: 241.3774\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 205.5887 - val_loss: 232.4631\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 195.9275 - val_loss: 224.4494\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 186.7117 - val_loss: 216.2716\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.8169 - val_loss: 212.1404\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 175.0074 - val_loss: 213.7270\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 172.5671 - val_loss: 201.5998\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.8063 - val_loss: 197.0925\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.0101 - val_loss: 195.8215\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.0778 - val_loss: 192.6319\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.3697 - val_loss: 190.8225\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.6176 - val_loss: 185.5013\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.4070 - val_loss: 188.1550\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.7085 - val_loss: 180.2224\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.0267 - val_loss: 178.7809\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.2406 - val_loss: 176.9612\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.9131 - val_loss: 173.0261\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.0657 - val_loss: 168.7707\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.1175 - val_loss: 166.7076\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.7426 - val_loss: 163.8038\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.0854 - val_loss: 161.9077\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.8828 - val_loss: 163.2341\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.6089 - val_loss: 160.3340\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.1988 - val_loss: 156.8857\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.6169 - val_loss: 156.1722\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.8701 - val_loss: 154.4614\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.9628 - val_loss: 154.3406\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 119.2171 - val_loss: 151.4261\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.0646 - val_loss: 151.3385\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.6660 - val_loss: 147.6443\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.9357 - val_loss: 150.5031\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.4973 - val_loss: 148.2036\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.8739 - val_loss: 143.9711\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.9155 - val_loss: 143.4828\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.1282 - val_loss: 144.0107\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.6353 - val_loss: 141.2227\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.4563 - val_loss: 139.7731\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.2228 - val_loss: 139.6242\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.5663 - val_loss: 137.9382\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 668us/step\n",
      "Mean Squared Error for Training Model #  13  :  132.08892209744192\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  14 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 392.3218 - val_loss: 362.0371\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 334.4454 - val_loss: 324.9818\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 295.8464 - val_loss: 294.7401\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 260.3793 - val_loss: 275.3302\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 229.6374 - val_loss: 250.0818\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.7407 - val_loss: 251.8065\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 197.5301 - val_loss: 227.2533\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 183.7518 - val_loss: 211.3220\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 169.9559 - val_loss: 203.6989\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.2717 - val_loss: 195.9751\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.1292 - val_loss: 189.5007\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.9301 - val_loss: 188.7919\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.1074 - val_loss: 184.8694\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.7156 - val_loss: 185.9018\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.4023 - val_loss: 180.5958\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.5074 - val_loss: 177.8914\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.8787 - val_loss: 177.9363\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.1861 - val_loss: 183.3303\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.8317 - val_loss: 177.1447\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.9960 - val_loss: 175.0466\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.2808 - val_loss: 175.9494\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.5440 - val_loss: 176.6057\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.1082 - val_loss: 172.4505\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.5706 - val_loss: 170.3743\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.8238 - val_loss: 172.9466\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.2750 - val_loss: 172.4524\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.4348 - val_loss: 173.9903\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.2987 - val_loss: 167.5697\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.1922 - val_loss: 165.2340\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.2612 - val_loss: 165.4228\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.3433 - val_loss: 162.9973\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.4340 - val_loss: 166.0624\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.1123 - val_loss: 167.4010\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.1835 - val_loss: 162.9676\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.7614 - val_loss: 160.7201\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.0328 - val_loss: 158.5669\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.5093 - val_loss: 157.7434\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.9928 - val_loss: 157.4607\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.1037 - val_loss: 159.7805\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.2813 - val_loss: 162.9439\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.0633 - val_loss: 153.7124\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.0805 - val_loss: 153.0552\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.3006 - val_loss: 149.5801\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.5182 - val_loss: 145.8053\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.7070 - val_loss: 145.2019\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.5151 - val_loss: 141.3968\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.7571 - val_loss: 139.3134\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.6784 - val_loss: 136.5201\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.7098 - val_loss: 135.2920\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.9214 - val_loss: 134.0073\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 554us/step\n",
      "Mean Squared Error for Training Model #  14  :  121.31859854712785\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  15 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 123686.8203 - val_loss: 70555.7734\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 44773.4648 - val_loss: 20086.3555\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 11561.7119 - val_loss: 5037.1606\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3963.3367 - val_loss: 3202.8618\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2932.6379 - val_loss: 2517.4275\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2279.4785 - val_loss: 2025.1592\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1919.4633 - val_loss: 1762.4877\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1704.5231 - val_loss: 1586.3309\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1555.1635 - val_loss: 1432.5297\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1383.5205 - val_loss: 1186.6215\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1106.9386 - val_loss: 958.5760\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 904.8102 - val_loss: 803.0730\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 769.5234 - val_loss: 692.2068\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 674.1030 - val_loss: 628.4944\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 610.0139 - val_loss: 582.4622\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 558.6539 - val_loss: 547.7385\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 522.8331 - val_loss: 523.7233\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 495.0914 - val_loss: 503.5726\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 472.6731 - val_loss: 486.1501\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 457.1037 - val_loss: 467.5069\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 441.9962 - val_loss: 454.3326\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 424.8020 - val_loss: 440.5053\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 410.7823 - val_loss: 436.3037\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 397.0417 - val_loss: 420.9543\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 391.5383 - val_loss: 409.7308\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 384.2168 - val_loss: 416.4640\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 378.9044 - val_loss: 405.8224\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 366.3006 - val_loss: 386.6909\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 355.5956 - val_loss: 380.0035\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 342.5690 - val_loss: 373.8597\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 336.1686 - val_loss: 365.3640\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 330.6719 - val_loss: 363.3591\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 326.8322 - val_loss: 357.8638\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 317.4362 - val_loss: 347.0800\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 309.9695 - val_loss: 342.2003\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 304.8270 - val_loss: 339.4171\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 301.9690 - val_loss: 330.6090\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 291.1819 - val_loss: 328.6441\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 287.3361 - val_loss: 322.5242\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 283.5814 - val_loss: 316.7669\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 279.0128 - val_loss: 323.6223\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 277.4664 - val_loss: 308.7616\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 283.3995 - val_loss: 328.4205\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 274.6799 - val_loss: 309.8404\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 261.5211 - val_loss: 297.0363\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 256.0454 - val_loss: 293.9628\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 252.6509 - val_loss: 292.1365\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 252.9751 - val_loss: 288.3962\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.2356 - val_loss: 283.6461\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.1218 - val_loss: 281.9441\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "Mean Squared Error for Training Model #  15  :  284.0873318607146\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  16 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 9021.2324 - val_loss: 2135.5525\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2635.4529 - val_loss: 2437.0708\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1857.6194 - val_loss: 1545.3827\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1435.2047 - val_loss: 1268.2183\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1152.2396 - val_loss: 1082.6276\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 954.1395 - val_loss: 917.8485\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 804.7142 - val_loss: 823.5660\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 707.6110 - val_loss: 743.8325\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 632.3292 - val_loss: 692.9765\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 571.4211 - val_loss: 639.8341\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 525.1200 - val_loss: 602.4159\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 488.7229 - val_loss: 562.6877\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 455.3983 - val_loss: 528.2178\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 426.3949 - val_loss: 498.9422\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 401.1153 - val_loss: 468.5649\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 375.6500 - val_loss: 442.1218\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 356.7628 - val_loss: 428.5781\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 335.7763 - val_loss: 397.4277\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 321.4824 - val_loss: 378.7266\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 302.0995 - val_loss: 358.7353\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 288.0084 - val_loss: 342.2390\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 279.8448 - val_loss: 339.9601\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.0673 - val_loss: 313.5892\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.6281 - val_loss: 300.4502\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.0614 - val_loss: 284.7558\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 218.5886 - val_loss: 271.8850\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 210.6648 - val_loss: 263.5647\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 212.2414 - val_loss: 265.5600\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.3879 - val_loss: 252.5231\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 191.2079 - val_loss: 240.3261\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 184.4398 - val_loss: 230.8670\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.5350 - val_loss: 228.7938\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 169.4146 - val_loss: 220.6696\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.1702 - val_loss: 216.0997\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.0930 - val_loss: 231.6075\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.2060 - val_loss: 205.6457\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.7501 - val_loss: 206.8712\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.2644 - val_loss: 196.8016\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.8121 - val_loss: 198.2999\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.7377 - val_loss: 191.8861\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.5986 - val_loss: 186.3203\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.7860 - val_loss: 183.5628\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.5359 - val_loss: 181.9093\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.6557 - val_loss: 177.9254\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.3530 - val_loss: 175.6693\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.8524 - val_loss: 177.2965\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.7839 - val_loss: 171.5381\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.9131 - val_loss: 168.6477\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.7787 - val_loss: 168.8631\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.6566 - val_loss: 164.9206\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 663us/step\n",
      "Mean Squared Error for Training Model #  16  :  156.21935487764625\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  17 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 87147.0859 - val_loss: 34957.8438\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 15048.2959 - val_loss: 5870.3853\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5558.9346 - val_loss: 4962.0601\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5325.2349 - val_loss: 4369.9595\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4517.3027 - val_loss: 4065.8706\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4116.8745 - val_loss: 3772.0269\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3760.1772 - val_loss: 3399.9231\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3424.6338 - val_loss: 3099.2610\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3118.5232 - val_loss: 2783.4290\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2827.4768 - val_loss: 2546.5645\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2558.3000 - val_loss: 2304.6082\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2304.8538 - val_loss: 2052.4429\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2066.3938 - val_loss: 1835.9677\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1851.4430 - val_loss: 1644.6779\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1647.3855 - val_loss: 1470.3773\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1469.6097 - val_loss: 1299.5929\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1310.4220 - val_loss: 1164.3689\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1153.7540 - val_loss: 1030.1854\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1029.2003 - val_loss: 925.8193\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 916.0764 - val_loss: 838.8759\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 818.0578 - val_loss: 752.4725\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 732.1874 - val_loss: 684.1302\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 662.6157 - val_loss: 624.0337\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 602.8834 - val_loss: 579.0875\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 550.0443 - val_loss: 544.6751\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 504.1855 - val_loss: 506.3221\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 469.2728 - val_loss: 479.8943\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 437.3591 - val_loss: 454.5280\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 411.5685 - val_loss: 436.2807\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 388.4974 - val_loss: 423.0375\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 369.8024 - val_loss: 401.1284\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 351.6408 - val_loss: 390.0436\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 336.5437 - val_loss: 380.3495\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 323.2704 - val_loss: 363.5048\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 310.4817 - val_loss: 353.9924\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 301.5638 - val_loss: 342.6663\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 289.8379 - val_loss: 334.6461\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 280.4317 - val_loss: 326.9505\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 272.1079 - val_loss: 319.0493\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 264.7949 - val_loss: 313.6138\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 257.6906 - val_loss: 296.7697\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 249.4443 - val_loss: 289.9704\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.6518 - val_loss: 281.0315\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.4111 - val_loss: 280.8410\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 230.0794 - val_loss: 278.7443\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 223.9619 - val_loss: 265.8972\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 218.7747 - val_loss: 259.4383\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 213.6894 - val_loss: 256.9870\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.6247 - val_loss: 258.1623\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.8025 - val_loss: 248.5586\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 661us/step\n",
      "Mean Squared Error for Training Model #  17  :  248.26660305588106\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  18 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 6164.7603 - val_loss: 2433.9668\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2175.5122 - val_loss: 1744.4263\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1740.6870 - val_loss: 1404.4591\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1395.5304 - val_loss: 1170.7275\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1202.0829 - val_loss: 1021.6891\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1065.4070 - val_loss: 910.3002\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 962.3201 - val_loss: 827.1571\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 878.2853 - val_loss: 753.5540\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 811.9017 - val_loss: 698.6401\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 754.7310 - val_loss: 649.8414\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 702.1985 - val_loss: 604.7286\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 655.2380 - val_loss: 564.5541\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 613.6935 - val_loss: 534.2061\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 577.2288 - val_loss: 502.5002\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 544.9809 - val_loss: 477.7568\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 515.6737 - val_loss: 452.2428\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 488.4462 - val_loss: 432.8997\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 465.9086 - val_loss: 416.9669\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 443.7483 - val_loss: 397.7347\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 426.6207 - val_loss: 384.0202\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 411.5851 - val_loss: 369.0822\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 394.1353 - val_loss: 360.5606\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 378.5640 - val_loss: 346.3826\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 365.7265 - val_loss: 336.9178\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 354.4881 - val_loss: 328.5695\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 342.7265 - val_loss: 320.5798\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 333.6474 - val_loss: 313.9654\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 324.3191 - val_loss: 307.1112\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 316.1846 - val_loss: 302.0298\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 309.8837 - val_loss: 296.0055\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 302.8806 - val_loss: 291.8707\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 295.5134 - val_loss: 286.9728\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 290.0027 - val_loss: 284.9498\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 285.3576 - val_loss: 280.4861\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 280.5110 - val_loss: 276.0391\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 277.4145 - val_loss: 273.6422\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 273.3539 - val_loss: 271.9331\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 268.9436 - val_loss: 267.9971\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 264.2171 - val_loss: 265.8710\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 260.2899 - val_loss: 263.1524\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.9414 - val_loss: 262.1767\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.0708 - val_loss: 260.3080\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 252.8270 - val_loss: 258.2905\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 247.5705 - val_loss: 257.1673\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.3537 - val_loss: 254.5923\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.0672 - val_loss: 253.1582\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.2114 - val_loss: 252.5541\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.6542 - val_loss: 252.8090\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.1444 - val_loss: 249.5823\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 234.2999 - val_loss: 248.5748\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "Mean Squared Error for Training Model #  18  :  241.57455309397037\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  19 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 10443.5820 - val_loss: 1201.2383\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 667.5695 - val_loss: 1047.5319\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 586.7036 - val_loss: 401.0476\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 354.8352 - val_loss: 396.4848\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 330.4795 - val_loss: 383.4387\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 302.9662 - val_loss: 356.7066\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 282.5742 - val_loss: 345.8008\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 272.4396 - val_loss: 331.3448\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 260.7390 - val_loss: 318.5002\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 249.8933 - val_loss: 306.7156\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.9990 - val_loss: 295.6036\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 231.6203 - val_loss: 285.3986\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.0502 - val_loss: 276.2411\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 216.5520 - val_loss: 267.5121\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 206.9753 - val_loss: 258.5234\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 200.7606 - val_loss: 250.5409\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.0368 - val_loss: 243.4805\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.7912 - val_loss: 235.6352\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.5760 - val_loss: 228.1185\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.2803 - val_loss: 222.8954\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 171.8571 - val_loss: 216.3168\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 169.4576 - val_loss: 213.2403\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.6126 - val_loss: 204.7977\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.8756 - val_loss: 198.9766\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.3044 - val_loss: 194.6387\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.3872 - val_loss: 189.0078\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.2049 - val_loss: 187.7207\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.5058 - val_loss: 181.5530\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.2114 - val_loss: 177.0101\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.5489 - val_loss: 173.9641\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.7915 - val_loss: 171.3352\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.2484 - val_loss: 166.9962\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.0890 - val_loss: 167.8532\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.0394 - val_loss: 162.4052\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.2532 - val_loss: 163.1705\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.7564 - val_loss: 156.0004\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 121.9301 - val_loss: 153.8734\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.9347 - val_loss: 155.8884\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.8455 - val_loss: 152.0878\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.3200 - val_loss: 148.1039\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.4055 - val_loss: 151.7831\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.5074 - val_loss: 145.4249\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.9938 - val_loss: 142.4318\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.0705 - val_loss: 142.5461\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.3876 - val_loss: 140.6421\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.1321 - val_loss: 140.7330\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.6967 - val_loss: 137.1341\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.5156 - val_loss: 138.8731\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.8595 - val_loss: 133.9537\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.5325 - val_loss: 135.1663\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 662us/step\n",
      "Mean Squared Error for Training Model #  19  :  124.38564314581667\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  20 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 18550.3789 - val_loss: 3381.5999\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1151.1819 - val_loss: 968.9672\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 999.3134 - val_loss: 707.9210\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 580.0995 - val_loss: 620.5044\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 509.7070 - val_loss: 561.1649\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 462.4260 - val_loss: 509.9622\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 429.3936 - val_loss: 483.3220\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 395.2207 - val_loss: 453.9978\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 365.5660 - val_loss: 421.7760\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 336.5711 - val_loss: 398.6622\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 316.0360 - val_loss: 378.4395\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 298.9266 - val_loss: 362.3837\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 287.5559 - val_loss: 353.4981\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 277.9826 - val_loss: 343.9221\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 269.8493 - val_loss: 334.9622\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.8895 - val_loss: 327.8955\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 258.4268 - val_loss: 320.1324\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 250.7972 - val_loss: 314.4281\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.9271 - val_loss: 309.9413\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.8126 - val_loss: 301.9555\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.1969 - val_loss: 296.6217\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 229.0113 - val_loss: 292.1682\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 224.5894 - val_loss: 285.0977\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 220.1093 - val_loss: 279.1671\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 214.8055 - val_loss: 273.2793\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 209.6511 - val_loss: 270.8682\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 204.1270 - val_loss: 261.8989\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 201.2783 - val_loss: 258.4243\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.6248 - val_loss: 253.6160\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.3854 - val_loss: 250.3411\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 189.4765 - val_loss: 245.2514\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.0837 - val_loss: 240.6009\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.7756 - val_loss: 235.2882\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.7122 - val_loss: 233.4014\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.8394 - val_loss: 228.8037\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 171.0783 - val_loss: 225.4487\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.1485 - val_loss: 221.5017\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.8160 - val_loss: 216.0340\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.4788 - val_loss: 212.9405\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.5106 - val_loss: 209.9136\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.1045 - val_loss: 207.0821\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.1331 - val_loss: 203.1041\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.2559 - val_loss: 200.3152\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.7633 - val_loss: 197.1689\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.6833 - val_loss: 195.6398\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.1682 - val_loss: 191.6589\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.2349 - val_loss: 188.0495\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.6389 - val_loss: 188.4636\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.8888 - val_loss: 183.0972\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.3209 - val_loss: 180.6615\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 557us/step\n",
      "Mean Squared Error for Training Model #  20  :  181.87190056939355\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  21 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7523.2847 - val_loss: 1659.1732\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 764.6661 - val_loss: 666.9492\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 587.7869 - val_loss: 373.4235\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 362.4360 - val_loss: 356.2429\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 328.1182 - val_loss: 321.4285\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 297.8647 - val_loss: 298.0958\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 274.1146 - val_loss: 276.6520\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 251.6796 - val_loss: 253.8738\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 231.0842 - val_loss: 233.0704\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 211.8515 - val_loss: 214.3183\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 195.5069 - val_loss: 199.7157\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.1237 - val_loss: 186.6119\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 169.2237 - val_loss: 175.9157\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.2748 - val_loss: 167.4848\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.7099 - val_loss: 160.7215\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.7293 - val_loss: 152.7709\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.6861 - val_loss: 149.3010\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.8445 - val_loss: 143.3007\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.6213 - val_loss: 140.6891\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.9771 - val_loss: 137.5014\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.5688 - val_loss: 135.9172\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.0071 - val_loss: 131.7818\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.9107 - val_loss: 129.7853\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.9138 - val_loss: 130.4788\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.3918 - val_loss: 125.1125\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.8407 - val_loss: 123.3752\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.5113 - val_loss: 123.5265\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.0717 - val_loss: 120.9551\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.1344 - val_loss: 121.6053\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.5271 - val_loss: 117.9174\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.5491 - val_loss: 116.8584\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.3330 - val_loss: 117.9672\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.5952 - val_loss: 114.4729\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.4619 - val_loss: 113.4077\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.8987 - val_loss: 113.4123\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.2698 - val_loss: 113.3115\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.8391 - val_loss: 112.6147\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.3776 - val_loss: 111.1783\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.8298 - val_loss: 110.1088\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.7560 - val_loss: 109.3932\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.2948 - val_loss: 109.2705\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.6823 - val_loss: 106.8100\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.7887 - val_loss: 106.5628\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 74.8731 - val_loss: 108.9404\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 74.7584 - val_loss: 104.6978\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 73.2691 - val_loss: 105.2739\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 74.9045 - val_loss: 113.7752\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.7867 - val_loss: 102.7602\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 72.2964 - val_loss: 102.8316\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 72.0401 - val_loss: 100.6162\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 554us/step\n",
      "Mean Squared Error for Training Model #  21  :  95.44729095373566\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  22 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1288.9276 - val_loss: 761.0494\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 426.4676 - val_loss: 349.3842\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 231.8409 - val_loss: 272.7835\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 188.5585 - val_loss: 253.1068\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 175.7183 - val_loss: 236.9759\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.0433 - val_loss: 229.8463\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.9466 - val_loss: 223.5246\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.9056 - val_loss: 220.2218\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.1896 - val_loss: 215.1358\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.7434 - val_loss: 212.2890\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.7346 - val_loss: 209.8515\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.7813 - val_loss: 203.2535\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.1781 - val_loss: 202.0200\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 145.0943 - val_loss: 198.2121\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.7912 - val_loss: 194.3389\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.1241 - val_loss: 192.1539\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.4413 - val_loss: 189.4714\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.3582 - val_loss: 186.3063\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.5185 - val_loss: 185.3139\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.5387 - val_loss: 183.5189\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.0773 - val_loss: 180.4961\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.9870 - val_loss: 187.0240\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.0380 - val_loss: 176.6591\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.6562 - val_loss: 179.5931\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.4254 - val_loss: 173.6985\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.5953 - val_loss: 174.7592\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.0208 - val_loss: 173.2265\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.1531 - val_loss: 175.8439\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.6822 - val_loss: 168.7286\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.5246 - val_loss: 173.6899\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.9213 - val_loss: 166.0783\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.7125 - val_loss: 172.2715\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.8968 - val_loss: 164.8280\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.0471 - val_loss: 163.8078\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.6357 - val_loss: 162.8059\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.1986 - val_loss: 161.0472\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.3251 - val_loss: 161.2843\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.5981 - val_loss: 162.7800\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.8518 - val_loss: 158.2440\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.6720 - val_loss: 158.6781\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.8343 - val_loss: 156.6368\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.2770 - val_loss: 161.3843\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 118.4157 - val_loss: 154.2478\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.7288 - val_loss: 154.0519\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.6856 - val_loss: 156.6578\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.8165 - val_loss: 157.3674\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.2784 - val_loss: 157.2118\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.7801 - val_loss: 152.1242\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.7323 - val_loss: 151.9719\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.1043 - val_loss: 150.6055\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 554us/step\n",
      "Mean Squared Error for Training Model #  22  :  138.22937606866776\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  23 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 8516.7930 - val_loss: 2148.9441\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1953.3225 - val_loss: 1601.9027\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1168.0157 - val_loss: 1276.3240\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1058.3424 - val_loss: 1106.0239\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 944.9356 - val_loss: 1018.7945\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 871.3294 - val_loss: 952.7800\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 808.7627 - val_loss: 880.1157\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 757.2755 - val_loss: 830.1342\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 707.5081 - val_loss: 785.4487\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 667.4476 - val_loss: 737.5849\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 627.1519 - val_loss: 697.7473\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 596.3997 - val_loss: 660.0323\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 557.0294 - val_loss: 622.6964\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 532.1671 - val_loss: 585.7383\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 503.4751 - val_loss: 558.8068\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 483.3616 - val_loss: 528.0451\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 468.3130 - val_loss: 506.0543\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 438.7516 - val_loss: 471.1757\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 409.4914 - val_loss: 442.4754\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 374.3643 - val_loss: 389.0760\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 313.3625 - val_loss: 340.6316\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 269.5555 - val_loss: 309.0492\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.3589 - val_loss: 281.8440\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 213.1541 - val_loss: 259.3637\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 197.3195 - val_loss: 237.6554\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 178.4899 - val_loss: 219.2965\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.7828 - val_loss: 205.5900\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.7109 - val_loss: 192.5475\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.8174 - val_loss: 182.0638\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.6163 - val_loss: 173.7370\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.6136 - val_loss: 172.4681\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.7430 - val_loss: 167.6289\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.9796 - val_loss: 160.8676\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.7759 - val_loss: 150.9386\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.0103 - val_loss: 147.9901\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.7924 - val_loss: 144.5612\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.0364 - val_loss: 142.8000\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.1382 - val_loss: 141.0979\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.0947 - val_loss: 136.8524\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.6706 - val_loss: 135.9177\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.0467 - val_loss: 133.2006\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.4891 - val_loss: 134.3596\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.9205 - val_loss: 132.2120\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.2228 - val_loss: 129.0344\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.2424 - val_loss: 129.0549\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.9316 - val_loss: 134.3931\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.5158 - val_loss: 129.1556\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.4994 - val_loss: 129.0178\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.3322 - val_loss: 123.1865\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.5045 - val_loss: 122.0930\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 661us/step\n",
      "Mean Squared Error for Training Model #  23  :  109.47576881733923\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  24 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1057.8428 - val_loss: 652.9602\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 487.5340 - val_loss: 367.0016\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 342.4962 - val_loss: 334.4486\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 310.9983 - val_loss: 324.0500\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 291.0995 - val_loss: 316.3327\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 281.7099 - val_loss: 307.9991\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 277.4688 - val_loss: 300.1752\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 270.4289 - val_loss: 293.9021\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 265.2234 - val_loss: 291.1731\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 261.0905 - val_loss: 282.6690\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 252.8385 - val_loss: 276.2061\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 247.6088 - val_loss: 271.3926\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.5409 - val_loss: 265.3881\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.1757 - val_loss: 261.7386\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 235.4183 - val_loss: 257.8031\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 231.0658 - val_loss: 251.2620\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 223.2810 - val_loss: 247.8129\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 215.7954 - val_loss: 243.7547\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 212.9354 - val_loss: 242.3967\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 207.7868 - val_loss: 228.3633\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.0963 - val_loss: 224.5162\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.3690 - val_loss: 219.7397\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.3434 - val_loss: 222.9415\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.0279 - val_loss: 216.3848\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.4796 - val_loss: 215.2312\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 169.6775 - val_loss: 211.0544\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 166.3418 - val_loss: 218.7967\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 167.0641 - val_loss: 207.7228\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.5453 - val_loss: 205.7193\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.0323 - val_loss: 204.1621\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.1013 - val_loss: 203.7608\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.2002 - val_loss: 205.5244\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.0051 - val_loss: 198.2426\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.1426 - val_loss: 196.3432\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.4893 - val_loss: 197.5645\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.4598 - val_loss: 192.8812\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.6645 - val_loss: 195.3755\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.4984 - val_loss: 192.5525\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.1764 - val_loss: 188.8350\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.9914 - val_loss: 191.0271\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.7235 - val_loss: 188.6555\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.4836 - val_loss: 186.5146\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.1025 - val_loss: 184.5864\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.4838 - val_loss: 184.2546\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.2349 - val_loss: 185.5589\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.0942 - val_loss: 185.2315\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.6552 - val_loss: 181.1020\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.0046 - val_loss: 184.6940\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.9272 - val_loss: 178.6163\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 139.3345 - val_loss: 178.3386\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 668us/step\n",
      "Mean Squared Error for Training Model #  24  :  168.44581913438162\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  25 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 24533.6504 - val_loss: 7371.2417\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3104.9714 - val_loss: 723.6487\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 994.5529 - val_loss: 1031.9907\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 952.6815 - val_loss: 679.5598\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 751.1321 - val_loss: 591.3540\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 712.2545 - val_loss: 559.0834\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 668.0200 - val_loss: 532.0494\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 626.4956 - val_loss: 501.4950\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 591.9311 - val_loss: 478.7864\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 560.2153 - val_loss: 458.3520\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 534.3116 - val_loss: 441.8440\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 507.5500 - val_loss: 426.8799\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 484.7310 - val_loss: 410.5733\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 465.0428 - val_loss: 396.7540\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 446.1862 - val_loss: 385.3900\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 425.9763 - val_loss: 371.5915\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 407.7342 - val_loss: 357.4341\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 383.2725 - val_loss: 341.0670\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 351.7408 - val_loss: 316.1416\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 316.9822 - val_loss: 295.0913\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 281.9748 - val_loss: 278.1782\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 256.7530 - val_loss: 267.7427\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.7110 - val_loss: 257.6328\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 225.8350 - val_loss: 248.0846\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 215.0319 - val_loss: 241.9425\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.9171 - val_loss: 233.4398\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 197.5614 - val_loss: 227.9557\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.3649 - val_loss: 227.8251\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 182.5750 - val_loss: 215.9592\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 175.3124 - val_loss: 210.8826\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 169.7567 - val_loss: 209.7336\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.9435 - val_loss: 203.7028\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.0906 - val_loss: 202.3094\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.4309 - val_loss: 211.3108\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 158.0036 - val_loss: 196.5095\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.4672 - val_loss: 197.1847\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.7509 - val_loss: 192.2877\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.3265 - val_loss: 197.3057\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.7997 - val_loss: 190.6850\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.9533 - val_loss: 190.9689\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.6608 - val_loss: 187.8569\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.9902 - val_loss: 187.5027\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.9068 - val_loss: 185.4731\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.7945 - val_loss: 186.8816\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.0771 - val_loss: 183.6467\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.9977 - val_loss: 185.8953\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.8473 - val_loss: 184.6159\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.9833 - val_loss: 180.9318\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.6510 - val_loss: 180.7089\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.5039 - val_loss: 180.2877\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 554us/step\n",
      "Mean Squared Error for Training Model #  25  :  166.01622123362253\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  26 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4175.8511 - val_loss: 2345.0515\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1591.0859 - val_loss: 903.2836\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 702.7667 - val_loss: 491.8075\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 420.1367 - val_loss: 386.5664\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 319.9764 - val_loss: 353.6053\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 288.1637 - val_loss: 338.5633\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 273.3968 - val_loss: 328.2933\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 260.7773 - val_loss: 316.1964\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 249.4504 - val_loss: 307.0151\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.9937 - val_loss: 297.2152\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 233.7175 - val_loss: 299.9731\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 221.3186 - val_loss: 279.9351\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 211.4483 - val_loss: 272.5981\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.1423 - val_loss: 262.9356\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.1478 - val_loss: 254.7987\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 190.1729 - val_loss: 244.8146\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 178.4493 - val_loss: 236.2943\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 169.1570 - val_loss: 227.5668\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.8709 - val_loss: 219.2023\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.7222 - val_loss: 208.5708\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.5802 - val_loss: 196.0004\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.5905 - val_loss: 189.9861\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.5836 - val_loss: 187.9861\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.6009 - val_loss: 174.3462\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.5885 - val_loss: 171.7910\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.6882 - val_loss: 163.4933\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.6597 - val_loss: 159.0439\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.1651 - val_loss: 156.7415\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.7413 - val_loss: 156.8632\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.3301 - val_loss: 149.3956\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.2995 - val_loss: 148.5537\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.5940 - val_loss: 145.1695\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.2134 - val_loss: 152.0072\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.2338 - val_loss: 141.0959\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.7485 - val_loss: 144.3051\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.4065 - val_loss: 138.7940\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.3420 - val_loss: 138.4571\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.1775 - val_loss: 138.2491\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.7704 - val_loss: 135.5359\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.4700 - val_loss: 135.3030\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.2446 - val_loss: 137.6250\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.1956 - val_loss: 137.0980\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.5171 - val_loss: 133.8176\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.3366 - val_loss: 138.1977\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.3406 - val_loss: 133.6621\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.9471 - val_loss: 134.1418\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.0294 - val_loss: 133.9617\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.8701 - val_loss: 138.1705\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.1910 - val_loss: 131.5291\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.1220 - val_loss: 132.6643\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 554us/step\n",
      "Mean Squared Error for Training Model #  26  :  117.75432057524249\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  27 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3506.3994 - val_loss: 2603.7378\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2133.0486 - val_loss: 1635.8379\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1372.5806 - val_loss: 1052.0767\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 907.3373 - val_loss: 766.4263\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 667.6025 - val_loss: 627.8021\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 533.9200 - val_loss: 525.6511\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 438.2718 - val_loss: 442.3602\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 361.9948 - val_loss: 373.6779\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 298.0848 - val_loss: 318.9613\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 251.7342 - val_loss: 283.4534\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 229.4060 - val_loss: 260.8733\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 213.6008 - val_loss: 246.9999\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 201.5841 - val_loss: 234.6277\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.6386 - val_loss: 223.5733\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.2469 - val_loss: 213.4037\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.7526 - val_loss: 207.7342\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 175.0267 - val_loss: 201.1277\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 167.8885 - val_loss: 193.5819\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.4603 - val_loss: 185.3627\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.1578 - val_loss: 175.4581\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.8772 - val_loss: 166.4103\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.7842 - val_loss: 155.3819\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.9756 - val_loss: 146.2189\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.8776 - val_loss: 136.5997\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.1372 - val_loss: 130.1854\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.9059 - val_loss: 124.5951\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.2930 - val_loss: 122.6106\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.0756 - val_loss: 119.2195\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.9069 - val_loss: 118.0676\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.1410 - val_loss: 116.7517\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.3829 - val_loss: 116.1058\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.9899 - val_loss: 114.7778\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.8369 - val_loss: 114.9003\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 96.3915 - val_loss: 113.4267\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.7986 - val_loss: 113.2544\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.1929 - val_loss: 110.1090\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.8810 - val_loss: 109.7480\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.6632 - val_loss: 108.3355\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.7364 - val_loss: 108.6374\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.7631 - val_loss: 107.7252\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.5956 - val_loss: 108.2763\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.0875 - val_loss: 105.3951\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.8772 - val_loss: 106.6540\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.7572 - val_loss: 104.9998\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.1848 - val_loss: 104.0221\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.0014 - val_loss: 101.7961\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.9302 - val_loss: 101.8149\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.8788 - val_loss: 101.9153\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.3863 - val_loss: 98.4650\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.7000 - val_loss: 99.5680\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 668us/step\n",
      "Mean Squared Error for Training Model #  27  :  107.73142934005813\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  28 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4785.3057 - val_loss: 1839.1018\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1990.7382 - val_loss: 1615.9064\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1503.5134 - val_loss: 1189.4541\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1224.7018 - val_loss: 1051.6763\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1028.9647 - val_loss: 942.0338\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 876.9273 - val_loss: 807.9829\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 758.7937 - val_loss: 714.2910\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 652.4407 - val_loss: 614.1311\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 563.9722 - val_loss: 530.8124\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 477.0357 - val_loss: 435.9803\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 393.7067 - val_loss: 355.9505\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 329.5336 - val_loss: 292.9487\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 266.7223 - val_loss: 265.6163\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 232.6108 - val_loss: 234.6312\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 207.4749 - val_loss: 219.5960\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.4628 - val_loss: 212.0096\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 182.4800 - val_loss: 202.2845\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 175.5569 - val_loss: 198.2076\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.2766 - val_loss: 191.1182\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.7730 - val_loss: 191.8176\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.6405 - val_loss: 181.1598\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.5023 - val_loss: 177.5787\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.7719 - val_loss: 175.0592\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.0840 - val_loss: 171.0803\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.7553 - val_loss: 169.6261\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.0875 - val_loss: 165.6816\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.8231 - val_loss: 162.3339\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.4013 - val_loss: 160.7972\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.9481 - val_loss: 156.5773\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.9236 - val_loss: 157.0256\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.8537 - val_loss: 151.7533\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.2144 - val_loss: 154.8049\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.0120 - val_loss: 149.8697\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.5423 - val_loss: 145.7208\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.0987 - val_loss: 143.5849\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.5509 - val_loss: 144.5436\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.5387 - val_loss: 140.5796\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.5647 - val_loss: 142.6072\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 113.4201 - val_loss: 137.5684\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 110.1227 - val_loss: 139.5881\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.2120 - val_loss: 134.7147\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.9496 - val_loss: 139.9505\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.9966 - val_loss: 135.6736\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.1494 - val_loss: 131.9378\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.8531 - val_loss: 130.5071\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.8816 - val_loss: 129.8226\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.9625 - val_loss: 128.9962\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.3692 - val_loss: 130.2235\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 103.5949 - val_loss: 127.8200\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.8164 - val_loss: 132.9579\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "Mean Squared Error for Training Model #  28  :  126.4319801908665\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  29 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2697.1301 - val_loss: 1811.0658\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1215.6885 - val_loss: 993.0332\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 871.9147 - val_loss: 772.8715\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 712.9686 - val_loss: 691.3193\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 649.8469 - val_loss: 619.6129\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 588.5842 - val_loss: 571.9857\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 538.5823 - val_loss: 530.3895\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 500.6256 - val_loss: 486.2059\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 452.1039 - val_loss: 450.7826\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 422.2014 - val_loss: 427.4327\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 381.5430 - val_loss: 397.7632\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 360.7413 - val_loss: 373.7991\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 336.9336 - val_loss: 354.4426\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 317.4886 - val_loss: 336.7944\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 301.3672 - val_loss: 326.9174\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 282.2596 - val_loss: 312.5602\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 275.1092 - val_loss: 300.7839\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.6421 - val_loss: 292.6352\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 257.0762 - val_loss: 276.9885\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 247.7613 - val_loss: 269.2717\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.9393 - val_loss: 266.0484\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 229.2819 - val_loss: 259.0247\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 221.6956 - val_loss: 248.4958\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 213.6194 - val_loss: 242.9372\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.0146 - val_loss: 242.1355\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.3108 - val_loss: 232.3556\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 197.5842 - val_loss: 233.7903\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 200.0334 - val_loss: 231.4977\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 188.8195 - val_loss: 220.5986\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 188.6016 - val_loss: 217.4157\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.4079 - val_loss: 222.6122\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.0348 - val_loss: 208.9221\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.2722 - val_loss: 216.5465\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 172.6279 - val_loss: 203.2744\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 167.4017 - val_loss: 201.3498\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.0381 - val_loss: 197.6801\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.4467 - val_loss: 198.4763\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.7102 - val_loss: 196.1349\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.3722 - val_loss: 200.1767\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.6641 - val_loss: 190.3259\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.8453 - val_loss: 187.8095\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.2174 - val_loss: 201.3095\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.9297 - val_loss: 184.8612\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.7616 - val_loss: 191.8165\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.4135 - val_loss: 185.8242\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.8219 - val_loss: 198.1150\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.8812 - val_loss: 186.4068\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.2961 - val_loss: 188.8481\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.9982 - val_loss: 178.9875\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.4545 - val_loss: 176.4772\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 668us/step\n",
      "Mean Squared Error for Training Model #  29  :  162.72713931345115\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  30 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 278.0473 - val_loss: 288.6578\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 208.6579 - val_loss: 236.0989\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 191.8941 - val_loss: 221.2410\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.8551 - val_loss: 217.4586\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 175.1584 - val_loss: 215.7832\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 172.3628 - val_loss: 212.9585\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.3598 - val_loss: 212.1061\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.4656 - val_loss: 206.2178\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.8073 - val_loss: 203.8567\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.4273 - val_loss: 200.6210\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.0932 - val_loss: 200.5840\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.6461 - val_loss: 194.6344\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.7041 - val_loss: 192.2000\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.5214 - val_loss: 189.2775\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.6475 - val_loss: 186.4909\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.6579 - val_loss: 188.4711\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.2710 - val_loss: 182.2336\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.0629 - val_loss: 186.3902\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.4586 - val_loss: 175.9537\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.4732 - val_loss: 177.2693\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.6680 - val_loss: 169.4969\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.1494 - val_loss: 167.6678\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.4142 - val_loss: 165.1429\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.3433 - val_loss: 163.5504\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.7906 - val_loss: 164.1995\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.4763 - val_loss: 158.0958\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.5507 - val_loss: 156.0234\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.4588 - val_loss: 157.1047\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.5190 - val_loss: 159.6672\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.5506 - val_loss: 150.8887\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.2784 - val_loss: 150.8378\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.4976 - val_loss: 148.6961\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.3426 - val_loss: 146.8759\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.7475 - val_loss: 147.2023\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.2362 - val_loss: 150.5315\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.9871 - val_loss: 144.7914\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.7804 - val_loss: 141.3331\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.9338 - val_loss: 141.9370\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.6873 - val_loss: 140.1651\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.2103 - val_loss: 141.0488\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.3821 - val_loss: 140.4961\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 100.5461 - val_loss: 139.6904\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.5437 - val_loss: 138.1627\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.6217 - val_loss: 138.1640\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.1695 - val_loss: 138.0018\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.7920 - val_loss: 135.5565\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.7012 - val_loss: 139.1397\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.8463 - val_loss: 134.5458\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.7820 - val_loss: 131.4539\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.3569 - val_loss: 130.6324\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 554us/step\n",
      "Mean Squared Error for Training Model #  30  :  112.73101991064829\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  31 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1038.0671 - val_loss: 524.0182\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 354.3706 - val_loss: 340.0073\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 277.6896 - val_loss: 320.6796\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 267.1487 - val_loss: 309.4601\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 258.2941 - val_loss: 300.3158\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 251.4122 - val_loss: 291.7872\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.8447 - val_loss: 283.6515\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.5520 - val_loss: 278.2624\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 232.4332 - val_loss: 272.7584\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 226.9434 - val_loss: 266.1611\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 221.4625 - val_loss: 262.2905\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 215.5411 - val_loss: 257.0303\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 211.2218 - val_loss: 252.3839\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 206.1787 - val_loss: 247.6892\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 201.4548 - val_loss: 241.3554\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 197.2367 - val_loss: 238.1506\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.8841 - val_loss: 234.7464\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 189.7208 - val_loss: 232.0992\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 186.3208 - val_loss: 227.4950\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 183.6339 - val_loss: 224.8983\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.0080 - val_loss: 222.6005\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.9561 - val_loss: 220.4505\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.1788 - val_loss: 216.1677\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 171.7701 - val_loss: 214.2912\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 169.0464 - val_loss: 210.5131\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 166.6240 - val_loss: 208.2907\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.4353 - val_loss: 206.7146\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.7559 - val_loss: 203.5936\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.5821 - val_loss: 202.4198\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.2527 - val_loss: 199.0522\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.9757 - val_loss: 197.0938\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.7775 - val_loss: 196.0242\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.5366 - val_loss: 192.0957\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.9511 - val_loss: 190.8965\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.9918 - val_loss: 187.5089\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.7869 - val_loss: 185.5864\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.3453 - val_loss: 183.4900\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.0484 - val_loss: 181.4459\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.3522 - val_loss: 179.3055\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.5881 - val_loss: 177.8798\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.3007 - val_loss: 175.0665\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.9313 - val_loss: 173.7394\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.0696 - val_loss: 172.1843\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.3520 - val_loss: 171.1809\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.8778 - val_loss: 168.9142\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.5339 - val_loss: 168.3499\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.4398 - val_loss: 166.8389\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.0825 - val_loss: 164.6549\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.9182 - val_loss: 164.1003\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.7080 - val_loss: 162.1539\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 662us/step\n",
      "Mean Squared Error for Training Model #  31  :  152.27605197638172\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  32 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4912.9316 - val_loss: 416.0992\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 615.7562 - val_loss: 568.6285\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 382.6534 - val_loss: 395.3189\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 285.7223 - val_loss: 283.0521\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 242.6932 - val_loss: 272.1916\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 230.3603 - val_loss: 265.1440\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 224.5159 - val_loss: 254.8633\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 219.0597 - val_loss: 246.9433\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 211.9685 - val_loss: 245.5892\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 206.5872 - val_loss: 238.9380\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 205.2782 - val_loss: 234.3468\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 200.0632 - val_loss: 233.0868\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.2013 - val_loss: 227.1081\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.0215 - val_loss: 224.6617\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 191.0695 - val_loss: 223.6667\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 183.7140 - val_loss: 216.2909\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 183.6321 - val_loss: 216.0298\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.7629 - val_loss: 211.1129\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 178.0057 - val_loss: 206.1483\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.4464 - val_loss: 203.9113\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.7229 - val_loss: 201.9244\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.3144 - val_loss: 202.2624\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 169.2137 - val_loss: 196.9330\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.8132 - val_loss: 196.8517\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.6250 - val_loss: 191.2339\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.5606 - val_loss: 190.6669\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.6889 - val_loss: 177.0386\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.5424 - val_loss: 173.7636\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.1285 - val_loss: 163.7303\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.6733 - val_loss: 159.4231\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.7554 - val_loss: 155.9695\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.0974 - val_loss: 146.7099\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.4091 - val_loss: 142.8781\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.5106 - val_loss: 138.5128\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.5489 - val_loss: 135.1959\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.5782 - val_loss: 133.1726\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.1945 - val_loss: 125.0840\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.9669 - val_loss: 121.7879\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.0599 - val_loss: 114.7593\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.4951 - val_loss: 114.4140\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.5854 - val_loss: 110.4653\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.6113 - val_loss: 105.0222\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.9130 - val_loss: 106.6612\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.4427 - val_loss: 101.5594\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.8956 - val_loss: 99.9276\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.2132 - val_loss: 99.1212\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.4544 - val_loss: 101.0682\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.6999 - val_loss: 95.6737\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 73.7818 - val_loss: 95.7981\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 72.8919 - val_loss: 93.4339\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 662us/step\n",
      "Mean Squared Error for Training Model #  32  :  97.90996882415844\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  33 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 92232.9141 - val_loss: 44885.6211\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 26706.6152 - val_loss: 11403.4365\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 7332.3862 - val_loss: 4021.1880\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3002.7053 - val_loss: 2658.7502\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2315.1292 - val_loss: 2381.8765\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1849.6039 - val_loss: 1729.4902\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1449.7577 - val_loss: 1403.4274\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1164.2892 - val_loss: 1093.9369\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 897.7203 - val_loss: 829.1705\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 675.6970 - val_loss: 627.7477\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 512.4152 - val_loss: 486.8576\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 398.6472 - val_loss: 399.7516\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 324.2575 - val_loss: 332.3245\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 273.9272 - val_loss: 297.1578\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.9664 - val_loss: 274.5914\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 223.8830 - val_loss: 260.8619\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 215.9357 - val_loss: 250.8529\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.3439 - val_loss: 250.1822\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.1696 - val_loss: 243.1312\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 190.9222 - val_loss: 240.8583\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 186.8757 - val_loss: 235.6920\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 183.9170 - val_loss: 234.0204\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 182.5161 - val_loss: 231.8629\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.7922 - val_loss: 247.6447\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.6357 - val_loss: 231.8268\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 175.1604 - val_loss: 229.5704\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.6086 - val_loss: 234.7469\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 172.8310 - val_loss: 222.2462\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 171.1701 - val_loss: 219.9621\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 166.7742 - val_loss: 224.3058\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.3626 - val_loss: 234.6553\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.3238 - val_loss: 227.5611\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.8719 - val_loss: 216.4097\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.3027 - val_loss: 248.0563\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.1034 - val_loss: 226.3173\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.0258 - val_loss: 213.1624\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.8023 - val_loss: 209.4046\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.5684 - val_loss: 210.7603\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.1915 - val_loss: 204.2300\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.0191 - val_loss: 203.8298\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.9855 - val_loss: 203.3943\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.8818 - val_loss: 212.1389\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.7926 - val_loss: 200.0334\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.3486 - val_loss: 199.6448\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.7010 - val_loss: 197.9779\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.5760 - val_loss: 194.1414\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.9537 - val_loss: 192.9234\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.5840 - val_loss: 191.3594\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.7449 - val_loss: 203.7437\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.4975 - val_loss: 189.6207\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 554us/step\n",
      "Mean Squared Error for Training Model #  33  :  186.11690521627727\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  34 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1983.8306 - val_loss: 1254.5887\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1411.9274 - val_loss: 1044.6787\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1107.3536 - val_loss: 810.0620\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 933.6987 - val_loss: 693.5211\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 789.7095 - val_loss: 582.3128\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 682.3494 - val_loss: 494.4264\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 595.3757 - val_loss: 464.0697\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 517.6443 - val_loss: 391.2055\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 468.6912 - val_loss: 362.6353\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 422.1507 - val_loss: 339.8996\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 393.2024 - val_loss: 318.8636\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 364.8536 - val_loss: 310.6862\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 343.5030 - val_loss: 295.0024\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 324.3330 - val_loss: 288.2873\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 309.0236 - val_loss: 279.7692\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 294.3581 - val_loss: 272.0244\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 284.0971 - val_loss: 265.4545\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 274.3899 - val_loss: 267.8431\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 265.0384 - val_loss: 254.5076\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 252.6668 - val_loss: 249.5858\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 247.1094 - val_loss: 244.4482\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.8263 - val_loss: 240.0922\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 232.4568 - val_loss: 236.5815\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.8079 - val_loss: 231.6013\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.6102 - val_loss: 228.7268\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 216.5138 - val_loss: 226.7587\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 205.1507 - val_loss: 219.5374\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 200.1067 - val_loss: 215.3336\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.6314 - val_loss: 219.2809\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.5274 - val_loss: 208.3699\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.7769 - val_loss: 205.0079\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 184.6234 - val_loss: 206.1024\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 183.6686 - val_loss: 196.8480\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.1731 - val_loss: 197.5640\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.6038 - val_loss: 189.4759\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 167.6683 - val_loss: 195.5315\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.8497 - val_loss: 186.5972\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.2302 - val_loss: 183.9647\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.9512 - val_loss: 175.4842\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.5439 - val_loss: 171.3089\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.5265 - val_loss: 170.4108\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.7701 - val_loss: 169.0455\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.3250 - val_loss: 170.5779\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.0438 - val_loss: 160.4057\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.4166 - val_loss: 158.1863\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.8501 - val_loss: 155.1822\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.0365 - val_loss: 155.0535\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.3073 - val_loss: 153.1781\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.9739 - val_loss: 151.0635\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.2633 - val_loss: 154.1232\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 668us/step\n",
      "Mean Squared Error for Training Model #  34  :  136.27357844651408\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  35 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4400.1689 - val_loss: 3208.2886\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 2685.0520 - val_loss: 2177.5469\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2003.0046 - val_loss: 1654.5239\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1555.5984 - val_loss: 1365.3419\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1269.2657 - val_loss: 1148.8937\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1068.4539 - val_loss: 998.2958\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 918.5321 - val_loss: 864.1295\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 796.5450 - val_loss: 775.1871\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 706.4020 - val_loss: 699.2437\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 625.0839 - val_loss: 623.2803\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 561.2065 - val_loss: 574.5613\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 503.8134 - val_loss: 520.9195\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 454.4560 - val_loss: 481.4577\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 415.0383 - val_loss: 447.5096\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 379.7133 - val_loss: 413.5224\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 349.7367 - val_loss: 390.1506\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 322.1159 - val_loss: 364.5564\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 299.7145 - val_loss: 345.8731\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 279.5178 - val_loss: 325.9667\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.2081 - val_loss: 312.2307\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 248.0291 - val_loss: 298.6903\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.5333 - val_loss: 284.5397\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.3586 - val_loss: 276.2699\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 212.6095 - val_loss: 263.1306\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 204.2497 - val_loss: 261.2878\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.4716 - val_loss: 246.8399\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 190.7767 - val_loss: 243.0555\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 184.1037 - val_loss: 235.9388\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.3749 - val_loss: 230.4804\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 175.7092 - val_loss: 223.8922\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.9206 - val_loss: 229.6706\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.5619 - val_loss: 215.6814\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.8864 - val_loss: 212.5344\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.0239 - val_loss: 208.0696\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.4095 - val_loss: 210.3898\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.7193 - val_loss: 201.6308\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.3863 - val_loss: 200.0001\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.7087 - val_loss: 197.0385\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.5643 - val_loss: 193.2649\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.3859 - val_loss: 191.2825\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.0241 - val_loss: 190.0482\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.2299 - val_loss: 186.2067\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 140.7124 - val_loss: 184.9108\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.7904 - val_loss: 182.0269\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.7085 - val_loss: 180.2504\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.0152 - val_loss: 180.0105\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.2519 - val_loss: 181.3951\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.5597 - val_loss: 174.8468\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.3089 - val_loss: 175.3713\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.4071 - val_loss: 171.7648\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 661us/step\n",
      "Mean Squared Error for Training Model #  35  :  153.57463808458894\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  36 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2297.5920 - val_loss: 592.7595\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 624.2289 - val_loss: 619.6643\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 476.0428 - val_loss: 494.2723\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 406.6232 - val_loss: 428.4410\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 358.0073 - val_loss: 390.2152\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 336.6171 - val_loss: 366.3049\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 320.3120 - val_loss: 351.9352\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 309.5557 - val_loss: 337.3675\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 299.5677 - val_loss: 327.6062\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 291.6714 - val_loss: 319.9816\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 285.5944 - val_loss: 314.0625\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 284.6920 - val_loss: 311.7151\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 279.4865 - val_loss: 303.0883\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 272.3014 - val_loss: 299.8474\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 266.3889 - val_loss: 287.0632\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 257.3349 - val_loss: 276.9460\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.9327 - val_loss: 244.6261\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 226.7552 - val_loss: 236.3978\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 211.3337 - val_loss: 228.3772\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.0516 - val_loss: 219.8470\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 191.8910 - val_loss: 207.1322\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.1979 - val_loss: 198.1560\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 166.9199 - val_loss: 188.3719\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.2721 - val_loss: 178.7291\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.2705 - val_loss: 167.3376\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.7314 - val_loss: 158.1165\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.5774 - val_loss: 148.5536\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.7243 - val_loss: 143.3425\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.0069 - val_loss: 137.8260\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.4485 - val_loss: 135.1586\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.1346 - val_loss: 130.3223\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.3685 - val_loss: 127.4096\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.0794 - val_loss: 126.8809\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.8318 - val_loss: 127.8527\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.7525 - val_loss: 121.4464\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.0828 - val_loss: 120.6327\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.7664 - val_loss: 117.3650\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.9615 - val_loss: 117.3973\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.0912 - val_loss: 116.7214\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.3696 - val_loss: 113.1598\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.6022 - val_loss: 112.4787\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.9848 - val_loss: 111.1285\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.2605 - val_loss: 110.8473\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.6764 - val_loss: 109.7926\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.9720 - val_loss: 121.0187\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.6228 - val_loss: 114.4805\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.8639 - val_loss: 111.4199\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.4863 - val_loss: 109.8037\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.0122 - val_loss: 109.0861\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.1932 - val_loss: 104.8413\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 666us/step\n",
      "Mean Squared Error for Training Model #  36  :  99.13541172451397\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  37 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2251.6157 - val_loss: 621.2455\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 511.7563 - val_loss: 601.4006\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 454.1790 - val_loss: 471.9187\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 374.1353 - val_loss: 413.5264\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 325.1060 - val_loss: 368.5278\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 288.9340 - val_loss: 346.5861\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 268.6711 - val_loss: 323.1258\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 247.5841 - val_loss: 304.1139\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 231.3153 - val_loss: 292.1443\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 221.1902 - val_loss: 276.6178\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 211.5272 - val_loss: 265.6084\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 201.9983 - val_loss: 254.5249\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.9913 - val_loss: 247.3744\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 189.4250 - val_loss: 239.3776\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 182.3347 - val_loss: 232.2059\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.3590 - val_loss: 223.0175\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 167.2796 - val_loss: 216.0889\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.7893 - val_loss: 210.4002\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.4877 - val_loss: 202.6618\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.1020 - val_loss: 194.7987\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.8898 - val_loss: 190.5209\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.7896 - val_loss: 188.0858\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.8400 - val_loss: 182.9285\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.8659 - val_loss: 179.7964\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.1285 - val_loss: 175.1875\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.3669 - val_loss: 173.3021\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.2467 - val_loss: 169.4749\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.5752 - val_loss: 167.6838\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.9552 - val_loss: 168.1954\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.8326 - val_loss: 167.6807\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.7047 - val_loss: 167.5266\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.2331 - val_loss: 163.2480\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.3964 - val_loss: 160.6213\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 124.7148 - val_loss: 159.0855\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.8576 - val_loss: 156.3272\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.6201 - val_loss: 156.8445\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.7081 - val_loss: 157.2945\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.4005 - val_loss: 157.8552\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.5893 - val_loss: 159.0778\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.9711 - val_loss: 153.9258\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.3680 - val_loss: 153.7675\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.9470 - val_loss: 149.7284\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.6672 - val_loss: 149.3953\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.6499 - val_loss: 146.9644\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.3449 - val_loss: 145.4551\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.8270 - val_loss: 144.4312\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.8803 - val_loss: 143.2617\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.1682 - val_loss: 142.1470\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.0713 - val_loss: 146.3553\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.4739 - val_loss: 146.9961\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 668us/step\n",
      "Mean Squared Error for Training Model #  37  :  125.8212756646204\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  38 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 10183.0664 - val_loss: 5207.9272\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2890.5261 - val_loss: 1280.5431\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 817.7289 - val_loss: 592.2747\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 602.3356 - val_loss: 588.2377\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 585.0717 - val_loss: 564.5648\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 560.9459 - val_loss: 549.5732\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 546.2708 - val_loss: 538.0392\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 532.0494 - val_loss: 525.4118\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 518.5809 - val_loss: 513.6030\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 505.0735 - val_loss: 502.8734\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 492.6971 - val_loss: 492.0767\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 480.1371 - val_loss: 479.6649\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 467.4746 - val_loss: 469.1816\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 456.6765 - val_loss: 459.3387\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 444.2183 - val_loss: 449.9587\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 433.8116 - val_loss: 439.9731\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 422.5923 - val_loss: 430.6434\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 412.3258 - val_loss: 422.0762\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 402.7938 - val_loss: 413.6186\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 393.4725 - val_loss: 405.2239\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 385.0778 - val_loss: 397.9074\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 376.1768 - val_loss: 390.4912\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 368.3535 - val_loss: 382.9111\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 360.4942 - val_loss: 376.8227\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 352.3894 - val_loss: 369.5148\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 345.5949 - val_loss: 363.0325\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 338.1874 - val_loss: 357.6440\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 331.6686 - val_loss: 351.5666\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 325.4330 - val_loss: 345.5228\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 319.6043 - val_loss: 339.7438\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 313.2157 - val_loss: 335.7366\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 307.3418 - val_loss: 329.2588\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 301.8911 - val_loss: 324.3293\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 296.5924 - val_loss: 321.4875\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 290.3809 - val_loss: 314.5184\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 286.9450 - val_loss: 310.7593\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 281.0339 - val_loss: 305.3775\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 275.6160 - val_loss: 301.8319\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 271.4467 - val_loss: 297.0379\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 266.3216 - val_loss: 293.7441\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.9390 - val_loss: 288.8516\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 258.0542 - val_loss: 284.5394\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 253.6514 - val_loss: 280.8337\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 249.3929 - val_loss: 277.0561\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.1445 - val_loss: 273.9663\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.3488 - val_loss: 269.7385\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.6670 - val_loss: 265.5117\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 233.4547 - val_loss: 261.0821\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 227.5634 - val_loss: 256.8519\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.6295 - val_loss: 253.2399\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 668us/step\n",
      "Mean Squared Error for Training Model #  38  :  238.04630737015168\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  39 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 29583.6367 - val_loss: 12005.6260\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5613.2466 - val_loss: 907.9888\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 661.3531 - val_loss: 731.4326\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 580.0547 - val_loss: 430.9975\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 380.8886 - val_loss: 388.2431\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 345.4764 - val_loss: 361.1212\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 317.2328 - val_loss: 345.5445\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 297.7229 - val_loss: 331.7697\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 281.3853 - val_loss: 317.3148\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 268.0976 - val_loss: 306.2269\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.0121 - val_loss: 294.7450\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.1822 - val_loss: 286.1609\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 234.4056 - val_loss: 277.4295\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 225.8442 - val_loss: 269.5404\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.5932 - val_loss: 262.1681\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 211.7922 - val_loss: 255.1504\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 206.0497 - val_loss: 248.9662\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.9199 - val_loss: 243.5916\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.6280 - val_loss: 241.2481\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 189.6409 - val_loss: 235.3274\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 186.2189 - val_loss: 231.7665\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 182.1058 - val_loss: 228.8304\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.9713 - val_loss: 226.6192\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.5891 - val_loss: 222.8244\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.9470 - val_loss: 219.2455\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 171.9705 - val_loss: 217.0408\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 169.3627 - val_loss: 215.6349\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 167.7814 - val_loss: 213.3508\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.5597 - val_loss: 210.9955\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.4778 - val_loss: 210.4261\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.8230 - val_loss: 209.4981\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.8069 - val_loss: 205.8759\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.8311 - val_loss: 205.7108\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.6655 - val_loss: 203.5180\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.2235 - val_loss: 203.0459\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.6107 - val_loss: 201.8696\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.9510 - val_loss: 198.7972\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.2330 - val_loss: 198.8652\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.7477 - val_loss: 197.8697\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.9404 - val_loss: 195.4566\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.2887 - val_loss: 195.5375\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.6212 - val_loss: 193.2895\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.3288 - val_loss: 193.3044\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.4417 - val_loss: 191.4717\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.6164 - val_loss: 190.4505\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.7804 - val_loss: 189.5632\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.2213 - val_loss: 187.8003\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.4643 - val_loss: 190.3683\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.1157 - val_loss: 186.6292\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.7441 - val_loss: 186.1647\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 554us/step\n",
      "Mean Squared Error for Training Model #  39  :  180.05283427142572\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  40 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 269546.6250 - val_loss: 139402.8594\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80746.8281 - val_loss: 35476.3750\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 21515.3613 - val_loss: 10292.8330\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 6051.1167 - val_loss: 2627.4277\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1478.1008 - val_loss: 629.6298\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 389.9417 - val_loss: 262.6316\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 226.0340 - val_loss: 229.6101\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 212.0121 - val_loss: 230.4431\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 211.0445 - val_loss: 228.5571\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 209.5738 - val_loss: 227.3676\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.4241 - val_loss: 226.7217\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 207.4612 - val_loss: 225.8794\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 206.9661 - val_loss: 224.7953\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 205.9025 - val_loss: 223.9568\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 205.2262 - val_loss: 223.3642\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 204.1843 - val_loss: 222.4147\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.6516 - val_loss: 221.2887\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.2380 - val_loss: 220.8244\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.0461 - val_loss: 219.5998\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 201.2531 - val_loss: 218.8213\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 200.7302 - val_loss: 217.7498\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 200.2134 - val_loss: 217.2352\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 200.0257 - val_loss: 216.6217\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.5127 - val_loss: 215.6972\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.1267 - val_loss: 215.1897\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.7714 - val_loss: 214.4308\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.9480 - val_loss: 214.0721\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.1059 - val_loss: 212.9288\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 197.8514 - val_loss: 212.3833\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 197.5710 - val_loss: 211.7614\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 197.2099 - val_loss: 210.9481\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 197.0548 - val_loss: 210.5015\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.5643 - val_loss: 209.7235\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.2863 - val_loss: 209.3136\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.1090 - val_loss: 208.9052\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.0585 - val_loss: 208.5642\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 195.4947 - val_loss: 207.6036\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 195.2529 - val_loss: 207.4052\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 195.5614 - val_loss: 207.0300\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.7521 - val_loss: 206.2482\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.3855 - val_loss: 205.9647\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.4329 - val_loss: 206.0517\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.2191 - val_loss: 204.9351\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.7566 - val_loss: 204.8810\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.3950 - val_loss: 204.5392\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.3082 - val_loss: 204.0542\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.7417 - val_loss: 203.9356\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.3283 - val_loss: 203.6358\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.2861 - val_loss: 203.1647\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.2184 - val_loss: 202.8728\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "Mean Squared Error for Training Model #  40  :  198.84253553878463\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  41 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3298.0371 - val_loss: 2197.3350\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2040.9321 - val_loss: 1589.0278\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1325.1406 - val_loss: 1000.0665\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 946.5413 - val_loss: 823.3177\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 780.6558 - val_loss: 703.5453\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 684.0454 - val_loss: 629.7349\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 602.4720 - val_loss: 561.9824\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 535.5612 - val_loss: 504.5107\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 472.4364 - val_loss: 454.1283\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 423.2846 - val_loss: 405.5852\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 377.6669 - val_loss: 390.5223\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 339.1893 - val_loss: 334.5369\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 303.0727 - val_loss: 309.4770\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 276.7648 - val_loss: 283.8178\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 259.2559 - val_loss: 271.3383\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.2996 - val_loss: 252.9331\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 221.1254 - val_loss: 247.4630\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 204.5389 - val_loss: 236.2832\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.9889 - val_loss: 245.0371\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.6336 - val_loss: 216.5420\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.8006 - val_loss: 203.1553\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 166.9894 - val_loss: 198.2043\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.0050 - val_loss: 190.3247\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.9406 - val_loss: 194.3472\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.6016 - val_loss: 185.2141\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.7091 - val_loss: 177.2461\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.8138 - val_loss: 175.0406\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.2850 - val_loss: 176.0965\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.2653 - val_loss: 167.1062\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.6636 - val_loss: 162.1573\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.0232 - val_loss: 158.5383\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.1182 - val_loss: 159.2190\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.8277 - val_loss: 161.6575\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.6341 - val_loss: 155.6943\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.6846 - val_loss: 159.2867\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.1121 - val_loss: 150.1465\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.1375 - val_loss: 147.1673\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.8459 - val_loss: 147.7491\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.4102 - val_loss: 149.3693\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.9583 - val_loss: 147.0162\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.7920 - val_loss: 141.8699\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.7804 - val_loss: 140.9351\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.3088 - val_loss: 139.6417\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 105.4397 - val_loss: 139.1103\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.1223 - val_loss: 137.6566\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.6427 - val_loss: 137.5198\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.5867 - val_loss: 137.2509\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.8908 - val_loss: 134.8454\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.8951 - val_loss: 135.4453\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.1335 - val_loss: 137.3992\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "Mean Squared Error for Training Model #  41  :  124.21945896386713\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  42 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2306.0645 - val_loss: 1191.0676\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1160.1985 - val_loss: 893.2775\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 800.0729 - val_loss: 665.4307\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 610.6379 - val_loss: 541.7193\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 508.5291 - val_loss: 466.4334\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 446.9767 - val_loss: 428.4787\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 409.6315 - val_loss: 399.0280\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 382.8503 - val_loss: 378.8098\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 360.9362 - val_loss: 356.0329\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 339.3201 - val_loss: 339.3635\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 323.4238 - val_loss: 326.2137\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 306.6667 - val_loss: 311.8508\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 293.4105 - val_loss: 297.4435\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 276.7682 - val_loss: 288.7623\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.8097 - val_loss: 273.7215\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.5505 - val_loss: 254.9881\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.9870 - val_loss: 231.4837\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.1699 - val_loss: 215.0825\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 171.3478 - val_loss: 189.6608\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.4549 - val_loss: 178.0377\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.8002 - val_loss: 166.5586\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.3415 - val_loss: 158.9376\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.7510 - val_loss: 153.5873\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.7620 - val_loss: 146.3249\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.5020 - val_loss: 142.1992\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.4347 - val_loss: 138.1323\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.0911 - val_loss: 131.8355\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.4461 - val_loss: 128.0522\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.2475 - val_loss: 128.6672\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.4313 - val_loss: 126.9525\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.1203 - val_loss: 121.6254\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.6822 - val_loss: 121.3323\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.4096 - val_loss: 120.0667\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.3193 - val_loss: 123.9669\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.7693 - val_loss: 127.1476\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.4872 - val_loss: 118.3051\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.5864 - val_loss: 123.1380\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.2740 - val_loss: 126.6325\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.4244 - val_loss: 142.1261\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.2189 - val_loss: 115.9683\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.3711 - val_loss: 116.7965\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.4671 - val_loss: 115.7449\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.1723 - val_loss: 114.8788\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.7618 - val_loss: 117.9481\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.0189 - val_loss: 119.9389\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.8241 - val_loss: 122.4179\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 85.3035 - val_loss: 112.2461\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.6166 - val_loss: 111.4868\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.2156 - val_loss: 117.7232\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.5463 - val_loss: 110.1982\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 557us/step\n",
      "Mean Squared Error for Training Model #  42  :  101.25615629085378\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  43 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7698.6592 - val_loss: 2795.5950\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1333.5392 - val_loss: 520.7241\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 364.0700 - val_loss: 355.5789\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 309.2182 - val_loss: 324.9227\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 279.9058 - val_loss: 301.6386\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.0266 - val_loss: 291.7641\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 251.9621 - val_loss: 283.4695\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.9994 - val_loss: 276.8085\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.0582 - val_loss: 271.7120\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 233.7767 - val_loss: 266.5476\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 228.8609 - val_loss: 261.7968\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 223.7981 - val_loss: 256.8394\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 219.5092 - val_loss: 252.2576\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 214.5875 - val_loss: 248.8886\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 210.8419 - val_loss: 244.2371\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 206.8103 - val_loss: 241.3965\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.4619 - val_loss: 237.3462\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.3991 - val_loss: 234.8298\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.9248 - val_loss: 231.0411\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 191.5285 - val_loss: 227.8944\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 188.3647 - val_loss: 224.9624\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 184.6998 - val_loss: 222.3138\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.8668 - val_loss: 219.3431\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.8329 - val_loss: 216.4879\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.5246 - val_loss: 213.2669\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 171.4193 - val_loss: 210.0908\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.0954 - val_loss: 207.3711\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.7827 - val_loss: 204.7345\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.6918 - val_loss: 201.7983\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.5817 - val_loss: 199.4303\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.6559 - val_loss: 197.4411\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.6285 - val_loss: 194.7949\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.0496 - val_loss: 193.7538\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.3677 - val_loss: 190.5908\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.8217 - val_loss: 189.0952\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.4743 - val_loss: 186.7479\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.6496 - val_loss: 184.8401\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.3260 - val_loss: 183.4911\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.6594 - val_loss: 181.4136\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.7900 - val_loss: 180.2946\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.4732 - val_loss: 178.7835\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.2426 - val_loss: 177.9601\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.0011 - val_loss: 176.1136\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.8587 - val_loss: 175.0549\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.5325 - val_loss: 173.6023\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.2688 - val_loss: 172.7943\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.3696 - val_loss: 171.8507\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.0177 - val_loss: 170.9039\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.6559 - val_loss: 169.6446\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.5288 - val_loss: 168.9090\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 662us/step\n",
      "Mean Squared Error for Training Model #  43  :  157.32182022469476\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  44 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3701.6536 - val_loss: 2546.5198\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1863.2753 - val_loss: 1035.6157\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 651.0005 - val_loss: 537.4440\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 473.7250 - val_loss: 553.3581\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 435.3630 - val_loss: 486.2451\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 408.1851 - val_loss: 467.7300\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 388.0907 - val_loss: 450.6978\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 369.2693 - val_loss: 431.1435\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 350.3936 - val_loss: 412.9438\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 334.7102 - val_loss: 399.3001\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 318.4447 - val_loss: 381.8088\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 303.0723 - val_loss: 368.5159\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 290.1145 - val_loss: 357.2505\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 273.3410 - val_loss: 336.8005\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 259.9216 - val_loss: 323.7053\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 247.4258 - val_loss: 311.8141\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.7581 - val_loss: 302.9464\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 225.3167 - val_loss: 290.7823\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 216.6058 - val_loss: 283.3077\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 209.7785 - val_loss: 274.4502\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 204.3802 - val_loss: 268.2269\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.4320 - val_loss: 264.6382\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.4379 - val_loss: 255.3160\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 188.6866 - val_loss: 254.0292\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 184.8430 - val_loss: 248.6820\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.9457 - val_loss: 243.1335\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.5945 - val_loss: 240.5107\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.1294 - val_loss: 236.7740\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 172.0021 - val_loss: 232.0065\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 169.0605 - val_loss: 229.8133\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.6482 - val_loss: 229.4523\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.9925 - val_loss: 223.6185\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.0654 - val_loss: 220.4160\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.7287 - val_loss: 217.9742\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.7032 - val_loss: 216.0224\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.1736 - val_loss: 215.7846\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.7036 - val_loss: 211.2947\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.1686 - val_loss: 210.0326\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.6252 - val_loss: 208.0936\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.7267 - val_loss: 204.6584\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.8230 - val_loss: 203.1978\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.2659 - val_loss: 202.4624\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.1199 - val_loss: 199.7559\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.9620 - val_loss: 197.1568\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.9999 - val_loss: 197.7839\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.7791 - val_loss: 193.8361\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.6140 - val_loss: 197.3720\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.3509 - val_loss: 191.1881\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 139.2211 - val_loss: 190.0794\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.4162 - val_loss: 194.0191\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "Mean Squared Error for Training Model #  44  :  177.21811350793757\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  45 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3290.5676 - val_loss: 1656.3920\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1140.6309 - val_loss: 692.3040\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 527.0288 - val_loss: 504.9482\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 427.5525 - val_loss: 449.0088\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 376.3734 - val_loss: 413.8639\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 338.5326 - val_loss: 385.1597\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 312.5331 - val_loss: 360.0768\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 293.9740 - val_loss: 340.6412\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 272.4636 - val_loss: 324.0020\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 254.8372 - val_loss: 308.9924\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.0248 - val_loss: 295.1601\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 229.1872 - val_loss: 284.5556\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 219.6354 - val_loss: 272.6794\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 209.2220 - val_loss: 264.7720\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 205.5048 - val_loss: 255.2560\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.9580 - val_loss: 243.3644\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 186.6978 - val_loss: 235.1662\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.8651 - val_loss: 229.1413\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.4804 - val_loss: 220.5589\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 171.4038 - val_loss: 216.1755\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 171.4165 - val_loss: 209.3035\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.7835 - val_loss: 206.0170\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.9220 - val_loss: 202.7143\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.1579 - val_loss: 200.8453\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.8449 - val_loss: 194.2398\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.6225 - val_loss: 185.9581\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.0511 - val_loss: 182.4812\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.3990 - val_loss: 179.8374\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.7886 - val_loss: 176.5788\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.1117 - val_loss: 172.0064\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.4364 - val_loss: 175.5973\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.7125 - val_loss: 170.3578\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.6252 - val_loss: 165.6940\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.6626 - val_loss: 166.1225\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.5760 - val_loss: 163.0235\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.7531 - val_loss: 157.8501\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.1080 - val_loss: 155.0228\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.1947 - val_loss: 154.3259\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.8003 - val_loss: 151.6133\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.9480 - val_loss: 150.0190\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.6007 - val_loss: 146.8344\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.5026 - val_loss: 145.4472\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.1596 - val_loss: 146.9601\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.8243 - val_loss: 142.2369\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.0056 - val_loss: 142.2258\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.4715 - val_loss: 148.2036\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.6306 - val_loss: 142.7947\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.0413 - val_loss: 139.7595\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.0997 - val_loss: 138.3373\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.4692 - val_loss: 138.1250\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 560us/step\n",
      "Mean Squared Error for Training Model #  45  :  142.37383087463448\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  46 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 145664.9375 - val_loss: 77795.9688\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 48157.1562 - val_loss: 20330.2305\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 14529.5742 - val_loss: 9596.1514\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 10281.1406 - val_loss: 8898.4424\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 9082.7822 - val_loss: 7538.5317\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 7790.9980 - val_loss: 6655.1025\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 6847.8574 - val_loss: 5875.1738\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 6009.1411 - val_loss: 5175.5508\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5255.3228 - val_loss: 4567.0093\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4615.1074 - val_loss: 4047.7354\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4093.0283 - val_loss: 3673.1399\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3659.0430 - val_loss: 3322.5444\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3306.8052 - val_loss: 3050.0884\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3020.0237 - val_loss: 2799.6792\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2759.2561 - val_loss: 2570.6902\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2530.8059 - val_loss: 2371.6584\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2328.3545 - val_loss: 2189.1992\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2128.4761 - val_loss: 2027.3044\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1974.0276 - val_loss: 1880.9738\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1817.1306 - val_loss: 1756.6163\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1683.5558 - val_loss: 1633.8165\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1563.6464 - val_loss: 1526.5486\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1451.8149 - val_loss: 1430.6498\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1351.9877 - val_loss: 1339.3499\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1261.1294 - val_loss: 1260.1820\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1172.9095 - val_loss: 1185.3351\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1098.8599 - val_loss: 1116.8046\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1029.4595 - val_loss: 1059.3763\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 971.9575 - val_loss: 998.3863\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 905.3745 - val_loss: 951.9202\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 853.5737 - val_loss: 903.8292\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 799.9606 - val_loss: 859.9899\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 754.3012 - val_loss: 820.0776\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 712.8317 - val_loss: 789.0307\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 672.0746 - val_loss: 752.5555\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 638.9744 - val_loss: 723.6142\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 605.9741 - val_loss: 693.2715\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 576.7903 - val_loss: 665.3674\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 554.2983 - val_loss: 641.1530\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 525.7928 - val_loss: 632.0742\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 507.5580 - val_loss: 601.0673\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 486.3869 - val_loss: 583.4009\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 467.2952 - val_loss: 566.0753\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 450.1698 - val_loss: 548.1780\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 435.0589 - val_loss: 534.8619\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 417.6910 - val_loss: 518.7715\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 405.8008 - val_loss: 506.0634\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 391.6742 - val_loss: 492.2909\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 379.5912 - val_loss: 480.4140\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 371.7709 - val_loss: 467.5385\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 663us/step\n",
      "Mean Squared Error for Training Model #  46  :  451.81838065659394\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  47 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 6893.2568 - val_loss: 2049.7007\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2355.9858 - val_loss: 2290.8308\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1913.2704 - val_loss: 1672.7122\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1615.5840 - val_loss: 1493.9792\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1395.1040 - val_loss: 1345.0304\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1222.1877 - val_loss: 1187.1001\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1080.4188 - val_loss: 1061.0770\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 946.9686 - val_loss: 949.1570\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 834.9255 - val_loss: 847.0704\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 737.7433 - val_loss: 749.3563\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 654.1869 - val_loss: 668.1274\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 576.9633 - val_loss: 608.4210\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 513.5934 - val_loss: 534.3658\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 461.7856 - val_loss: 483.7009\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 406.7258 - val_loss: 423.9410\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 358.2081 - val_loss: 374.8672\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 315.3839 - val_loss: 338.2460\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 285.5164 - val_loss: 303.8503\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.7654 - val_loss: 278.2757\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 231.9192 - val_loss: 261.8546\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 215.5207 - val_loss: 240.4345\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.1727 - val_loss: 227.5298\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 183.3949 - val_loss: 213.7876\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 171.8663 - val_loss: 206.0047\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.4673 - val_loss: 199.9948\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.5298 - val_loss: 188.2204\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.9792 - val_loss: 181.0739\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.9529 - val_loss: 175.3703\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.0029 - val_loss: 171.5022\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.5680 - val_loss: 167.5863\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.9926 - val_loss: 162.8803\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.5881 - val_loss: 161.1227\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.0124 - val_loss: 161.3952\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.9893 - val_loss: 158.0194\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.6333 - val_loss: 152.8975\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.8478 - val_loss: 153.2974\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.0490 - val_loss: 153.8812\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.0271 - val_loss: 150.1856\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.7133 - val_loss: 146.7349\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.8839 - val_loss: 157.4270\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.5977 - val_loss: 151.8954\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.3282 - val_loss: 148.6126\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.6134 - val_loss: 142.8839\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.6006 - val_loss: 141.8287\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.9244 - val_loss: 141.4965\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.7342 - val_loss: 140.5240\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.2164 - val_loss: 138.6020\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.2207 - val_loss: 137.9553\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 99.9161 - val_loss: 137.6662\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.1613 - val_loss: 136.5712\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 555us/step\n",
      "Mean Squared Error for Training Model #  47  :  123.71672228431636\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  48 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 24070.2578 - val_loss: 8338.9141\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3466.4109 - val_loss: 789.9679\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 949.9728 - val_loss: 1032.5250\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 831.7494 - val_loss: 623.0350\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 592.5838 - val_loss: 543.5588\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 509.7031 - val_loss: 496.4380\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 447.6098 - val_loss: 455.3858\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 405.9607 - val_loss: 422.4286\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 373.0617 - val_loss: 397.5627\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 347.2629 - val_loss: 370.2518\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 321.0564 - val_loss: 350.8760\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 300.6710 - val_loss: 328.4678\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 283.7035 - val_loss: 311.0062\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 267.1145 - val_loss: 293.1138\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 250.5982 - val_loss: 278.5461\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.3484 - val_loss: 267.2230\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 227.2987 - val_loss: 251.3320\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 212.3683 - val_loss: 240.9456\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.6558 - val_loss: 230.5037\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.5743 - val_loss: 220.6052\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.4831 - val_loss: 214.2188\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.5194 - val_loss: 204.6364\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.6017 - val_loss: 199.9248\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.6538 - val_loss: 192.8752\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.8189 - val_loss: 187.8468\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.5984 - val_loss: 181.4846\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.1325 - val_loss: 178.5491\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.7778 - val_loss: 173.1362\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.1025 - val_loss: 169.8624\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.8116 - val_loss: 165.2062\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.7684 - val_loss: 162.4902\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.5770 - val_loss: 159.9922\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.1512 - val_loss: 156.2039\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.4896 - val_loss: 155.0606\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.3976 - val_loss: 151.4588\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.3981 - val_loss: 151.4894\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.7804 - val_loss: 147.2894\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.3540 - val_loss: 149.6186\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.6276 - val_loss: 144.0828\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.6852 - val_loss: 142.8436\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.4412 - val_loss: 141.5636\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.3302 - val_loss: 140.9475\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.4577 - val_loss: 142.4805\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.0648 - val_loss: 138.5381\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.9622 - val_loss: 138.7688\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.9278 - val_loss: 137.8064\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 112.5654 - val_loss: 135.6368\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.1030 - val_loss: 134.7436\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.4783 - val_loss: 136.8503\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.6340 - val_loss: 133.8799\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "Mean Squared Error for Training Model #  48  :  128.45645860045775\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  49 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1240.4717 - val_loss: 862.7788\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 651.6194 - val_loss: 547.0883\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 394.8038 - val_loss: 380.8286\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 254.4427 - val_loss: 281.1815\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.7682 - val_loss: 247.6260\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.8083 - val_loss: 232.9878\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.8641 - val_loss: 222.1767\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.9898 - val_loss: 213.3975\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.2714 - val_loss: 204.4474\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.1986 - val_loss: 197.0243\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.1340 - val_loss: 190.0457\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.6215 - val_loss: 184.1808\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.4844 - val_loss: 185.0343\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.9954 - val_loss: 174.4683\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.8369 - val_loss: 176.3586\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.5009 - val_loss: 164.2296\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.9646 - val_loss: 161.6750\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.8168 - val_loss: 155.2687\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.5636 - val_loss: 151.1551\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.1245 - val_loss: 147.6226\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.9862 - val_loss: 145.9039\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.6735 - val_loss: 147.8287\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.4017 - val_loss: 142.4333\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.5721 - val_loss: 141.8492\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.8616 - val_loss: 142.1892\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.2032 - val_loss: 140.3880\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.1400 - val_loss: 141.1945\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.3980 - val_loss: 144.3919\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.3684 - val_loss: 141.2547\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.9496 - val_loss: 153.0412\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.2147 - val_loss: 137.9482\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.8988 - val_loss: 138.2643\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.4222 - val_loss: 139.2048\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.9982 - val_loss: 137.9906\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.3638 - val_loss: 141.0181\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.1762 - val_loss: 137.2505\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.6771 - val_loss: 160.3105\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.9745 - val_loss: 136.7467\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.0734 - val_loss: 142.8062\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.4564 - val_loss: 140.2749\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.9439 - val_loss: 135.5355\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.9957 - val_loss: 135.6940\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.6976 - val_loss: 143.4066\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.9096 - val_loss: 135.3255\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.5852 - val_loss: 139.8279\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.2257 - val_loss: 134.5309\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.0241 - val_loss: 134.7485\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.4951 - val_loss: 135.7055\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.3260 - val_loss: 144.2467\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.7051 - val_loss: 139.6806\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "Mean Squared Error for Training Model #  49  :  120.53578755725755\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  50 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2243.5427 - val_loss: 1664.9120\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1660.0334 - val_loss: 1275.9200\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1273.8984 - val_loss: 997.0718\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1006.8415 - val_loss: 786.1322\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 755.4241 - val_loss: 575.1743\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 566.0063 - val_loss: 458.0813\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 448.6881 - val_loss: 385.7774\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 377.6856 - val_loss: 342.1809\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 325.3294 - val_loss: 298.0363\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 285.3594 - val_loss: 274.1529\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 252.9319 - val_loss: 268.6586\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 242.6111 - val_loss: 239.0218\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 220.1658 - val_loss: 229.8772\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 206.1080 - val_loss: 216.1340\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.7943 - val_loss: 212.4310\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 186.6880 - val_loss: 199.9241\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 172.6662 - val_loss: 192.1689\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.8309 - val_loss: 181.7758\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.8055 - val_loss: 173.4691\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.5785 - val_loss: 166.6950\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.8632 - val_loss: 161.8545\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.0966 - val_loss: 155.3229\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.4046 - val_loss: 152.6733\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.8020 - val_loss: 151.8494\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.8054 - val_loss: 159.0265\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.6292 - val_loss: 139.7281\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.9991 - val_loss: 136.7847\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.6289 - val_loss: 135.0431\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.6669 - val_loss: 132.4825\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.6321 - val_loss: 129.9486\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.5869 - val_loss: 129.8998\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.2202 - val_loss: 125.8755\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.7505 - val_loss: 124.5963\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.6284 - val_loss: 122.7722\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.3780 - val_loss: 128.8973\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.4659 - val_loss: 124.5986\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.4935 - val_loss: 124.0183\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.1883 - val_loss: 117.3775\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.4824 - val_loss: 116.1923\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.5012 - val_loss: 115.4367\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.7041 - val_loss: 114.4518\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.0898 - val_loss: 113.5719\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.3759 - val_loss: 112.4982\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.3561 - val_loss: 111.8461\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.4389 - val_loss: 111.8256\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.5247 - val_loss: 110.4790\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.8450 - val_loss: 110.0283\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.2416 - val_loss: 110.9328\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.1210 - val_loss: 110.6915\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.4196 - val_loss: 109.5712\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "Mean Squared Error for Training Model #  50  :  102.20938219277124\n",
      "\n",
      "\n",
      "Total Execution Time :  0:01:37.700640\n"
     ]
    }
   ],
   "source": [
    "# Create the for loop to split the data, create, compile & fit model, evaluate & nake predictions, caluclate mse and store\n",
    "# in list_of_mse\n",
    "\n",
    "start_time = datetime.now() # Starting time of the for loop execution\n",
    "\n",
    "for i in range(50) :\n",
    "    # Split the data into train and test set\n",
    "    data_split()\n",
    "    \n",
    "    # Create and compile the regression model using the function regression_model as defined in TASK 1\n",
    "    model = regression_model()\n",
    "\n",
    "    # Fit the model on the train set\n",
    "    print('\\n\\n\\nTraining Model # ' , i+1 , '\\n\\n') # Print the Model Number that is being trained\n",
    "    model.fit(X_train, Y_train, validation_split=0.3, epochs=50)\n",
    "    print('\\n')\n",
    "    \n",
    "    # Make prediction on the test set\n",
    "    Y_predicted = predict()\n",
    "    \n",
    "    # Calculate the mean square error\n",
    "    mse = calculate_mse()\n",
    "    print('Mean Squared Error for Training Model # ', i+1 , ' : ', mse)\n",
    "    \n",
    "    # Add the mse to the list_of_mse list\n",
    "    list_of_mse.append(mse)\n",
    "\n",
    "end_time = datetime.now() # Ending time of the for loop execution\n",
    "\n",
    "# Print time taken for fitting 50 models and calucating the Mean and Standard Deviation of MSE of 50 models\n",
    "print('\\n\\nTotal Execution Time : ' , format(end_time - start_time))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Mean of the MSE of 50 Models :  162.93970112839511\n",
      "Standard Deviation of MSE of 50 Models :  64.11036205554542\n"
     ]
    }
   ],
   "source": [
    "# Calculate the Mean of the MSE of 50 models\n",
    "mean_of_mse = stats.mean(list_of_mse)\n",
    "\n",
    "# Calculate the Standard Deviation of the MSE of 50 models\n",
    "std_of_mse = stats.stdev(list_of_mse)\n",
    "\n",
    "# Print the Mean and Standard Deviation of MSE of 50 models\n",
    "print('\\n\\nMean of the MSE of 50 Models : ' , str(mean_of_mse))\n",
    "print('Standard Deviation of MSE of 50 Models : ' , str(std_of_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = blue> END OF PART A </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = blue> PART B : BASELINE MODEL WITH NORMALIZED FEATURES </font>\n",
    "\n",
    "\n",
    "In this part, all the tasks from <b>PART A</b> are performed, but this time the values for the features (X) will be normalized using the formula `X - µ / σ`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = darkorange>Task 1 : Train and Test the Baseline Model with Normalized Features</font>\n",
    "\n",
    "In order to train and test the the baseline model with normalized features, the following steps are performed :\n",
    "<ol>\n",
    "    <li>Normalize the features (X)</li>\n",
    "    <li>Randomly split the data into <i>X_train, X_test, Y_train, Y_test</i> sets</li>\n",
    "    <li>Train the model using <i>X_train, Y_train</i> using <b>50</b> epochs</li>\n",
    "    <li>Get the predictions on the <i>X_test</i> set </li>\n",
    "    <li>Compute the <b>mean squared error</b> on the test set using sklearn</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #2980B9> Step 1 : Normalize the features (X) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X - X.mean() / X.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #2980B9> Step 2 : Randomly split the data into <i>X_train, X_test, Y_train, Y_test </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X_train, X_test, Y_train and Y_test sets\n",
    "X_train, X_test, Y_train, Y_test = data_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #2980B9> Step 3 : Train the model using <i>X_train, Y_train</i> using 50 epochs </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note </b> : Since the regression model has already been created using `def regression_model` in <b>TASK 1</b> and there are no changes being made to it, hence there is no need to write the code for the model again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 71599.0000 - val_loss: 9961.9092\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3447.7935 - val_loss: 2926.8191\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2773.4614 - val_loss: 1582.9602\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1443.4080 - val_loss: 1169.0935\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1322.6135 - val_loss: 1090.3793\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1231.7902 - val_loss: 1051.2170\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1184.8513 - val_loss: 998.7578\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1130.1832 - val_loss: 956.3284\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1081.8765 - val_loss: 913.8438\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1032.3518 - val_loss: 872.6879\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 986.9490 - val_loss: 831.3547\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 945.6859 - val_loss: 794.1354\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 903.8228 - val_loss: 761.2108\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 861.4119 - val_loss: 723.5344\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 819.8643 - val_loss: 690.8657\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 780.3776 - val_loss: 659.3807\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 747.3812 - val_loss: 628.6743\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 713.1007 - val_loss: 600.9496\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 680.1654 - val_loss: 573.3167\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 652.1350 - val_loss: 549.5700\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 622.1227 - val_loss: 523.7875\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 591.7562 - val_loss: 495.5400\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 564.0289 - val_loss: 470.7296\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 540.2562 - val_loss: 450.4245\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 516.1694 - val_loss: 431.1137\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 495.6583 - val_loss: 414.7199\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 476.1293 - val_loss: 397.1927\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 458.5506 - val_loss: 380.5374\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 436.2047 - val_loss: 365.8590\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 419.6152 - val_loss: 344.9660\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 401.2901 - val_loss: 326.7364\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 383.3065 - val_loss: 310.3808\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 364.8557 - val_loss: 293.9581\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 345.5765 - val_loss: 278.8493\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 333.8691 - val_loss: 267.8181\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 313.6179 - val_loss: 249.4146\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 295.7372 - val_loss: 238.5360\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 285.3499 - val_loss: 228.7691\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 277.1070 - val_loss: 223.2774\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 269.8367 - val_loss: 223.1687\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 266.1177 - val_loss: 211.5069\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 251.8172 - val_loss: 204.7786\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.6520 - val_loss: 200.4052\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 240.4472 - val_loss: 195.3210\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 233.9459 - val_loss: 190.0377\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 227.3252 - val_loss: 185.5511\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 223.1718 - val_loss: 181.8540\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 216.7678 - val_loss: 178.4536\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 211.6849 - val_loss: 173.8850\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 207.5294 - val_loss: 170.3211\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f8bb9d9a80>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = regression_model()\n",
    "\n",
    "# Fit the model on the train set\n",
    "model.fit(X_train, Y_train, validation_split=0.3, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #2980B9> Step 4 : Get the predictions on the X_test </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 554us/step\n"
     ]
    }
   ],
   "source": [
    "# Store the predictions in a variable Y_Predicted\n",
    "Y_predicted = predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #2980B9> Step 5 : Compute the <i>mean squared error</i> on the test set using sklearn </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error (MSE) of the Baseline Model with Normalized Features is :  178.0724631579991\n"
     ]
    }
   ],
   "source": [
    "# Calculate the mean square error\n",
    "\n",
    "mse = calculate_mse()\n",
    "print('Mean Square Error (MSE) of the Baseline Model with Normalized Features is : ' , str(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = darkorange>Task 2 : Create 50 Models and Calculate the <i>µ</i> & <i>σ</i> of their MSE Errors with new Features</font>\n",
    "\n",
    "In order to train 50 models with new features (normalized features) and calulate the mean (µ) and standard deviation (σ) of their mean square errors (MSE) the following steps are performed :\n",
    "<ol>\n",
    "    <li>Create an empty list <code>list_of_mse</code> to store the mean square error of each of the models</li>\n",
    "    <li>Define a for loop and perform each of the following steps in the loop</li>\n",
    "        <ol>\n",
    "        <li>Randomly split the data into <i>X_train, X_test, Y_train, Y_test</i> sets</li>\n",
    "        <li>Train the model using <i>X_train, Y_train</i> using <b>50</b> epochs</li>\n",
    "        <li>Evaluate the model using <i>X_test, Y_test</i></li>\n",
    "        <li>Get the predictions on the <i>X_test</i> set </li>\n",
    "        <li>Compute the <b>mean squared error</b> on the test set using sklearn</li>\n",
    "    </ol>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #2980B9> Step 1 : Creating the <i>list_of_mse</i> list</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the empty lists\n",
    "list_of_mse = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #2980B9> Step 2 : Creating 50 Models, Calculating The MSE for Each and <i>µ</i> & <i>σ</i> of the 50 MSE Values in <i>list_of_mse</i> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Training Model #  1 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 68762.5469 - val_loss: 42924.5547\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 28165.8008 - val_loss: 18694.4316\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 11448.7754 - val_loss: 7116.5327\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4014.8865 - val_loss: 2595.6309\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1541.9781 - val_loss: 1137.8036\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 882.5629 - val_loss: 785.2267\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 733.6618 - val_loss: 669.8439\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 645.4706 - val_loss: 592.0771\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 567.1196 - val_loss: 528.7433\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 509.8582 - val_loss: 484.3440\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 465.7439 - val_loss: 452.0591\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 433.7809 - val_loss: 421.3388\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 407.1108 - val_loss: 399.7225\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 382.5721 - val_loss: 386.7726\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 363.4709 - val_loss: 373.4099\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 347.0348 - val_loss: 356.8986\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 332.7333 - val_loss: 348.3238\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 318.1578 - val_loss: 334.8335\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 305.7487 - val_loss: 324.8816\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 294.6763 - val_loss: 310.7823\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 283.5677 - val_loss: 302.4994\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 273.1996 - val_loss: 291.5015\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 263.7787 - val_loss: 288.0447\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 253.7081 - val_loss: 272.7033\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.3260 - val_loss: 266.7757\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.1933 - val_loss: 253.5410\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 227.0770 - val_loss: 245.3745\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 219.7529 - val_loss: 237.8103\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 213.3969 - val_loss: 229.5193\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 206.4140 - val_loss: 221.5381\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 201.3350 - val_loss: 211.7370\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 195.7982 - val_loss: 208.2086\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 190.8781 - val_loss: 200.6701\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 186.1906 - val_loss: 194.4420\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 182.2631 - val_loss: 187.9057\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 178.5037 - val_loss: 184.0295\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.6334 - val_loss: 178.2287\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 171.4206 - val_loss: 173.1390\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.3272 - val_loss: 168.9323\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.4265 - val_loss: 165.5433\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 162.8198 - val_loss: 162.1048\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.5680 - val_loss: 158.4071\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.5586 - val_loss: 156.5799\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.4920 - val_loss: 153.7258\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.0317 - val_loss: 150.7357\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.5125 - val_loss: 150.3483\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.5875 - val_loss: 146.1405\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.5529 - val_loss: 146.0562\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.3567 - val_loss: 141.7038\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.6595 - val_loss: 142.6265\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 554us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  2 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 267201.2500 - val_loss: 179047.6250\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106513.0625 - val_loss: 64332.7656\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32226.8574 - val_loss: 18885.8242\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 7963.7285 - val_loss: 6106.9512\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3051.4868 - val_loss: 3876.5737\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2656.9807 - val_loss: 3604.0952\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2619.5476 - val_loss: 3563.4968\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2530.3799 - val_loss: 3549.4475\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2457.9028 - val_loss: 3505.7903\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2392.6584 - val_loss: 3418.2112\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2326.8789 - val_loss: 3359.8301\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2252.8972 - val_loss: 3232.6838\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2181.3467 - val_loss: 3133.0544\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2099.7419 - val_loss: 3031.4580\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2009.9502 - val_loss: 2959.3831\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1945.1897 - val_loss: 2874.9246\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1870.0255 - val_loss: 2753.9407\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1803.4604 - val_loss: 2661.0801\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1738.7692 - val_loss: 2606.2703\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1675.3018 - val_loss: 2508.6445\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1619.7887 - val_loss: 2397.3096\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1558.4641 - val_loss: 2379.5261\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1502.6821 - val_loss: 2273.8445\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1451.5067 - val_loss: 2194.9531\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1393.2985 - val_loss: 2143.4954\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1343.1553 - val_loss: 2070.3442\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1295.7284 - val_loss: 1973.1290\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1246.8634 - val_loss: 1929.4614\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1203.7209 - val_loss: 1852.1885\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1163.7462 - val_loss: 1786.9287\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1124.8984 - val_loss: 1741.6824\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1087.7413 - val_loss: 1673.7347\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1057.3060 - val_loss: 1608.5396\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1019.9540 - val_loss: 1584.4504\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 989.2622 - val_loss: 1515.3146\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 960.7224 - val_loss: 1466.7699\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 934.5455 - val_loss: 1413.1720\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 904.6842 - val_loss: 1376.2234\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 875.2555 - val_loss: 1346.1388\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 851.6074 - val_loss: 1293.5386\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 824.6382 - val_loss: 1241.4583\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 804.8641 - val_loss: 1207.6754\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 779.2643 - val_loss: 1177.1969\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 755.9148 - val_loss: 1125.3597\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 735.1846 - val_loss: 1081.9014\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 710.3398 - val_loss: 1067.1744\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 690.7296 - val_loss: 1014.9238\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 670.4720 - val_loss: 986.4454\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 652.0312 - val_loss: 946.3629\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 634.5160 - val_loss: 922.1142\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 668us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  3 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 66675.4844 - val_loss: 43412.9688\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 30932.5859 - val_loss: 18151.8418\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 11594.8662 - val_loss: 5562.7671\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3146.9473 - val_loss: 1737.5687\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1530.3351 - val_loss: 1403.7026\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1253.2939 - val_loss: 1135.9320\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1026.7437 - val_loss: 980.2902\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 889.1901 - val_loss: 851.1261\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 778.8227 - val_loss: 743.0466\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 682.8708 - val_loss: 662.6815\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 609.9133 - val_loss: 590.8669\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 550.8885 - val_loss: 536.6788\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 503.2899 - val_loss: 491.6812\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 462.9178 - val_loss: 455.1390\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 428.5174 - val_loss: 422.4593\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 399.3163 - val_loss: 395.6494\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 373.8664 - val_loss: 373.4690\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 354.8001 - val_loss: 353.7961\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 335.0777 - val_loss: 335.1407\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 315.5681 - val_loss: 321.9203\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 301.1096 - val_loss: 306.4433\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 288.1916 - val_loss: 296.4907\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 275.3074 - val_loss: 286.2117\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 263.2140 - val_loss: 276.2921\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 252.5029 - val_loss: 268.0450\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.8976 - val_loss: 258.0025\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 230.7027 - val_loss: 250.4623\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.4929 - val_loss: 243.6620\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 214.3353 - val_loss: 237.1899\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 207.9583 - val_loss: 231.0616\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.4244 - val_loss: 225.6293\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.3024 - val_loss: 220.1808\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 191.9572 - val_loss: 215.5820\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 188.2665 - val_loss: 211.3519\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 184.5979 - val_loss: 207.7468\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 182.7550 - val_loss: 204.0456\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.9576 - val_loss: 199.6262\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.0919 - val_loss: 195.4525\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.9228 - val_loss: 190.9743\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 167.0964 - val_loss: 187.1396\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.4953 - val_loss: 181.8687\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.4343 - val_loss: 178.3431\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.3691 - val_loss: 173.2786\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.1853 - val_loss: 170.7950\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.7055 - val_loss: 166.6809\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.3633 - val_loss: 164.1461\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.1953 - val_loss: 160.3561\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.1202 - val_loss: 158.3259\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.6230 - val_loss: 155.8288\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 134.3117 - val_loss: 154.4990\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  4 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 30801.1602 - val_loss: 11753.4434\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4648.2573 - val_loss: 639.0778\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 654.7680 - val_loss: 575.5672\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 647.7753 - val_loss: 320.0115\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 416.1641 - val_loss: 301.8287\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 362.4865 - val_loss: 267.7927\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 326.3083 - val_loss: 241.5271\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 303.2975 - val_loss: 229.9503\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 285.6923 - val_loss: 216.1987\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 271.2092 - val_loss: 208.1514\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 260.9540 - val_loss: 201.4514\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 252.7833 - val_loss: 195.5832\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.9971 - val_loss: 193.1569\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 240.0071 - val_loss: 186.7861\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 231.1825 - val_loss: 184.2572\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 227.3504 - val_loss: 179.4281\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 221.3723 - val_loss: 177.1305\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 216.6573 - val_loss: 174.4261\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 212.2835 - val_loss: 172.9305\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.7227 - val_loss: 171.6357\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 205.3999 - val_loss: 168.6276\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 201.9299 - val_loss: 166.8586\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 197.6411 - val_loss: 166.0899\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.2822 - val_loss: 163.4536\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 188.6731 - val_loss: 160.1181\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.0601 - val_loss: 158.3150\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.3223 - val_loss: 155.2073\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.8304 - val_loss: 154.0328\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.4831 - val_loss: 148.7655\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 167.3932 - val_loss: 145.6977\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.6482 - val_loss: 144.0743\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.1567 - val_loss: 140.7806\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.4644 - val_loss: 138.6665\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.6152 - val_loss: 137.0021\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.1721 - val_loss: 135.7533\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.0944 - val_loss: 134.0508\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.2859 - val_loss: 132.4946\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.4385 - val_loss: 130.7100\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.1424 - val_loss: 129.0207\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.8307 - val_loss: 128.0930\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.4966 - val_loss: 126.8165\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.1950 - val_loss: 126.4133\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.2600 - val_loss: 124.7014\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.8696 - val_loss: 123.8473\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.7712 - val_loss: 124.6776\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.6506 - val_loss: 124.1316\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.1436 - val_loss: 123.8656\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.1708 - val_loss: 120.9858\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.5354 - val_loss: 119.9351\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.4622 - val_loss: 121.5844\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 557us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  5 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2567.5459 - val_loss: 1817.5924\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1280.7802 - val_loss: 1051.4996\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 753.6863 - val_loss: 583.1190\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 492.8341 - val_loss: 420.8538\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 419.4022 - val_loss: 362.0934\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 371.5836 - val_loss: 332.2385\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 344.2010 - val_loss: 309.6112\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 320.7910 - val_loss: 289.1618\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 301.9667 - val_loss: 273.5260\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 285.2469 - val_loss: 262.0647\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 273.9064 - val_loss: 252.5549\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.2315 - val_loss: 244.2850\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 251.5545 - val_loss: 240.7102\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.0994 - val_loss: 230.4350\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.7117 - val_loss: 222.8180\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 229.7305 - val_loss: 218.3737\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 224.9870 - val_loss: 213.2319\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 220.0302 - val_loss: 208.1071\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 204.7193 - val_loss: 184.5658\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 189.8778 - val_loss: 184.8223\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.8505 - val_loss: 164.8681\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.5000 - val_loss: 156.8691\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.5111 - val_loss: 150.4363\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.7605 - val_loss: 157.8566\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.4346 - val_loss: 149.4301\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.4076 - val_loss: 139.0621\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.3887 - val_loss: 142.5182\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.1742 - val_loss: 139.9518\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.9910 - val_loss: 129.2513\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.0918 - val_loss: 132.0581\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.2628 - val_loss: 133.1877\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.9117 - val_loss: 125.3415\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.4779 - val_loss: 123.6562\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.6600 - val_loss: 120.0008\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.4177 - val_loss: 120.6906\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.4565 - val_loss: 118.7969\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.3992 - val_loss: 117.8963\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.3512 - val_loss: 135.6676\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.5015 - val_loss: 113.1662\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.5781 - val_loss: 111.8882\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.1637 - val_loss: 118.3079\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.2859 - val_loss: 115.1339\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.7905 - val_loss: 108.8039\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.7639 - val_loss: 112.0317\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.1171 - val_loss: 107.7313\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.9432 - val_loss: 105.1344\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.0312 - val_loss: 105.4395\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.7216 - val_loss: 106.0129\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.0596 - val_loss: 110.9954\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.4684 - val_loss: 106.9192\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 554us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  6 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2662.4583 - val_loss: 2140.3899\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1662.6450 - val_loss: 1438.5409\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1130.8901 - val_loss: 1026.1594\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 816.7965 - val_loss: 720.1288\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 582.3375 - val_loss: 519.3937\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 443.2464 - val_loss: 434.8380\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 397.6027 - val_loss: 393.8999\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 348.2220 - val_loss: 346.9588\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 321.4509 - val_loss: 344.5435\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 299.8676 - val_loss: 289.0239\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 272.5622 - val_loss: 267.2533\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.1366 - val_loss: 249.0135\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.5394 - val_loss: 243.6402\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.2758 - val_loss: 228.4756\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.5029 - val_loss: 199.7757\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.0629 - val_loss: 188.0451\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 191.9782 - val_loss: 183.6606\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.6859 - val_loss: 202.6921\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.9349 - val_loss: 181.3427\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 167.9514 - val_loss: 153.9116\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.7015 - val_loss: 146.8782\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.3261 - val_loss: 148.1259\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.6717 - val_loss: 142.6578\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.4110 - val_loss: 140.0273\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.3133 - val_loss: 135.0319\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.4163 - val_loss: 128.3768\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.3445 - val_loss: 127.9621\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.2915 - val_loss: 125.3237\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.6244 - val_loss: 121.0764\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.5449 - val_loss: 123.9909\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.0181 - val_loss: 118.0113\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.9975 - val_loss: 118.8140\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.7974 - val_loss: 114.4793\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.1901 - val_loss: 119.0701\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.6005 - val_loss: 113.1790\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.1117 - val_loss: 110.5156\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.8673 - val_loss: 108.9560\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.3512 - val_loss: 108.8503\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.6657 - val_loss: 137.9916\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.0168 - val_loss: 110.0352\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.0630 - val_loss: 114.7402\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.1068 - val_loss: 106.2754\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.8683 - val_loss: 105.0420\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.3852 - val_loss: 104.4017\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.2247 - val_loss: 103.6203\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.0819 - val_loss: 107.2649\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.8359 - val_loss: 132.9615\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.9646 - val_loss: 110.1724\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.6076 - val_loss: 108.9586\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.3441 - val_loss: 98.3025\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 668us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  7 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 576232.2500 - val_loss: 352157.5000\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244695.4375 - val_loss: 127533.9922\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82138.8828 - val_loss: 35794.4453\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 21949.4805 - val_loss: 10117.8721\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 8232.5146 - val_loss: 7412.6895\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 6157.0835 - val_loss: 5294.6416\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4338.8013 - val_loss: 3854.8481\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3482.6052 - val_loss: 3127.5352\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2805.3240 - val_loss: 2624.6335\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2354.6465 - val_loss: 2225.5605\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2039.2527 - val_loss: 1949.7422\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1806.3636 - val_loss: 1748.8336\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1623.9191 - val_loss: 1596.7012\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1482.7889 - val_loss: 1479.1389\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1363.8671 - val_loss: 1375.0760\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1273.8256 - val_loss: 1288.7294\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1184.4696 - val_loss: 1219.3403\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1118.3295 - val_loss: 1151.0107\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1055.8911 - val_loss: 1096.6271\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1006.1514 - val_loss: 1043.4476\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 944.3139 - val_loss: 1004.9688\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 910.2261 - val_loss: 956.6855\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 856.7244 - val_loss: 923.0309\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 818.2688 - val_loss: 884.5653\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 787.3093 - val_loss: 844.3192\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 749.4510 - val_loss: 819.6265\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 717.0494 - val_loss: 777.1492\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 685.5774 - val_loss: 754.2637\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 656.9966 - val_loss: 721.3375\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 629.8062 - val_loss: 695.0710\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 607.4713 - val_loss: 668.9455\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 580.1387 - val_loss: 647.5137\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 559.7488 - val_loss: 616.0292\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 537.3989 - val_loss: 601.9814\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 515.0610 - val_loss: 584.0870\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 499.7887 - val_loss: 553.3823\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 479.5631 - val_loss: 540.3185\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 463.5925 - val_loss: 537.2307\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 447.3170 - val_loss: 504.2053\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 429.3893 - val_loss: 491.2572\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 413.7136 - val_loss: 477.5217\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 399.9813 - val_loss: 463.6314\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 387.6148 - val_loss: 451.9084\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 376.3460 - val_loss: 449.1263\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 370.4974 - val_loss: 428.1529\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 354.1623 - val_loss: 438.9517\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 354.9016 - val_loss: 411.9779\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 349.9994 - val_loss: 406.0525\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 337.3646 - val_loss: 406.0229\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 329.7473 - val_loss: 397.3062\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 663us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  8 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 33561.2539 - val_loss: 17694.2422\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 11001.8623 - val_loss: 4131.0190\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2143.3770 - val_loss: 663.7064\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 555.4891 - val_loss: 525.5264\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 541.6411 - val_loss: 490.8274\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 485.7974 - val_loss: 447.6080\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 458.1683 - val_loss: 430.9994\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 438.4645 - val_loss: 416.2668\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 420.1761 - val_loss: 401.0826\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 401.3084 - val_loss: 388.9764\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 385.1603 - val_loss: 375.0312\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 370.1242 - val_loss: 362.4658\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 356.1747 - val_loss: 350.0935\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 341.8354 - val_loss: 336.7557\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 328.8629 - val_loss: 326.6112\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 313.9013 - val_loss: 313.8121\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 298.9703 - val_loss: 300.0929\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 284.4879 - val_loss: 288.3815\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 272.1535 - val_loss: 276.8849\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 260.0960 - val_loss: 266.7308\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 249.3469 - val_loss: 255.7089\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.0300 - val_loss: 245.2084\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 232.7690 - val_loss: 238.8540\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 225.0419 - val_loss: 230.4801\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 218.6975 - val_loss: 223.8470\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 211.9427 - val_loss: 218.5061\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 204.5056 - val_loss: 211.2852\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.8842 - val_loss: 203.6794\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 190.2468 - val_loss: 196.7359\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 182.2342 - val_loss: 189.8919\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 175.2130 - val_loss: 183.0684\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.8943 - val_loss: 176.8910\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.2087 - val_loss: 169.9359\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.1181 - val_loss: 165.5692\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.8213 - val_loss: 160.7877\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.5222 - val_loss: 156.2791\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.7912 - val_loss: 150.6017\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.4780 - val_loss: 146.8578\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.2269 - val_loss: 143.2148\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.7396 - val_loss: 139.1493\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.3849 - val_loss: 135.6861\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.6591 - val_loss: 132.6978\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.8366 - val_loss: 130.3044\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.5517 - val_loss: 126.3121\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.6546 - val_loss: 123.7049\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.4268 - val_loss: 121.3686\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.2616 - val_loss: 119.8770\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.3676 - val_loss: 116.1473\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.1461 - val_loss: 114.0344\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.0886 - val_loss: 111.6916\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  9 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 66236.4844 - val_loss: 38935.3633\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 23854.0938 - val_loss: 9274.6348\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4956.8945 - val_loss: 1686.1028\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1872.7538 - val_loss: 1667.7983\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1789.4951 - val_loss: 1487.8685\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1630.7811 - val_loss: 1349.9622\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1540.9177 - val_loss: 1283.2091\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1466.0040 - val_loss: 1226.2810\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1397.3722 - val_loss: 1166.6998\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1329.6576 - val_loss: 1106.4617\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1263.4443 - val_loss: 1055.2273\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1202.8939 - val_loss: 1005.3729\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1142.9030 - val_loss: 950.8027\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1087.2466 - val_loss: 905.6197\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1037.3345 - val_loss: 862.4763\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 985.4935 - val_loss: 819.7715\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 940.2811 - val_loss: 789.9372\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 892.2428 - val_loss: 748.4432\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 853.8276 - val_loss: 711.3843\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 810.6552 - val_loss: 684.5091\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 774.7669 - val_loss: 654.1559\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 738.9683 - val_loss: 627.9207\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 706.9861 - val_loss: 600.2350\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 674.7911 - val_loss: 572.3889\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 644.9922 - val_loss: 551.0474\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 618.9279 - val_loss: 528.1415\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 590.4060 - val_loss: 508.2296\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 565.8818 - val_loss: 492.3723\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 541.8499 - val_loss: 471.3644\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 519.8611 - val_loss: 451.8831\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 497.7237 - val_loss: 438.9650\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 478.7276 - val_loss: 420.6340\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 458.6147 - val_loss: 408.1416\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 440.3077 - val_loss: 393.1257\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 423.1373 - val_loss: 379.9810\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 407.3821 - val_loss: 367.5720\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 393.3500 - val_loss: 357.9626\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 376.3135 - val_loss: 344.1107\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 362.2067 - val_loss: 337.5864\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 349.9749 - val_loss: 324.4615\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 337.3234 - val_loss: 317.3990\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 324.4943 - val_loss: 307.1923\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 313.3190 - val_loss: 298.5970\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 302.3218 - val_loss: 290.5462\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 292.8111 - val_loss: 281.8399\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 282.3243 - val_loss: 276.5181\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 274.2608 - val_loss: 266.9211\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 264.1008 - val_loss: 260.6115\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 256.8180 - val_loss: 254.7184\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 247.7109 - val_loss: 246.0028\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 663us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  10 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 72278.0781 - val_loss: 42554.1250\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 28037.9062 - val_loss: 14358.2324\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 8589.6328 - val_loss: 4070.7632\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2888.9919 - val_loss: 2268.8184\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2181.3811 - val_loss: 2033.2885\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1922.9220 - val_loss: 1779.7556\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1690.6327 - val_loss: 1586.6312\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1501.3809 - val_loss: 1440.3168\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1356.1512 - val_loss: 1314.0826\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1226.6544 - val_loss: 1200.8893\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1116.8241 - val_loss: 1106.6111\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1023.5993 - val_loss: 1022.1060\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 940.3812 - val_loss: 948.0914\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 868.8374 - val_loss: 885.4288\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 804.8332 - val_loss: 830.8963\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 753.6696 - val_loss: 782.4055\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 708.2631 - val_loss: 738.7182\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 668.1066 - val_loss: 699.5553\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 631.5627 - val_loss: 663.7859\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 602.1101 - val_loss: 631.7292\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 571.5753 - val_loss: 601.6746\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 547.0524 - val_loss: 575.3530\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 525.7432 - val_loss: 552.3438\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 502.5432 - val_loss: 529.3608\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 483.7278 - val_loss: 509.6364\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 466.6220 - val_loss: 493.0197\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 449.0572 - val_loss: 475.7065\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 432.8285 - val_loss: 459.2474\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 417.9615 - val_loss: 443.2451\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 402.8326 - val_loss: 428.8536\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 388.9257 - val_loss: 413.6739\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 374.9863 - val_loss: 399.7836\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 362.1182 - val_loss: 386.5586\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 351.0136 - val_loss: 374.4179\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 338.5533 - val_loss: 364.6617\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 327.9481 - val_loss: 354.0010\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 316.8815 - val_loss: 344.2212\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 307.7231 - val_loss: 336.3421\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 298.1082 - val_loss: 330.4047\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 289.6818 - val_loss: 322.9127\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 281.9950 - val_loss: 316.2444\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 274.4969 - val_loss: 310.6235\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 267.7746 - val_loss: 305.1094\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 261.9435 - val_loss: 300.4927\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 256.2232 - val_loss: 295.7872\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 251.3425 - val_loss: 291.5001\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 246.5081 - val_loss: 286.9161\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 242.7020 - val_loss: 283.3329\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.5441 - val_loss: 279.2436\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 233.5960 - val_loss: 275.7949\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 668us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  11 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 117476.3125 - val_loss: 77764.2734\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 55310.3047 - val_loss: 33032.8438\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 22043.1953 - val_loss: 11886.0889\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 7565.1064 - val_loss: 3787.7769\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2380.7900 - val_loss: 1289.2091\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 949.6105 - val_loss: 744.7797\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 695.8112 - val_loss: 676.3377\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 665.6254 - val_loss: 667.0653\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 657.8210 - val_loss: 655.4608\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 645.8907 - val_loss: 644.6269\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 634.6070 - val_loss: 632.9808\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 623.1202 - val_loss: 621.6273\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 611.8156 - val_loss: 609.9703\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 600.3259 - val_loss: 598.5075\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 588.8036 - val_loss: 587.0238\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 577.5010 - val_loss: 575.5511\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 566.3949 - val_loss: 564.6677\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 554.7341 - val_loss: 552.8965\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 542.7917 - val_loss: 541.5125\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 531.0814 - val_loss: 530.7256\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 520.1315 - val_loss: 519.8676\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 509.4852 - val_loss: 509.6734\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 497.6996 - val_loss: 498.7460\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 487.5402 - val_loss: 488.5347\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 475.7659 - val_loss: 478.7027\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 465.8830 - val_loss: 469.2711\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 455.4001 - val_loss: 459.2787\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 445.4088 - val_loss: 449.8721\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 435.6967 - val_loss: 440.9690\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 426.5250 - val_loss: 430.9614\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 416.9699 - val_loss: 422.0952\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 407.3410 - val_loss: 412.5805\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 398.6094 - val_loss: 401.7801\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 390.0200 - val_loss: 391.6912\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 381.5058 - val_loss: 381.8617\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 374.9518 - val_loss: 372.5008\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 366.6396 - val_loss: 365.2501\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 359.4370 - val_loss: 358.2726\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 353.5940 - val_loss: 350.8284\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 347.4373 - val_loss: 345.2611\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 342.3785 - val_loss: 340.8218\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 337.0553 - val_loss: 334.9489\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 331.9557 - val_loss: 329.9867\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 328.1919 - val_loss: 325.7600\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 323.8689 - val_loss: 323.3797\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 318.3672 - val_loss: 316.8315\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 314.6369 - val_loss: 313.6541\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 310.3055 - val_loss: 309.0239\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 306.4199 - val_loss: 304.9342\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 303.4564 - val_loss: 302.1338\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 669us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  12 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 260458.9219 - val_loss: 196140.3750\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154202.4688 - val_loss: 117805.2812\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93944.1797 - val_loss: 74700.7734\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 61306.3867 - val_loss: 50375.8438\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 42155.4336 - val_loss: 35380.1250\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 29760.8770 - val_loss: 24645.9707\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 20410.3359 - val_loss: 16262.6514\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 13309.1035 - val_loss: 10148.8350\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 8286.3994 - val_loss: 5986.8047\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4857.5371 - val_loss: 3401.5605\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2843.6763 - val_loss: 1968.2074\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1766.9066 - val_loss: 1324.9423\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1309.5392 - val_loss: 1064.2283\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1120.2421 - val_loss: 974.5269\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1049.7947 - val_loss: 936.2186\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1009.1298 - val_loss: 910.7457\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 980.3328 - val_loss: 888.0773\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 953.8187 - val_loss: 863.9783\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 927.0479 - val_loss: 840.4520\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 902.0516 - val_loss: 818.3113\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 878.4057 - val_loss: 797.2307\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 854.6304 - val_loss: 777.3769\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 833.4854 - val_loss: 758.0291\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 812.0630 - val_loss: 737.7540\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 790.8466 - val_loss: 719.1716\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 770.9177 - val_loss: 701.9013\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 751.7582 - val_loss: 684.5249\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 733.0167 - val_loss: 668.0363\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 715.6577 - val_loss: 651.7247\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 696.8740 - val_loss: 636.1587\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 680.4637 - val_loss: 620.9087\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 664.7471 - val_loss: 605.9198\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 648.6090 - val_loss: 591.6523\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 633.3148 - val_loss: 577.5480\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 620.1523 - val_loss: 564.9426\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 604.4155 - val_loss: 551.7568\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 590.2601 - val_loss: 539.4802\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 576.6973 - val_loss: 527.3560\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 563.8795 - val_loss: 515.5229\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 551.0163 - val_loss: 504.0658\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 538.7520 - val_loss: 492.7645\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 527.3412 - val_loss: 481.8007\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 516.0527 - val_loss: 471.5712\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 504.4333 - val_loss: 460.9129\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 493.7379 - val_loss: 450.9282\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 483.0031 - val_loss: 441.6081\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 472.9966 - val_loss: 432.3749\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 463.1091 - val_loss: 423.5269\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 452.8357 - val_loss: 414.3868\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 444.1672 - val_loss: 405.1839\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 557us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  13 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2545.4026 - val_loss: 788.3724\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 743.7919 - val_loss: 802.2569\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 623.1130 - val_loss: 608.2307\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 529.2332 - val_loss: 543.0745\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 476.7333 - val_loss: 496.6615\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 442.3167 - val_loss: 458.2296\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 417.0715 - val_loss: 430.9325\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 389.0896 - val_loss: 411.1110\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 369.5824 - val_loss: 391.5841\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 349.5058 - val_loss: 373.7596\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 335.7087 - val_loss: 358.9507\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 319.6670 - val_loss: 342.8479\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 308.1067 - val_loss: 329.7930\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 293.1360 - val_loss: 312.3896\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 282.2414 - val_loss: 295.7225\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 270.3783 - val_loss: 283.5343\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 261.0078 - val_loss: 271.3043\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 250.8737 - val_loss: 261.6429\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.5178 - val_loss: 252.9424\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.0180 - val_loss: 244.3798\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 228.3056 - val_loss: 237.0637\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.5909 - val_loss: 229.9240\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 215.9538 - val_loss: 223.4471\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 209.8468 - val_loss: 217.5090\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 205.8600 - val_loss: 215.3181\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.1714 - val_loss: 209.8596\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 200.4814 - val_loss: 204.4945\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 189.1925 - val_loss: 196.4115\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.8206 - val_loss: 191.9972\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.6198 - val_loss: 185.1026\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.8731 - val_loss: 180.3538\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 171.6840 - val_loss: 175.6880\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 169.4158 - val_loss: 171.8053\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.1525 - val_loss: 166.2858\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.5639 - val_loss: 159.3746\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.2842 - val_loss: 154.6028\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.6439 - val_loss: 149.7012\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.7134 - val_loss: 145.2017\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.4237 - val_loss: 143.6897\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 140.9832 - val_loss: 139.9486\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.8694 - val_loss: 134.8735\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.1688 - val_loss: 129.3111\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.5918 - val_loss: 124.5565\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.4330 - val_loss: 118.1344\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.0167 - val_loss: 115.2353\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.9783 - val_loss: 110.8951\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.7724 - val_loss: 107.7994\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.2655 - val_loss: 103.6104\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.2431 - val_loss: 100.1583\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.5069 - val_loss: 101.1391\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  14 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 146845.2500 - val_loss: 125490.4219\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110698.7422 - val_loss: 94907.9453\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84172.7031 - val_loss: 72647.5234\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 64553.2695 - val_loss: 55807.3359\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 48665.2266 - val_loss: 40793.1641\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33695.4102 - val_loss: 26326.4824\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 20644.6543 - val_loss: 15558.8984\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 12063.6172 - val_loss: 8822.7979\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 6270.3662 - val_loss: 4105.2847\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2568.9385 - val_loss: 1492.9214\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 766.2728 - val_loss: 434.9533\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 302.5646 - val_loss: 352.3116\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 290.8425 - val_loss: 334.9838\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 271.7494 - val_loss: 329.6139\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 267.1531 - val_loss: 325.8077\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 263.1875 - val_loss: 319.1955\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 259.6020 - val_loss: 315.6702\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 256.2829 - val_loss: 310.9414\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 253.6129 - val_loss: 307.2072\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 250.8766 - val_loss: 304.8892\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 248.5072 - val_loss: 301.2447\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 246.0529 - val_loss: 298.6217\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.9435 - val_loss: 294.7480\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.9271 - val_loss: 293.3441\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.5598 - val_loss: 289.8483\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.4715 - val_loss: 288.1698\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.8688 - val_loss: 285.5765\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 233.8918 - val_loss: 283.0962\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 232.6145 - val_loss: 281.3314\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 230.7287 - val_loss: 278.4978\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 228.8076 - val_loss: 277.2333\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 227.8555 - val_loss: 275.6048\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 225.9920 - val_loss: 273.5612\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 224.6093 - val_loss: 271.6421\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.9924 - val_loss: 269.4152\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.1365 - val_loss: 267.6271\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 220.4712 - val_loss: 267.2656\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 219.3702 - val_loss: 265.3643\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 218.1434 - val_loss: 262.8931\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 216.6049 - val_loss: 262.0194\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 215.6568 - val_loss: 260.9951\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 214.4681 - val_loss: 258.8998\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 213.3029 - val_loss: 257.2017\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 212.3490 - val_loss: 256.6256\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 211.0657 - val_loss: 255.2513\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 210.6667 - val_loss: 253.5123\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.9979 - val_loss: 251.2468\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.4527 - val_loss: 250.6713\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 207.3652 - val_loss: 250.0452\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 206.2706 - val_loss: 247.8965\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 662us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  15 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 628.3222 - val_loss: 514.8029\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 473.0821 - val_loss: 427.0869\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 392.5354 - val_loss: 368.8445\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 345.0075 - val_loss: 334.0090\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 316.0708 - val_loss: 303.1678\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 294.5815 - val_loss: 277.6660\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 271.6276 - val_loss: 255.0006\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 260.0821 - val_loss: 243.6703\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 244.1590 - val_loss: 235.0893\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 234.0342 - val_loss: 222.8347\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 220.9760 - val_loss: 223.6116\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 212.3135 - val_loss: 206.3968\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 205.4972 - val_loss: 199.0695\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.9602 - val_loss: 202.8061\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.5671 - val_loss: 186.4892\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.5106 - val_loss: 182.6949\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.5193 - val_loss: 180.1867\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 171.6745 - val_loss: 177.2755\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 166.5152 - val_loss: 163.0472\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.7878 - val_loss: 174.1602\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.6516 - val_loss: 153.6510\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.6749 - val_loss: 148.8459\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.8661 - val_loss: 144.9094\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.4319 - val_loss: 138.7444\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.2606 - val_loss: 132.7679\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.2764 - val_loss: 133.5906\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.9764 - val_loss: 127.0195\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.4424 - val_loss: 122.7307\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.0173 - val_loss: 121.7141\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.6100 - val_loss: 120.4735\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.4745 - val_loss: 123.3849\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.4867 - val_loss: 117.2691\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.5464 - val_loss: 116.5003\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.5967 - val_loss: 113.7668\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.0885 - val_loss: 111.9752\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.0897 - val_loss: 112.1617\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.9320 - val_loss: 114.8319\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.3037 - val_loss: 113.0878\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.8770 - val_loss: 113.0397\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.6919 - val_loss: 111.4960\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.5800 - val_loss: 111.0161\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.4264 - val_loss: 112.2813\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.1453 - val_loss: 108.1631\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.8001 - val_loss: 111.9209\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 104.6636 - val_loss: 109.2392\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.6873 - val_loss: 109.4815\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.0782 - val_loss: 109.2473\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.5733 - val_loss: 106.6012\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.3260 - val_loss: 105.0161\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.9974 - val_loss: 105.1668\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 667us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  16 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3649.6274 - val_loss: 736.5876\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 592.1083 - val_loss: 448.1991\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 490.8216 - val_loss: 368.9997\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 399.2077 - val_loss: 319.0431\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 343.8868 - val_loss: 300.5126\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 318.4293 - val_loss: 282.2203\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 296.4916 - val_loss: 265.6797\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 279.8009 - val_loss: 252.6622\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 267.0701 - val_loss: 239.3554\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 253.9373 - val_loss: 227.8690\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.9111 - val_loss: 220.0634\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 232.7148 - val_loss: 207.2252\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 220.7643 - val_loss: 201.6477\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 210.6567 - val_loss: 190.1617\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.8429 - val_loss: 181.5089\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 189.1998 - val_loss: 172.0600\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.7839 - val_loss: 165.8164\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.7592 - val_loss: 159.6573\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 169.3424 - val_loss: 153.7180\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.2285 - val_loss: 152.0925\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.3879 - val_loss: 145.2494\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.1357 - val_loss: 143.3914\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.4446 - val_loss: 139.2610\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.9403 - val_loss: 138.0122\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.7043 - val_loss: 135.2274\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.2831 - val_loss: 129.0396\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.2136 - val_loss: 135.6343\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.0908 - val_loss: 122.8914\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.3190 - val_loss: 125.0143\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.6720 - val_loss: 117.1169\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.8449 - val_loss: 114.4652\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.4912 - val_loss: 122.2124\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.8671 - val_loss: 112.2791\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.2721 - val_loss: 110.1313\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.2957 - val_loss: 109.4518\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.5787 - val_loss: 111.5076\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.5444 - val_loss: 108.9751\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.9800 - val_loss: 104.7776\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.2826 - val_loss: 105.1060\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.7950 - val_loss: 110.9332\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.0623 - val_loss: 103.5846\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.9550 - val_loss: 104.1663\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.1456 - val_loss: 98.9349\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.4000 - val_loss: 103.4347\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.5783 - val_loss: 98.5588\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.8171 - val_loss: 98.4443\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.9187 - val_loss: 102.1436\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.4410 - val_loss: 96.6772\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.0900 - val_loss: 93.8347\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.8888 - val_loss: 98.2937\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 557us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  17 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2218.9541 - val_loss: 1224.0631\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1020.8554 - val_loss: 676.7980\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 606.0356 - val_loss: 525.5224\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 499.4464 - val_loss: 420.4640\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 410.8112 - val_loss: 378.2368\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 342.1932 - val_loss: 327.2001\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 312.1428 - val_loss: 301.2318\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 287.1609 - val_loss: 280.0237\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 263.0534 - val_loss: 255.0151\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.1575 - val_loss: 243.9113\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 232.9468 - val_loss: 226.6275\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 218.6720 - val_loss: 201.7035\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.3634 - val_loss: 189.0126\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.9169 - val_loss: 183.7724\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 167.7697 - val_loss: 164.5173\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.8590 - val_loss: 163.1618\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.9743 - val_loss: 146.0303\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.5062 - val_loss: 139.6188\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.3507 - val_loss: 138.1589\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.6202 - val_loss: 128.5004\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.7356 - val_loss: 127.8587\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.0549 - val_loss: 125.4881\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 123.2636 - val_loss: 132.9763\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.9516 - val_loss: 139.2966\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.7245 - val_loss: 148.5734\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.7500 - val_loss: 133.5753\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.9934 - val_loss: 119.5053\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.0241 - val_loss: 116.6092\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.4539 - val_loss: 142.1201\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.8453 - val_loss: 119.3084\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.5547 - val_loss: 122.6979\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.2580 - val_loss: 114.4273\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.6644 - val_loss: 114.2250\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.5380 - val_loss: 113.1813\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.2369 - val_loss: 107.9498\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.1637 - val_loss: 114.6819\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.6458 - val_loss: 117.6148\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.4301 - val_loss: 109.6524\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.2195 - val_loss: 114.5938\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.8133 - val_loss: 111.4818\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.5766 - val_loss: 115.8481\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.9868 - val_loss: 106.8827\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.5542 - val_loss: 117.5370\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.4678 - val_loss: 108.5600\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.2496 - val_loss: 121.0453\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.3035 - val_loss: 115.5068\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.8477 - val_loss: 107.7566\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.3746 - val_loss: 112.9044\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.5446 - val_loss: 106.7834\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.1132 - val_loss: 109.1625\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  18 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 6927.0708 - val_loss: 2993.8074\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1697.8297 - val_loss: 665.0177\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 502.1004 - val_loss: 533.2445\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 466.3397 - val_loss: 512.6284\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 424.9306 - val_loss: 463.8208\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 403.4920 - val_loss: 440.5362\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 388.0512 - val_loss: 422.5873\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 371.1501 - val_loss: 401.4205\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 353.7112 - val_loss: 384.8813\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 338.2671 - val_loss: 365.1578\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 323.6600 - val_loss: 347.1582\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 308.1171 - val_loss: 331.5843\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 294.9610 - val_loss: 313.8106\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 281.2038 - val_loss: 297.2164\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 269.8530 - val_loss: 282.7916\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 257.9724 - val_loss: 270.6648\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 247.9435 - val_loss: 257.1519\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.1840 - val_loss: 247.4701\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 229.9062 - val_loss: 234.9484\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 221.0870 - val_loss: 228.3183\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 212.7804 - val_loss: 217.7641\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 205.3906 - val_loss: 210.0489\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.8862 - val_loss: 201.4798\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.1003 - val_loss: 195.4422\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.8988 - val_loss: 186.7856\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.3685 - val_loss: 181.2991\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.1497 - val_loss: 174.0177\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.6524 - val_loss: 168.8298\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.9331 - val_loss: 163.1479\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.9631 - val_loss: 158.5596\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.7651 - val_loss: 152.9597\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 148.0885 - val_loss: 148.0935\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.9882 - val_loss: 143.4816\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.8506 - val_loss: 138.9451\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.3680 - val_loss: 135.9941\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.2854 - val_loss: 132.1987\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.1415 - val_loss: 127.5324\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.0144 - val_loss: 125.6198\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.3964 - val_loss: 120.9793\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.1050 - val_loss: 117.5599\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.9922 - val_loss: 114.4558\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.8264 - val_loss: 113.7418\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.8532 - val_loss: 109.0678\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.8967 - val_loss: 108.8732\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.9840 - val_loss: 104.2831\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.4815 - val_loss: 102.4882\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.4028 - val_loss: 103.1368\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.1971 - val_loss: 98.9846\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.9873 - val_loss: 96.4588\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.8293 - val_loss: 96.7183\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 662us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  19 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 899.2603 - val_loss: 682.8423\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 476.9787 - val_loss: 513.6870\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 427.2226 - val_loss: 478.5889\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 382.8480 - val_loss: 434.4491\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 345.3198 - val_loss: 394.0072\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 316.9513 - val_loss: 358.7583\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 297.5195 - val_loss: 354.0023\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 269.8393 - val_loss: 308.0582\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 248.7120 - val_loss: 318.4574\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.1528 - val_loss: 275.1473\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 218.9951 - val_loss: 255.6182\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 209.5199 - val_loss: 240.9669\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.3561 - val_loss: 229.4185\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.1198 - val_loss: 222.0529\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.0704 - val_loss: 228.8428\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.3607 - val_loss: 204.1919\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 171.7570 - val_loss: 194.3170\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.8744 - val_loss: 191.3588\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.2423 - val_loss: 192.5909\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.7830 - val_loss: 185.9878\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.6906 - val_loss: 175.1166\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.3401 - val_loss: 173.9182\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.1929 - val_loss: 171.8475\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.2464 - val_loss: 198.9800\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.3696 - val_loss: 177.9695\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.3745 - val_loss: 155.3497\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.5580 - val_loss: 156.0694\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.5989 - val_loss: 149.6305\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.5210 - val_loss: 153.6614\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.6954 - val_loss: 150.2353\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.4658 - val_loss: 146.8640\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.7452 - val_loss: 142.9193\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.6412 - val_loss: 142.7380\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.6899 - val_loss: 140.1827\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.7049 - val_loss: 140.2669\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.1857 - val_loss: 137.4928\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.4793 - val_loss: 136.9685\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.3657 - val_loss: 136.0161\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.5067 - val_loss: 135.0535\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.1116 - val_loss: 139.5728\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.4688 - val_loss: 136.8521\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.1090 - val_loss: 154.3672\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.4950 - val_loss: 149.6618\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.9002 - val_loss: 152.5571\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.9673 - val_loss: 139.2612\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.3429 - val_loss: 133.7954\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 112.1240 - val_loss: 130.9090\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.7864 - val_loss: 129.7658\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.5269 - val_loss: 130.9065\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.3178 - val_loss: 128.8083\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 668us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  20 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 208527.4219 - val_loss: 119938.8906\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 71441.4609 - val_loss: 26784.0078\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 10547.9824 - val_loss: 1663.5071\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1881.6578 - val_loss: 2383.0059\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1677.6263 - val_loss: 1377.1923\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1164.8630 - val_loss: 1230.3872\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1092.0563 - val_loss: 1163.1582\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1017.9376 - val_loss: 1116.5798\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 967.1379 - val_loss: 1065.5038\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 920.4883 - val_loss: 1018.6143\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 883.5091 - val_loss: 983.7184\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 849.4543 - val_loss: 949.8287\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 821.8058 - val_loss: 921.8983\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 791.7329 - val_loss: 890.1008\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 769.0624 - val_loss: 864.6766\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 750.9955 - val_loss: 846.3008\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 723.8348 - val_loss: 818.8121\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 707.4582 - val_loss: 798.8019\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 687.6055 - val_loss: 779.6596\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 670.4844 - val_loss: 762.0156\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 656.0634 - val_loss: 743.5107\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 641.0367 - val_loss: 726.3267\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 626.7345 - val_loss: 709.7639\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 612.0336 - val_loss: 694.9144\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 598.2534 - val_loss: 678.4501\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 585.6143 - val_loss: 662.8513\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 572.1362 - val_loss: 647.4050\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 559.3456 - val_loss: 633.6054\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 550.0334 - val_loss: 618.5292\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 536.9974 - val_loss: 606.9987\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 524.3615 - val_loss: 590.3918\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 511.8657 - val_loss: 578.2535\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 500.2274 - val_loss: 562.5181\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 488.0156 - val_loss: 550.8048\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 476.4711 - val_loss: 537.6216\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 466.9842 - val_loss: 524.4917\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 458.6935 - val_loss: 513.4495\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 445.6206 - val_loss: 499.7608\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 435.0153 - val_loss: 490.7932\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 425.0926 - val_loss: 476.2041\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 414.9539 - val_loss: 464.1690\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 403.8707 - val_loss: 453.1223\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 396.0390 - val_loss: 443.1824\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 385.0139 - val_loss: 429.1240\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 375.0217 - val_loss: 419.0029\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 367.0356 - val_loss: 409.0811\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 359.1502 - val_loss: 399.2352\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 352.0890 - val_loss: 389.8649\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 343.3659 - val_loss: 379.8309\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 335.9380 - val_loss: 369.5825\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 554us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  21 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 63654.3164 - val_loss: 38565.2891\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 24373.1074 - val_loss: 12237.9014\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 6989.2500 - val_loss: 2902.7644\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1655.8430 - val_loss: 845.0533\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 711.6461 - val_loss: 612.1104\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 615.5131 - val_loss: 589.4808\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 581.3600 - val_loss: 563.4710\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 556.7996 - val_loss: 538.9009\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 532.2319 - val_loss: 519.3837\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 512.0596 - val_loss: 500.4535\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 493.0363 - val_loss: 482.2678\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 475.2158 - val_loss: 464.6795\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 457.6967 - val_loss: 448.0092\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 441.5048 - val_loss: 432.6347\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 426.9761 - val_loss: 417.6683\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 412.6920 - val_loss: 403.6575\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 399.2643 - val_loss: 390.4168\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 386.2176 - val_loss: 377.2077\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 373.4539 - val_loss: 364.9041\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 361.8938 - val_loss: 353.7202\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 350.2880 - val_loss: 342.9279\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 339.8019 - val_loss: 333.0200\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 329.7643 - val_loss: 323.6111\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 320.0933 - val_loss: 314.5491\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 311.1820 - val_loss: 306.4856\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 303.2210 - val_loss: 299.0611\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 295.3443 - val_loss: 291.3514\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 288.6151 - val_loss: 284.4751\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 280.9077 - val_loss: 278.7685\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 274.5190 - val_loss: 272.6301\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 268.6711 - val_loss: 267.1642\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 263.0463 - val_loss: 262.2596\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 258.2302 - val_loss: 257.7531\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 253.0237 - val_loss: 253.6717\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 248.7263 - val_loss: 250.0252\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.7683 - val_loss: 246.9173\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 240.7290 - val_loss: 243.2881\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.0769 - val_loss: 241.0281\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 234.4324 - val_loss: 237.7199\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 232.0116 - val_loss: 235.7411\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 229.1878 - val_loss: 233.1414\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 226.6114 - val_loss: 231.3394\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 224.2499 - val_loss: 229.4430\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.2058 - val_loss: 227.5248\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 220.9577 - val_loss: 226.2158\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 218.5286 - val_loss: 224.3391\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.0503 - val_loss: 222.8696\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 215.6794 - val_loss: 221.9466\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 214.1607 - val_loss: 220.3244\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 212.6720 - val_loss: 219.2440\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 551us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  22 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 37652.1914 - val_loss: 30391.1895\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 23472.0703 - val_loss: 16907.3887\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 10642.1533 - val_loss: 5472.9380\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2819.9875 - val_loss: 1740.5583\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1466.9790 - val_loss: 1402.5953\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1149.1245 - val_loss: 1173.3427\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1003.1955 - val_loss: 1056.6263\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 906.3712 - val_loss: 932.6818\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 813.1737 - val_loss: 838.1345\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 731.0114 - val_loss: 760.3843\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 661.4593 - val_loss: 685.8096\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 599.2972 - val_loss: 621.9011\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 553.5381 - val_loss: 572.7799\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 515.0615 - val_loss: 531.2276\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 482.2005 - val_loss: 495.0863\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 453.8722 - val_loss: 471.2627\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 427.4010 - val_loss: 440.7570\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 408.4542 - val_loss: 419.4613\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 390.3763 - val_loss: 401.4917\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 374.6245 - val_loss: 386.2939\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 362.9396 - val_loss: 364.7751\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 348.2566 - val_loss: 356.9998\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 336.7282 - val_loss: 339.4912\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 327.6056 - val_loss: 332.4882\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 319.6080 - val_loss: 322.4544\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 311.6137 - val_loss: 311.0670\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 304.4702 - val_loss: 307.4960\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 298.0813 - val_loss: 297.3548\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 293.4363 - val_loss: 291.4059\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 287.7592 - val_loss: 285.8332\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 282.8340 - val_loss: 280.0994\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 279.5056 - val_loss: 274.2979\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 276.1346 - val_loss: 272.6373\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 273.6180 - val_loss: 267.9175\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 267.8773 - val_loss: 265.8422\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 266.1042 - val_loss: 261.5288\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 261.1754 - val_loss: 255.6379\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 259.3289 - val_loss: 254.1491\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 256.5397 - val_loss: 252.1676\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 254.9307 - val_loss: 249.0645\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.1726 - val_loss: 247.5796\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 250.5370 - val_loss: 241.8777\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 248.8380 - val_loss: 241.6207\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 246.5887 - val_loss: 236.7155\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.7360 - val_loss: 238.5342\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 242.7522 - val_loss: 234.1813\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.2644 - val_loss: 231.2671\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.1372 - val_loss: 234.8588\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.4883 - val_loss: 230.3651\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.3361 - val_loss: 229.1044\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 668us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  23 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5842.5669 - val_loss: 2679.0991\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1449.6920 - val_loss: 611.9563\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 469.2734 - val_loss: 382.7934\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 405.8085 - val_loss: 352.5505\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 363.5458 - val_loss: 316.9425\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 320.2053 - val_loss: 295.5975\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 291.6036 - val_loss: 282.3326\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 270.0184 - val_loss: 273.7914\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.8837 - val_loss: 272.6341\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 247.1287 - val_loss: 271.8252\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 240.8615 - val_loss: 269.6483\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.9705 - val_loss: 267.0764\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 233.4572 - val_loss: 264.3559\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 230.7944 - val_loss: 261.1260\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 228.5140 - val_loss: 258.4956\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 225.5069 - val_loss: 255.6234\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 223.3348 - val_loss: 252.9304\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 221.5420 - val_loss: 250.7793\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 219.2625 - val_loss: 248.2360\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 216.6837 - val_loss: 246.3923\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 214.3792 - val_loss: 244.2884\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 212.7072 - val_loss: 242.4671\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 210.1116 - val_loss: 240.3699\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.2388 - val_loss: 238.6897\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 206.2835 - val_loss: 236.2182\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.7139 - val_loss: 234.3369\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 201.6234 - val_loss: 232.0381\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.2952 - val_loss: 230.0227\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.7766 - val_loss: 227.1389\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.5294 - val_loss: 225.3365\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.7448 - val_loss: 223.5345\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 190.8113 - val_loss: 220.4036\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 188.3129 - val_loss: 218.4212\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.4752 - val_loss: 216.1504\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 182.9255 - val_loss: 212.9825\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.9151 - val_loss: 210.2193\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.2738 - val_loss: 208.5509\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 167.1952 - val_loss: 205.9960\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.7207 - val_loss: 204.9851\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 158.9580 - val_loss: 201.9030\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.4226 - val_loss: 199.4523\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.8118 - val_loss: 197.4068\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.2092 - val_loss: 194.7665\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.7020 - val_loss: 193.3584\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.5913 - val_loss: 189.9867\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.6639 - val_loss: 187.8139\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.8810 - val_loss: 186.2491\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.4483 - val_loss: 184.4464\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.0056 - val_loss: 182.5834\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.7376 - val_loss: 181.4103\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 662us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  24 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 394.0475 - val_loss: 296.2350\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 275.6130 - val_loss: 247.8775\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.3094 - val_loss: 229.7529\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 218.1764 - val_loss: 225.8175\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 210.6019 - val_loss: 210.4238\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 197.1136 - val_loss: 201.6805\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 191.9925 - val_loss: 198.8238\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.4013 - val_loss: 189.3593\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.1461 - val_loss: 184.1359\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.0973 - val_loss: 188.6740\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.9475 - val_loss: 177.3175\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.1514 - val_loss: 171.7548\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.5162 - val_loss: 171.0918\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.3141 - val_loss: 165.3107\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.3277 - val_loss: 161.5771\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.4140 - val_loss: 162.2299\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.2422 - val_loss: 163.8977\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.1571 - val_loss: 153.7136\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.4883 - val_loss: 153.7581\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.0045 - val_loss: 152.5928\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.0747 - val_loss: 147.2579\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.0950 - val_loss: 144.8564\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.9345 - val_loss: 159.5760\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.1902 - val_loss: 141.9568\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.2532 - val_loss: 141.7499\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.9572 - val_loss: 141.7348\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.6965 - val_loss: 138.0685\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.4285 - val_loss: 138.6770\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.7648 - val_loss: 134.2410\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.8519 - val_loss: 133.9308\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.8817 - val_loss: 132.6553\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.4000 - val_loss: 131.5998\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.4250 - val_loss: 130.0486\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.9808 - val_loss: 130.5167\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.4458 - val_loss: 131.5784\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.5606 - val_loss: 131.3417\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.8902 - val_loss: 129.3406\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.2059 - val_loss: 127.4261\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.6963 - val_loss: 126.0484\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.7990 - val_loss: 125.8118\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.8143 - val_loss: 127.9355\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.7370 - val_loss: 124.6371\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.3226 - val_loss: 119.9755\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.6688 - val_loss: 118.8265\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.9270 - val_loss: 121.1322\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.5929 - val_loss: 120.4912\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.2221 - val_loss: 128.7041\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.6215 - val_loss: 131.8107\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.8193 - val_loss: 125.8184\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.3591 - val_loss: 115.2644\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 556us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  25 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5406.0063 - val_loss: 1638.3214\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 812.3109 - val_loss: 789.8069\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 655.7507 - val_loss: 689.6678\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 545.1070 - val_loss: 640.6049\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 498.2627 - val_loss: 568.9968\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 446.5990 - val_loss: 504.5080\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 402.2562 - val_loss: 444.6402\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 361.2812 - val_loss: 389.1167\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 323.9390 - val_loss: 346.7825\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 291.3042 - val_loss: 314.9597\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 270.5670 - val_loss: 289.0445\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 253.0680 - val_loss: 267.9987\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.4619 - val_loss: 251.9954\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 227.0108 - val_loss: 238.3563\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.5533 - val_loss: 229.4063\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 211.5200 - val_loss: 218.3695\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.9503 - val_loss: 211.2760\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.8509 - val_loss: 207.7074\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.4536 - val_loss: 203.2042\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.0710 - val_loss: 201.8268\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 184.2458 - val_loss: 191.0060\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.5379 - val_loss: 187.4926\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.0272 - val_loss: 183.3010\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.2847 - val_loss: 181.1288\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 178.8856 - val_loss: 182.9184\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 166.2740 - val_loss: 175.2115\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.1066 - val_loss: 171.5603\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.1998 - val_loss: 169.0755\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.2798 - val_loss: 166.7807\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.2670 - val_loss: 164.5342\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.1470 - val_loss: 161.4346\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.4573 - val_loss: 159.9229\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.9133 - val_loss: 161.8890\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.8588 - val_loss: 156.7193\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.2085 - val_loss: 152.7603\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.5817 - val_loss: 151.3237\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.0022 - val_loss: 150.9747\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.1056 - val_loss: 146.2171\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.2432 - val_loss: 143.9780\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.1186 - val_loss: 142.7195\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 135.2215 - val_loss: 139.6511\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.2211 - val_loss: 137.9653\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.3990 - val_loss: 135.7487\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.8035 - val_loss: 133.9483\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.8053 - val_loss: 143.8594\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.0850 - val_loss: 131.6723\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.9179 - val_loss: 128.7419\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.7009 - val_loss: 126.9648\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.8580 - val_loss: 125.4929\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.5411 - val_loss: 129.7363\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  26 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7189.1733 - val_loss: 2184.2427\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2427.0276 - val_loss: 2199.4141\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1871.6962 - val_loss: 1781.3066\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1581.6614 - val_loss: 1514.3340\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1394.1223 - val_loss: 1352.5023\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1232.5779 - val_loss: 1205.2357\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1082.2566 - val_loss: 1037.3208\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 924.0082 - val_loss: 949.0923\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 800.9364 - val_loss: 940.8124\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 696.2333 - val_loss: 888.1659\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 638.1254 - val_loss: 815.0120\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 592.6169 - val_loss: 739.0797\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 550.2330 - val_loss: 697.5192\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 505.1549 - val_loss: 652.6855\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 472.8267 - val_loss: 606.9896\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 442.1839 - val_loss: 565.6900\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 416.8587 - val_loss: 534.1607\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 390.3217 - val_loss: 504.5953\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 366.6149 - val_loss: 475.9035\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 341.1634 - val_loss: 442.6819\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 323.6448 - val_loss: 421.7805\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 303.1978 - val_loss: 398.7311\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 286.7598 - val_loss: 371.2758\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 275.4896 - val_loss: 376.5400\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 256.2531 - val_loss: 334.7912\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.7847 - val_loss: 323.4802\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 226.5068 - val_loss: 303.9509\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 214.8222 - val_loss: 287.9514\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.4813 - val_loss: 273.3867\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.5079 - val_loss: 272.4759\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 186.0731 - val_loss: 251.1374\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.5052 - val_loss: 249.4540\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 172.7915 - val_loss: 229.5177\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.7686 - val_loss: 220.9534\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.2587 - val_loss: 217.8371\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.8653 - val_loss: 221.5334\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.1366 - val_loss: 197.1790\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.5770 - val_loss: 192.8427\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.7868 - val_loss: 188.2797\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.4826 - val_loss: 180.0322\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.7894 - val_loss: 174.6060\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.0309 - val_loss: 170.6828\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.0316 - val_loss: 166.2820\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.5217 - val_loss: 165.6207\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.4442 - val_loss: 163.0710\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.9118 - val_loss: 158.7482\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.2696 - val_loss: 152.8815\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.2508 - val_loss: 151.5717\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.2915 - val_loss: 151.2619\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.4899 - val_loss: 151.7755\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 671us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  27 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2278.4592 - val_loss: 1015.1475\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1009.5342 - val_loss: 724.6443\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 704.6958 - val_loss: 570.5091\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 568.0439 - val_loss: 467.1328\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 478.5563 - val_loss: 405.9806\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 410.6701 - val_loss: 369.5823\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 374.0503 - val_loss: 332.8358\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 332.1204 - val_loss: 305.6466\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 299.5854 - val_loss: 290.7457\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 275.6551 - val_loss: 261.0333\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 253.7643 - val_loss: 258.2479\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.6471 - val_loss: 232.1780\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.2883 - val_loss: 214.4824\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.9919 - val_loss: 200.6445\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.4230 - val_loss: 188.4915\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.8768 - val_loss: 183.8804\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.0087 - val_loss: 164.0481\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 143.7562 - val_loss: 151.2494\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.5601 - val_loss: 142.6021\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.0190 - val_loss: 125.3695\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.1845 - val_loss: 117.5624\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.1305 - val_loss: 109.1895\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.1708 - val_loss: 105.5130\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.4142 - val_loss: 103.2176\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.8906 - val_loss: 99.5936\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.3754 - val_loss: 96.6404\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.6875 - val_loss: 98.7438\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.2826 - val_loss: 95.5231\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.8061 - val_loss: 91.1434\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.2122 - val_loss: 88.7840\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.0614 - val_loss: 100.4919\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.3036 - val_loss: 88.6540\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 71.0704 - val_loss: 86.2102\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 73.3936 - val_loss: 94.1841\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 69.7063 - val_loss: 85.0841\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 68.2379 - val_loss: 82.6077\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 67.0936 - val_loss: 83.8559\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 67.4710 - val_loss: 87.3654\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 65.6732 - val_loss: 79.7938\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 67.2673 - val_loss: 86.3622\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 64.4589 - val_loss: 78.6432\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 63.4799 - val_loss: 78.1926\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 62.0858 - val_loss: 84.1562\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 65.5531 - val_loss: 84.7838\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 65.9170 - val_loss: 77.2849\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 62.9443 - val_loss: 73.4050\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 60.9824 - val_loss: 74.0760\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 62.2423 - val_loss: 76.1656\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 63.5652 - val_loss: 71.5050\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 61.2586 - val_loss: 72.4961\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 548us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  28 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1604.6051 - val_loss: 1443.8661\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1106.9546 - val_loss: 1079.5469\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 950.3143 - val_loss: 919.4014\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 826.8077 - val_loss: 784.0049\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 723.2293 - val_loss: 688.6730\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 627.9734 - val_loss: 602.9064\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 566.7210 - val_loss: 565.7174\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 519.6412 - val_loss: 506.7574\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 473.9635 - val_loss: 469.0433\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 440.1333 - val_loss: 446.7640\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 409.0805 - val_loss: 429.6534\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 388.0515 - val_loss: 399.1812\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 366.5053 - val_loss: 382.2782\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 343.4586 - val_loss: 375.5429\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 326.0838 - val_loss: 357.7013\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 311.9330 - val_loss: 346.7419\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 299.1449 - val_loss: 338.3424\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 295.3907 - val_loss: 330.8215\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 289.5409 - val_loss: 322.7646\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 267.9147 - val_loss: 317.4028\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 259.2972 - val_loss: 312.2370\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 258.2212 - val_loss: 304.1388\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 254.0600 - val_loss: 303.1176\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.4813 - val_loss: 292.7121\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.5184 - val_loss: 287.5961\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 228.8613 - val_loss: 288.5616\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 225.8818 - val_loss: 279.3305\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 221.2341 - val_loss: 285.0347\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 218.6610 - val_loss: 272.3910\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 213.8154 - val_loss: 268.4820\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 209.4524 - val_loss: 267.9415\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 206.2830 - val_loss: 265.8328\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.2409 - val_loss: 259.3076\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.9799 - val_loss: 258.0180\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.6175 - val_loss: 257.5425\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.3362 - val_loss: 258.6594\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.6137 - val_loss: 250.1337\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 189.0407 - val_loss: 247.8160\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.9379 - val_loss: 244.8195\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 186.6487 - val_loss: 241.2283\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 183.5551 - val_loss: 238.6894\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.3307 - val_loss: 258.4024\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 183.1957 - val_loss: 234.9263\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 183.7827 - val_loss: 239.3076\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.6178 - val_loss: 230.5481\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.1725 - val_loss: 228.1062\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.4037 - val_loss: 227.3160\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 171.1448 - val_loss: 234.4011\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.0289 - val_loss: 226.1394\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 167.7641 - val_loss: 220.3473\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 664us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  29 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 77036.4609 - val_loss: 65652.2500\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 55936.2539 - val_loss: 47881.8633\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 40942.5898 - val_loss: 35449.8242\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 30492.4004 - val_loss: 26711.0801\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 23102.5703 - val_loss: 20445.7695\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 17759.4492 - val_loss: 15843.4443\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 13786.9463 - val_loss: 12388.8037\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 10788.7637 - val_loss: 9729.6689\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 8461.1123 - val_loss: 7671.9941\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 6660.9717 - val_loss: 6032.2114\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5222.5151 - val_loss: 4742.3711\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4090.8662 - val_loss: 3711.8040\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3188.7275 - val_loss: 2893.6470\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2477.8086 - val_loss: 2242.8081\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1914.9779 - val_loss: 1734.6460\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1478.0226 - val_loss: 1339.0769\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1143.1375 - val_loss: 1033.2784\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 889.6161 - val_loss: 802.4198\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 699.6242 - val_loss: 632.9650\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 563.5511 - val_loss: 507.6754\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 466.0291 - val_loss: 418.3818\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 398.3090 - val_loss: 355.9904\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 352.1812 - val_loss: 313.1937\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 321.5464 - val_loss: 284.3011\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 301.0664 - val_loss: 265.7364\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 289.1792 - val_loss: 252.0523\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 280.6803 - val_loss: 243.5851\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 275.7068 - val_loss: 238.2203\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 272.8785 - val_loss: 234.5656\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 271.0899 - val_loss: 232.2611\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 270.3278 - val_loss: 230.1099\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 269.6176 - val_loss: 229.3110\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 269.6390 - val_loss: 228.1044\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 269.1774 - val_loss: 227.9723\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 269.1166 - val_loss: 227.5531\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 269.0271 - val_loss: 227.3945\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 269.1116 - val_loss: 227.0132\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 268.9661 - val_loss: 226.9967\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 269.0498 - val_loss: 227.1455\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 269.0192 - val_loss: 226.7446\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 268.8609 - val_loss: 226.6342\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 268.8277 - val_loss: 226.7041\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 268.7910 - val_loss: 226.7299\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 269.0826 - val_loss: 226.2232\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 268.7852 - val_loss: 226.5390\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 268.8726 - val_loss: 226.3057\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 268.8875 - val_loss: 226.9078\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 268.6436 - val_loss: 226.4668\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 268.7503 - val_loss: 226.0967\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 268.5499 - val_loss: 226.5039\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  30 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 32222.4902 - val_loss: 17278.9023\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 10657.7910 - val_loss: 4411.8555\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1913.8821 - val_loss: 367.9553\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 539.4431 - val_loss: 472.7044\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 430.7024 - val_loss: 302.2852\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 359.5334 - val_loss: 279.6806\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 323.1412 - val_loss: 256.1378\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 298.4944 - val_loss: 235.1464\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 274.9595 - val_loss: 217.5022\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 255.0358 - val_loss: 202.4269\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.8096 - val_loss: 189.6841\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.3034 - val_loss: 177.9189\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.5107 - val_loss: 166.8788\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 197.2370 - val_loss: 158.4135\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.2829 - val_loss: 149.9873\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.0881 - val_loss: 142.7822\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.3372 - val_loss: 137.0311\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.9051 - val_loss: 130.8864\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.2005 - val_loss: 127.1945\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.7302 - val_loss: 123.3395\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.4685 - val_loss: 119.4443\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.6238 - val_loss: 117.9071\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.6817 - val_loss: 115.1245\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.8799 - val_loss: 114.0089\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.9303 - val_loss: 112.7092\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.1906 - val_loss: 111.7210\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.7937 - val_loss: 111.2579\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.0005 - val_loss: 110.5424\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.5689 - val_loss: 110.3438\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.6810 - val_loss: 109.7043\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.8108 - val_loss: 109.4519\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.2621 - val_loss: 109.3296\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.1608 - val_loss: 109.0264\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.3675 - val_loss: 108.9815\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.8998 - val_loss: 108.8339\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.5978 - val_loss: 107.9345\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.7098 - val_loss: 108.2479\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.8917 - val_loss: 107.6228\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.3050 - val_loss: 107.5529\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.5278 - val_loss: 107.3460\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.3307 - val_loss: 107.5775\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.3444 - val_loss: 107.3915\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.2056 - val_loss: 107.0534\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.7466 - val_loss: 107.1072\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.8700 - val_loss: 106.9431\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.6376 - val_loss: 106.5863\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.6824 - val_loss: 106.9687\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.6756 - val_loss: 106.6393\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.5052 - val_loss: 106.1422\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.6194 - val_loss: 105.8567\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  31 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 6570.8018 - val_loss: 4187.0356\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3140.8562 - val_loss: 2293.8760\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2106.2192 - val_loss: 1629.7322\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1441.4872 - val_loss: 1098.7837\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1008.3474 - val_loss: 803.5314\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 788.8597 - val_loss: 659.8909\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 679.2181 - val_loss: 577.0981\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 601.0841 - val_loss: 513.8859\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 543.4584 - val_loss: 471.4766\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 493.8647 - val_loss: 434.0133\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 454.2791 - val_loss: 403.4806\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 418.7626 - val_loss: 373.1008\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 395.4653 - val_loss: 372.6379\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 370.3540 - val_loss: 336.1875\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 346.2749 - val_loss: 309.7024\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 319.1124 - val_loss: 291.5027\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 303.2051 - val_loss: 283.4670\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 286.2539 - val_loss: 274.8140\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 268.3466 - val_loss: 252.5693\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.2097 - val_loss: 242.0901\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 250.4339 - val_loss: 246.2018\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.6200 - val_loss: 217.7516\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.2944 - val_loss: 212.0214\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 214.1863 - val_loss: 209.8147\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 211.8715 - val_loss: 194.4545\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.5821 - val_loss: 201.0681\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 184.7873 - val_loss: 183.7269\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.6619 - val_loss: 183.8253\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 171.5085 - val_loss: 175.6210\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.4450 - val_loss: 173.2940\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.5753 - val_loss: 173.0307\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.5092 - val_loss: 166.8166\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.9793 - val_loss: 165.5310\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.3541 - val_loss: 168.3978\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.0394 - val_loss: 161.7839\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.6146 - val_loss: 160.1691\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.3135 - val_loss: 159.1142\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.7391 - val_loss: 158.4588\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.0709 - val_loss: 160.8651\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.4704 - val_loss: 155.8185\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.5565 - val_loss: 158.4378\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.5867 - val_loss: 151.8849\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.1924 - val_loss: 153.9642\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.1034 - val_loss: 152.4153\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.0563 - val_loss: 149.0016\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.6059 - val_loss: 148.4359\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.6088 - val_loss: 165.5833\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.4479 - val_loss: 146.1986\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.6106 - val_loss: 146.2402\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.8848 - val_loss: 156.7463\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  32 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4448.1768 - val_loss: 909.4417\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1098.6128 - val_loss: 850.8717\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 679.3113 - val_loss: 539.8500\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 552.4861 - val_loss: 462.7540\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 456.7156 - val_loss: 382.1408\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 391.4057 - val_loss: 355.7692\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 348.6908 - val_loss: 315.9261\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 318.5444 - val_loss: 293.6693\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 295.0948 - val_loss: 289.1255\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 277.0698 - val_loss: 266.4265\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.6054 - val_loss: 252.5164\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 250.5363 - val_loss: 256.6671\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 233.7781 - val_loss: 236.6224\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.6036 - val_loss: 223.8488\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 210.2262 - val_loss: 211.2991\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.6896 - val_loss: 203.9914\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 188.5980 - val_loss: 194.9004\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.4047 - val_loss: 192.7378\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.8886 - val_loss: 182.8009\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 172.7779 - val_loss: 183.8973\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 172.4343 - val_loss: 177.0005\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.2192 - val_loss: 172.5949\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.5763 - val_loss: 185.2958\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.8372 - val_loss: 163.1387\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.8752 - val_loss: 158.3938\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.2750 - val_loss: 153.9162\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.6711 - val_loss: 150.1994\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.7100 - val_loss: 145.5906\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.8841 - val_loss: 142.0477\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.9143 - val_loss: 148.0818\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.7646 - val_loss: 144.4827\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.1318 - val_loss: 132.4609\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.3815 - val_loss: 130.5989\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.2369 - val_loss: 128.0293\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 121.9780 - val_loss: 125.6039\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.9135 - val_loss: 129.2217\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.5996 - val_loss: 122.4387\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.3301 - val_loss: 122.5754\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.4515 - val_loss: 127.9397\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.8661 - val_loss: 131.1125\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.2118 - val_loss: 126.2129\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.1078 - val_loss: 123.8147\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.3744 - val_loss: 121.2863\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.6075 - val_loss: 119.4730\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.2139 - val_loss: 118.5199\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.8517 - val_loss: 118.2988\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.7355 - val_loss: 117.7149\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.3495 - val_loss: 118.6664\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.2047 - val_loss: 119.3234\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.1131 - val_loss: 121.9701\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 668us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  33 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 328080.8750 - val_loss: 245535.1875\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 195749.3125 - val_loss: 143978.2656\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115119.9609 - val_loss: 84292.9141\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 65093.8398 - val_loss: 46181.0859\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34621.8633 - val_loss: 23817.9844\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 17734.8594 - val_loss: 11847.7715\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 9139.3115 - val_loss: 6307.6865\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5409.1484 - val_loss: 3956.5540\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3894.6653 - val_loss: 3079.5625\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3297.1567 - val_loss: 2741.1086\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3031.6216 - val_loss: 2554.7366\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2847.7849 - val_loss: 2406.3999\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2687.7363 - val_loss: 2269.8171\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2530.4475 - val_loss: 2138.9956\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2380.9773 - val_loss: 2015.7633\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2241.2546 - val_loss: 1896.9729\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2108.1370 - val_loss: 1786.4973\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1989.1353 - val_loss: 1687.9781\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1873.9702 - val_loss: 1591.9362\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1770.3663 - val_loss: 1494.1553\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1650.7732 - val_loss: 1355.7650\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1517.8896 - val_loss: 1256.6266\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1418.1183 - val_loss: 1140.1001\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1305.8732 - val_loss: 1022.4006\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1203.0114 - val_loss: 934.6187\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1113.8549 - val_loss: 865.5020\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1025.4158 - val_loss: 811.7672\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 963.0821 - val_loss: 770.7491\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 913.3256 - val_loss: 736.4250\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 874.8275 - val_loss: 708.8751\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 834.6363 - val_loss: 682.7749\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 802.7242 - val_loss: 658.5598\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 777.5474 - val_loss: 640.8759\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 754.7951 - val_loss: 615.8199\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 727.0042 - val_loss: 603.7615\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 705.5646 - val_loss: 586.8182\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 691.4806 - val_loss: 575.3242\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 673.9782 - val_loss: 565.3170\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 652.7604 - val_loss: 545.7497\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 639.1481 - val_loss: 539.9514\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 623.9155 - val_loss: 530.4052\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 612.2004 - val_loss: 521.4007\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 599.4122 - val_loss: 511.7585\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 588.0593 - val_loss: 509.5675\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 577.5318 - val_loss: 494.1644\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 565.6605 - val_loss: 494.6315\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 555.9136 - val_loss: 481.3107\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 546.9285 - val_loss: 482.1806\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 536.2020 - val_loss: 468.7950\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 532.4350 - val_loss: 478.0698\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 555us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  34 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 347707.1250 - val_loss: 284510.7188\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245968.0312 - val_loss: 198670.3906\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168714.5781 - val_loss: 133174.0625\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107876.8438 - val_loss: 79241.0234\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 58547.9570 - val_loss: 34952.4570\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 20112.9746 - val_loss: 7778.8052\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4471.7749 - val_loss: 3450.7805\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3145.2461 - val_loss: 2670.2278\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2294.0706 - val_loss: 2106.5068\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1856.9348 - val_loss: 1813.7290\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1626.9584 - val_loss: 1586.7830\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1451.1708 - val_loss: 1425.2659\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1309.5792 - val_loss: 1297.9248\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1194.2078 - val_loss: 1181.5494\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1089.9469 - val_loss: 1078.8588\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 994.7566 - val_loss: 979.4001\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 910.7978 - val_loss: 896.9420\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 836.5016 - val_loss: 817.8277\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 770.5155 - val_loss: 749.2734\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 707.4437 - val_loss: 682.2319\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 650.9609 - val_loss: 633.3130\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 600.8091 - val_loss: 572.2304\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 561.2262 - val_loss: 529.2489\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 512.2280 - val_loss: 489.1442\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 477.6859 - val_loss: 459.8608\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 451.1323 - val_loss: 427.1459\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 421.6155 - val_loss: 395.6595\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 397.8560 - val_loss: 377.0230\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 377.5975 - val_loss: 355.8599\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 356.8841 - val_loss: 335.0121\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 338.8479 - val_loss: 321.6927\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 325.3585 - val_loss: 303.3713\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 310.0932 - val_loss: 287.4520\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 297.2637 - val_loss: 279.5386\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 287.7838 - val_loss: 268.7832\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 277.7119 - val_loss: 255.6063\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 270.4499 - val_loss: 250.0290\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 259.3548 - val_loss: 240.6037\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 253.3747 - val_loss: 237.6196\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 246.0733 - val_loss: 230.2832\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.7857 - val_loss: 225.2460\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.4227 - val_loss: 220.7133\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 228.9796 - val_loss: 212.5622\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 223.8205 - val_loss: 212.0689\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 219.2385 - val_loss: 205.6175\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 216.1096 - val_loss: 202.1655\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 210.3911 - val_loss: 201.1267\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 205.9530 - val_loss: 198.8523\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 201.8099 - val_loss: 192.9064\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.0617 - val_loss: 192.8142\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 554us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  35 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 140656.8281 - val_loss: 85318.8828\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 54282.7500 - val_loss: 25580.0137\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 13830.9844 - val_loss: 5376.8008\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3690.8569 - val_loss: 2975.4009\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3109.3545 - val_loss: 2988.5413\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2937.1521 - val_loss: 2722.5540\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2734.5933 - val_loss: 2575.4773\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2604.4712 - val_loss: 2448.1694\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2469.8689 - val_loss: 2329.5828\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2348.1475 - val_loss: 2209.6609\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2227.8264 - val_loss: 2096.0349\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2116.2026 - val_loss: 1987.6082\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2005.2047 - val_loss: 1882.4440\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1899.5054 - val_loss: 1782.1536\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1796.4792 - val_loss: 1687.4636\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1706.3220 - val_loss: 1592.9164\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1612.4857 - val_loss: 1507.9012\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1524.8140 - val_loss: 1429.7295\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1450.2621 - val_loss: 1354.1345\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1369.8772 - val_loss: 1279.8055\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1298.4052 - val_loss: 1211.5161\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1229.9174 - val_loss: 1147.9569\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1166.7205 - val_loss: 1086.0128\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1106.7330 - val_loss: 1027.9089\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1050.7761 - val_loss: 975.0918\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1000.5135 - val_loss: 923.4478\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 950.8932 - val_loss: 878.7930\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 898.3804 - val_loss: 830.6071\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 853.9214 - val_loss: 787.5108\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 812.7096 - val_loss: 748.3151\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 772.7515 - val_loss: 711.6843\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 736.5158 - val_loss: 675.5154\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 699.9290 - val_loss: 643.1066\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 668.0076 - val_loss: 611.4175\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 637.6097 - val_loss: 582.7968\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 605.3172 - val_loss: 553.1814\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 582.2159 - val_loss: 526.1343\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 554.5977 - val_loss: 504.7190\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 525.9996 - val_loss: 477.4140\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 503.0103 - val_loss: 455.4070\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 480.3180 - val_loss: 435.3608\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 461.4777 - val_loss: 414.5706\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 442.4897 - val_loss: 396.7905\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 421.1214 - val_loss: 377.8159\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 404.2600 - val_loss: 361.0752\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 385.2702 - val_loss: 344.4889\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 371.5595 - val_loss: 329.3322\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 354.6750 - val_loss: 317.5599\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 339.5518 - val_loss: 301.7303\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 325.6174 - val_loss: 289.4355\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  36 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1974.7035 - val_loss: 458.0287\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 268.4034 - val_loss: 221.7985\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 213.8282 - val_loss: 192.9121\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.3123 - val_loss: 176.1446\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 175.8243 - val_loss: 173.2133\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.4179 - val_loss: 171.5623\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.1878 - val_loss: 167.5773\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.7422 - val_loss: 165.2949\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.7155 - val_loss: 164.3755\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.5844 - val_loss: 160.8037\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.6622 - val_loss: 159.8167\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.5434 - val_loss: 158.3749\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.8678 - val_loss: 156.7492\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.2716 - val_loss: 157.6462\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.4756 - val_loss: 155.2745\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.6518 - val_loss: 154.7302\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.7527 - val_loss: 154.5150\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.1751 - val_loss: 152.5470\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.4844 - val_loss: 154.9950\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.8254 - val_loss: 150.5936\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.8021 - val_loss: 151.9412\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.9085 - val_loss: 149.0684\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.8090 - val_loss: 148.9958\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.5172 - val_loss: 146.6447\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.2818 - val_loss: 148.1496\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.2833 - val_loss: 145.0167\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.4150 - val_loss: 145.3346\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.9591 - val_loss: 144.1954\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.9557 - val_loss: 143.3974\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.3716 - val_loss: 142.0869\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.0476 - val_loss: 140.8427\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.8646 - val_loss: 142.4074\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.5065 - val_loss: 139.4410\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.0373 - val_loss: 138.1884\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.9690 - val_loss: 137.3857\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.9307 - val_loss: 136.5014\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.0937 - val_loss: 136.6357\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.4462 - val_loss: 135.3093\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.7160 - val_loss: 135.2397\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.8188 - val_loss: 134.9540\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.9411 - val_loss: 134.2734\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.2392 - val_loss: 132.9688\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.0976 - val_loss: 132.8195\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.6903 - val_loss: 131.1775\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.8114 - val_loss: 130.3123\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.4242 - val_loss: 130.1068\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.2088 - val_loss: 128.0646\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.4393 - val_loss: 129.2295\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.4479 - val_loss: 129.3108\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 109.1125 - val_loss: 126.3448\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 663us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  37 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7373.9775 - val_loss: 3302.3323\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2249.7634 - val_loss: 1441.2437\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1497.6384 - val_loss: 1158.2772\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1161.0525 - val_loss: 963.3884\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 993.7814 - val_loss: 851.5977\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 863.8839 - val_loss: 763.7697\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 760.2699 - val_loss: 685.3671\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 678.9376 - val_loss: 624.3959\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 610.8060 - val_loss: 572.2797\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 562.2339 - val_loss: 534.0574\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 519.0783 - val_loss: 507.5467\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 489.4078 - val_loss: 485.0992\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 462.8791 - val_loss: 463.9230\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 441.9802 - val_loss: 448.3686\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 420.3584 - val_loss: 430.7649\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 401.5580 - val_loss: 418.1981\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 385.5974 - val_loss: 406.5953\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 372.2596 - val_loss: 395.8386\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 360.8110 - val_loss: 385.9615\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 349.8558 - val_loss: 378.5706\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 339.5362 - val_loss: 369.3019\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 332.1814 - val_loss: 361.9709\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 323.3490 - val_loss: 355.6855\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 316.0211 - val_loss: 348.2328\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 309.2776 - val_loss: 342.1079\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 302.8039 - val_loss: 336.0303\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 296.4910 - val_loss: 329.7598\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 291.2624 - val_loss: 322.5894\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 285.4300 - val_loss: 318.7252\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 279.9876 - val_loss: 312.1653\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 274.1046 - val_loss: 304.9927\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 268.3729 - val_loss: 298.5261\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 262.8441 - val_loss: 291.7241\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 256.3302 - val_loss: 285.0593\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 251.9789 - val_loss: 277.4863\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.3753 - val_loss: 270.3839\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.4323 - val_loss: 261.3196\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 228.6880 - val_loss: 253.6199\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.5939 - val_loss: 247.0866\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 216.6855 - val_loss: 239.0300\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 210.6461 - val_loss: 232.6501\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 205.2593 - val_loss: 227.3653\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 200.4605 - val_loss: 222.2906\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.0816 - val_loss: 216.3519\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 190.7822 - val_loss: 210.9185\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 186.4714 - val_loss: 206.9113\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.2087 - val_loss: 201.0361\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.1547 - val_loss: 196.5271\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.3551 - val_loss: 191.9765\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.0288 - val_loss: 188.0897\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  38 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 37264.1758 - val_loss: 15228.7285\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 7416.6172 - val_loss: 2218.1504\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1971.3090 - val_loss: 1648.2078\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1788.1971 - val_loss: 1392.9397\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1474.8722 - val_loss: 1205.8920\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1321.9471 - val_loss: 1091.2123\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1191.6454 - val_loss: 975.0579\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1070.3285 - val_loss: 880.4891\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 966.2810 - val_loss: 797.0633\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 874.8030 - val_loss: 726.2841\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 794.4532 - val_loss: 671.5377\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 729.2654 - val_loss: 624.2341\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 672.1870 - val_loss: 584.2115\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 624.9381 - val_loss: 550.3165\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 581.5726 - val_loss: 521.0360\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 545.7005 - val_loss: 492.0459\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 511.9988 - val_loss: 468.9257\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 483.8903 - val_loss: 445.9466\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 452.8929 - val_loss: 422.1294\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 413.7508 - val_loss: 370.4473\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 339.2044 - val_loss: 300.5008\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 272.6510 - val_loss: 263.6561\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 233.6747 - val_loss: 232.0461\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.6615 - val_loss: 212.0889\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 186.5621 - val_loss: 199.5402\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 175.4576 - val_loss: 186.1478\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 167.0716 - val_loss: 177.8863\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.7472 - val_loss: 172.8692\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.2001 - val_loss: 162.6465\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.9030 - val_loss: 156.3497\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.8048 - val_loss: 151.8931\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.0330 - val_loss: 147.9261\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.5436 - val_loss: 144.2462\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.7547 - val_loss: 141.0463\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.6877 - val_loss: 138.4357\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.0506 - val_loss: 135.8435\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.2792 - val_loss: 133.7978\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.2729 - val_loss: 131.5121\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.7150 - val_loss: 129.4451\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.2401 - val_loss: 128.1828\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.6226 - val_loss: 126.9412\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.6243 - val_loss: 127.1509\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.5790 - val_loss: 123.7571\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.5811 - val_loss: 122.4906\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.0495 - val_loss: 122.2951\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.1362 - val_loss: 124.8459\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.9681 - val_loss: 120.3173\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.2132 - val_loss: 120.7092\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.3076 - val_loss: 121.9810\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.1616 - val_loss: 120.7848\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  39 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 11286.4160 - val_loss: 2266.7239\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2799.5525 - val_loss: 2688.1199\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2374.0010 - val_loss: 1698.2336\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1904.7620 - val_loss: 1461.7206\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1588.9882 - val_loss: 1301.3428\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1368.5708 - val_loss: 1146.5203\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1185.7639 - val_loss: 1025.4414\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1039.0114 - val_loss: 906.8737\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 911.7429 - val_loss: 816.2574\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 818.2654 - val_loss: 733.4641\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 742.9020 - val_loss: 685.1214\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 684.2906 - val_loss: 642.1079\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 641.2386 - val_loss: 617.9229\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 613.6760 - val_loss: 596.7347\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 587.1516 - val_loss: 579.9203\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 569.9999 - val_loss: 564.4544\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 554.3360 - val_loss: 551.0742\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 539.2098 - val_loss: 541.9134\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 528.7797 - val_loss: 527.3374\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 514.5593 - val_loss: 521.4000\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 501.3977 - val_loss: 508.1133\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 490.3339 - val_loss: 499.8956\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 481.8179 - val_loss: 490.9653\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 471.3638 - val_loss: 479.8251\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 461.4077 - val_loss: 473.6450\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 451.5638 - val_loss: 462.6480\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 444.1241 - val_loss: 454.7184\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 433.7789 - val_loss: 446.8978\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 426.4036 - val_loss: 442.1460\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 419.1076 - val_loss: 431.1408\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 411.2714 - val_loss: 424.6032\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 403.2942 - val_loss: 419.8769\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 396.3546 - val_loss: 411.4925\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 387.4709 - val_loss: 405.2770\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 383.9955 - val_loss: 399.7276\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 375.7052 - val_loss: 393.0155\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 368.6291 - val_loss: 387.7086\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 362.0240 - val_loss: 380.9149\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 356.4458 - val_loss: 377.1634\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 351.4382 - val_loss: 371.7878\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 344.7491 - val_loss: 365.1528\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 340.4641 - val_loss: 360.6805\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 335.3218 - val_loss: 359.0175\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 333.2697 - val_loss: 350.4419\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 326.0259 - val_loss: 347.8320\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 321.6736 - val_loss: 341.2849\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 319.9403 - val_loss: 339.5496\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 315.4205 - val_loss: 333.0355\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 309.9527 - val_loss: 330.5299\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 305.2924 - val_loss: 326.2921\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  40 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 13623.5234 - val_loss: 3855.0823\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2287.2410 - val_loss: 1315.8672\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1154.4459 - val_loss: 1098.7538\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 954.0468 - val_loss: 861.6047\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 743.9790 - val_loss: 665.8047\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 599.5601 - val_loss: 555.0266\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 524.0500 - val_loss: 488.1556\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 480.0082 - val_loss: 442.7884\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 445.5491 - val_loss: 410.8375\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 420.4655 - val_loss: 386.4858\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 401.7295 - val_loss: 368.5883\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 389.5813 - val_loss: 351.7374\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 375.3980 - val_loss: 337.1667\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 362.8008 - val_loss: 326.0070\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 351.3629 - val_loss: 312.7610\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 340.1331 - val_loss: 301.8930\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 331.4164 - val_loss: 292.1290\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 322.6698 - val_loss: 284.5037\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 313.2872 - val_loss: 278.6799\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 305.6656 - val_loss: 271.3884\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 299.8548 - val_loss: 265.8163\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 293.0612 - val_loss: 262.6937\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 286.8741 - val_loss: 255.7222\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 279.8652 - val_loss: 250.6865\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 274.5272 - val_loss: 246.3506\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 269.1889 - val_loss: 243.4636\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 264.1345 - val_loss: 238.3140\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 259.0806 - val_loss: 233.7011\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 254.2258 - val_loss: 230.3493\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 249.5847 - val_loss: 225.8355\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.6539 - val_loss: 220.6660\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.8557 - val_loss: 218.1129\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.9145 - val_loss: 213.1588\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 232.0110 - val_loss: 209.4025\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 226.5872 - val_loss: 206.9262\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.7989 - val_loss: 202.8022\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.9941 - val_loss: 197.9010\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 213.7869 - val_loss: 195.7611\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.9380 - val_loss: 192.8753\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 205.2931 - val_loss: 190.1077\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.8733 - val_loss: 186.2024\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 197.6924 - val_loss: 185.3710\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.6064 - val_loss: 181.1402\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 190.8422 - val_loss: 178.9118\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.6774 - val_loss: 177.1079\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 184.2674 - val_loss: 173.4365\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.2703 - val_loss: 170.6263\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.9962 - val_loss: 170.0503\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 175.2161 - val_loss: 165.9921\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 172.2320 - val_loss: 163.3875\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 557us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  41 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 9881.4707 - val_loss: 4815.0894\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2761.6448 - val_loss: 1206.6191\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 800.7434 - val_loss: 494.7779\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 515.8608 - val_loss: 436.9523\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 474.5882 - val_loss: 396.8365\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 431.8944 - val_loss: 370.4965\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 407.7954 - val_loss: 356.8487\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 395.4091 - val_loss: 346.1070\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 386.6626 - val_loss: 338.8275\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 378.7513 - val_loss: 329.2133\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 368.1205 - val_loss: 311.4451\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 351.7446 - val_loss: 303.9253\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 337.1965 - val_loss: 303.0152\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 326.7381 - val_loss: 291.9670\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 322.0725 - val_loss: 286.2564\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 314.0290 - val_loss: 281.1968\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 310.4760 - val_loss: 274.6243\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 302.8273 - val_loss: 270.4210\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 295.0476 - val_loss: 264.0804\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 289.9679 - val_loss: 261.2206\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 285.0945 - val_loss: 254.9532\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 277.9809 - val_loss: 248.5850\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 270.0301 - val_loss: 241.3232\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.4821 - val_loss: 235.7124\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.0101 - val_loss: 227.3783\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 248.6133 - val_loss: 224.7846\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 242.5858 - val_loss: 214.2529\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 232.4673 - val_loss: 208.2610\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 225.7118 - val_loss: 199.5112\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 215.2064 - val_loss: 192.6877\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 206.4591 - val_loss: 186.2126\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.8803 - val_loss: 180.0631\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 197.3830 - val_loss: 174.2700\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 191.3192 - val_loss: 170.6077\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.9178 - val_loss: 165.7394\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.4217 - val_loss: 164.7868\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 178.5850 - val_loss: 159.5212\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.2345 - val_loss: 163.8073\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 178.7197 - val_loss: 154.8889\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 169.5078 - val_loss: 154.2198\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 169.4298 - val_loss: 154.2315\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.0435 - val_loss: 150.3091\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.1156 - val_loss: 147.5735\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.0790 - val_loss: 145.6658\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.9102 - val_loss: 143.4566\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.8250 - val_loss: 142.7003\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.6339 - val_loss: 139.8582\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.0747 - val_loss: 138.3610\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.2135 - val_loss: 136.8338\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.2044 - val_loss: 135.2590\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 667us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  42 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 32302.6133 - val_loss: 20922.2480\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 13083.7695 - val_loss: 7889.7554\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4384.2993 - val_loss: 2413.5815\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1296.8979 - val_loss: 894.5201\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 752.9041 - val_loss: 737.1951\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 665.5543 - val_loss: 656.0136\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 586.3872 - val_loss: 598.1417\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 535.3667 - val_loss: 551.7183\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 495.7849 - val_loss: 509.6294\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 461.3550 - val_loss: 475.2522\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 433.5230 - val_loss: 445.2849\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 411.7539 - val_loss: 421.1828\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 393.0618 - val_loss: 402.1445\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 376.0511 - val_loss: 381.6088\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 361.9723 - val_loss: 365.4087\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 349.4094 - val_loss: 348.9211\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 337.1742 - val_loss: 336.3947\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 325.0532 - val_loss: 323.5890\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 316.1443 - val_loss: 311.3900\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 307.4185 - val_loss: 303.2862\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 298.2753 - val_loss: 294.5971\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 291.6078 - val_loss: 286.4232\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 284.8763 - val_loss: 277.4395\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 277.6545 - val_loss: 272.2401\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 272.2678 - val_loss: 265.6113\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 265.7654 - val_loss: 258.4310\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 260.7263 - val_loss: 251.9647\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 254.9370 - val_loss: 246.5882\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 250.1811 - val_loss: 241.6848\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.0924 - val_loss: 236.7323\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 240.4368 - val_loss: 231.4787\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.9247 - val_loss: 226.8548\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 231.2770 - val_loss: 222.6122\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 226.3187 - val_loss: 217.9851\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.1101 - val_loss: 213.5599\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 218.0650 - val_loss: 209.6409\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 213.4900 - val_loss: 206.0376\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 210.7519 - val_loss: 201.8922\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 207.6893 - val_loss: 199.3184\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 201.5225 - val_loss: 194.6714\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 197.5121 - val_loss: 190.9683\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.8918 - val_loss: 186.3781\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 190.0645 - val_loss: 183.5759\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 186.9438 - val_loss: 179.9702\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 184.3037 - val_loss: 176.1968\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.4508 - val_loss: 173.2162\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.1639 - val_loss: 169.7399\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.8340 - val_loss: 166.3366\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 169.5415 - val_loss: 163.6279\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 166.4205 - val_loss: 160.4064\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 668us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  43 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 6465.2163 - val_loss: 3984.0940\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3168.3364 - val_loss: 2948.7048\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2244.2312 - val_loss: 2126.4900\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1665.1180 - val_loss: 1651.8295\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1261.5078 - val_loss: 1169.8446\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 884.6960 - val_loss: 856.9985\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 686.8163 - val_loss: 715.0209\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 570.0391 - val_loss: 608.1906\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 492.0180 - val_loss: 539.1144\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 431.9139 - val_loss: 483.3144\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 389.8236 - val_loss: 448.7093\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 350.7189 - val_loss: 411.1638\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 321.6877 - val_loss: 381.9532\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 300.4940 - val_loss: 356.8828\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 282.7070 - val_loss: 344.4262\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 263.4864 - val_loss: 320.2137\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 249.1288 - val_loss: 313.2422\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 233.0235 - val_loss: 291.7782\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 224.5506 - val_loss: 281.0875\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 211.6111 - val_loss: 269.6627\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.5197 - val_loss: 257.3029\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 191.9953 - val_loss: 246.6108\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 182.1495 - val_loss: 236.7325\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 172.5905 - val_loss: 225.2543\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.7373 - val_loss: 225.9107\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.5714 - val_loss: 210.8791\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.7001 - val_loss: 204.3512\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.4049 - val_loss: 195.7957\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.4092 - val_loss: 187.7711\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.0574 - val_loss: 182.3659\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.0874 - val_loss: 177.7491\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.3564 - val_loss: 178.2040\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.7185 - val_loss: 172.2558\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.3164 - val_loss: 167.8544\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.3685 - val_loss: 168.4909\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.1500 - val_loss: 162.7414\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.2133 - val_loss: 161.1815\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.1293 - val_loss: 161.5589\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.8926 - val_loss: 160.7764\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.6708 - val_loss: 157.3649\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.4387 - val_loss: 159.7594\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.9285 - val_loss: 157.0493\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.8895 - val_loss: 152.8768\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.0851 - val_loss: 169.0489\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.6901 - val_loss: 151.3899\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.6973 - val_loss: 149.6253\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.6321 - val_loss: 147.1620\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.3566 - val_loss: 147.7151\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.7822 - val_loss: 145.4424\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.7069 - val_loss: 146.7148\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  44 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 51516.9805 - val_loss: 11757.6641\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 6508.9551 - val_loss: 4922.8267\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5383.2607 - val_loss: 4260.3120\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4098.2690 - val_loss: 3189.3313\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3629.8945 - val_loss: 2880.2297\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3240.2126 - val_loss: 2626.6538\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2919.8948 - val_loss: 2347.5532\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2623.4231 - val_loss: 2102.3494\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2368.8967 - val_loss: 1894.0562\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2109.9272 - val_loss: 1685.4333\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1898.2592 - val_loss: 1501.7830\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1704.2781 - val_loss: 1376.8186\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1553.5170 - val_loss: 1240.7745\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1401.7086 - val_loss: 1104.2146\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1277.5830 - val_loss: 1009.1916\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1166.2345 - val_loss: 933.7389\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1071.1025 - val_loss: 844.9953\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 984.5856 - val_loss: 781.0817\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 910.3016 - val_loss: 729.0946\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 843.3743 - val_loss: 663.4356\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 781.2812 - val_loss: 631.4485\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 728.7643 - val_loss: 580.8468\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 678.8632 - val_loss: 556.9415\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 639.6966 - val_loss: 513.9544\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 597.8679 - val_loss: 494.9143\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 562.0738 - val_loss: 459.2973\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 531.3992 - val_loss: 444.0119\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 499.5983 - val_loss: 415.9137\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 476.7733 - val_loss: 398.0371\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 451.0516 - val_loss: 379.9536\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 431.4172 - val_loss: 376.0059\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 408.2471 - val_loss: 352.3685\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 390.5483 - val_loss: 348.2010\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 374.2735 - val_loss: 336.5433\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 359.7705 - val_loss: 320.4689\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 345.6259 - val_loss: 322.4313\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 336.1584 - val_loss: 303.4516\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 323.2149 - val_loss: 305.0414\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 313.6581 - val_loss: 292.7131\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 303.1429 - val_loss: 280.2835\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 294.3858 - val_loss: 286.8097\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 284.2801 - val_loss: 265.2557\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 279.2523 - val_loss: 270.4746\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 270.0602 - val_loss: 261.3501\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 265.0911 - val_loss: 259.9939\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 256.0887 - val_loss: 246.9293\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 250.4509 - val_loss: 249.1748\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.4251 - val_loss: 246.0148\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.9787 - val_loss: 231.3551\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.6915 - val_loss: 243.7352\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 554us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  45 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 17406.0508 - val_loss: 5467.3618\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3276.6218 - val_loss: 1449.6782\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1887.7163 - val_loss: 1453.3564\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1533.7598 - val_loss: 1027.3391\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1257.1072 - val_loss: 889.9072\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1095.4529 - val_loss: 785.4343\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 958.4132 - val_loss: 696.6266\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 850.2429 - val_loss: 618.6846\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 758.6084 - val_loss: 555.6557\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 683.9150 - val_loss: 506.5967\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 615.0341 - val_loss: 462.5428\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 565.2626 - val_loss: 426.3788\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 521.1637 - val_loss: 397.4070\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 483.6494 - val_loss: 370.7224\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 451.5663 - val_loss: 352.1375\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 424.9388 - val_loss: 329.9103\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 400.5359 - val_loss: 316.9207\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 377.7528 - val_loss: 298.5002\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 357.2849 - val_loss: 286.5590\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 340.0772 - val_loss: 274.7108\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 326.6491 - val_loss: 263.9803\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 312.2715 - val_loss: 252.9661\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 298.3659 - val_loss: 246.8615\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 286.4082 - val_loss: 236.7694\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 277.6988 - val_loss: 230.2599\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 266.4491 - val_loss: 222.0285\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.4460 - val_loss: 211.1312\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 242.4412 - val_loss: 205.4854\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 230.2625 - val_loss: 198.2654\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.6723 - val_loss: 193.3403\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 216.0263 - val_loss: 187.6822\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 209.1578 - val_loss: 183.3939\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.3979 - val_loss: 179.1009\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.3775 - val_loss: 176.4962\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.1908 - val_loss: 172.5351\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 186.5517 - val_loss: 168.6232\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 182.4648 - val_loss: 164.4888\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.7818 - val_loss: 162.0061\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 171.8644 - val_loss: 160.0570\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.6060 - val_loss: 156.7264\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.6054 - val_loss: 156.2627\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.0285 - val_loss: 150.8746\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.5015 - val_loss: 152.2611\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.7699 - val_loss: 147.1620\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.3701 - val_loss: 146.7943\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.7544 - val_loss: 145.1920\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.9236 - val_loss: 143.9112\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.1619 - val_loss: 140.9188\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.7741 - val_loss: 140.3100\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.5297 - val_loss: 138.6048\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  46 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1201.4142 - val_loss: 1004.5557\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 593.6609 - val_loss: 543.7477\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 353.6378 - val_loss: 411.9437\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 280.4755 - val_loss: 350.4032\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 258.9631 - val_loss: 305.9859\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 249.1707 - val_loss: 278.7660\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.0781 - val_loss: 267.6985\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 230.4461 - val_loss: 253.8705\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 223.8114 - val_loss: 242.7220\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 218.4770 - val_loss: 236.0331\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 213.9637 - val_loss: 228.4087\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 212.3223 - val_loss: 223.2865\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 209.0586 - val_loss: 222.9165\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 207.1037 - val_loss: 218.1176\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 204.3530 - val_loss: 216.7646\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.6037 - val_loss: 213.5327\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 204.4718 - val_loss: 213.0021\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.9030 - val_loss: 208.5706\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.4901 - val_loss: 208.6147\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 195.4035 - val_loss: 204.5542\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.8154 - val_loss: 202.3729\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 191.0200 - val_loss: 202.0375\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 189.3363 - val_loss: 200.9337\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 188.6205 - val_loss: 200.6282\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 186.1143 - val_loss: 199.1351\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 183.6956 - val_loss: 195.9775\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.6352 - val_loss: 194.3816\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.2323 - val_loss: 191.3115\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 178.3424 - val_loss: 191.6321\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 175.4173 - val_loss: 187.3556\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.1195 - val_loss: 184.6869\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.1038 - val_loss: 185.7693\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 166.3210 - val_loss: 180.4273\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.8466 - val_loss: 178.3977\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.4494 - val_loss: 175.4542\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.5803 - val_loss: 171.9543\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.9488 - val_loss: 170.1054\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.1323 - val_loss: 166.6807\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.4998 - val_loss: 156.5074\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.3189 - val_loss: 158.0890\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.6048 - val_loss: 148.1849\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.6044 - val_loss: 146.3237\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.0307 - val_loss: 142.9249\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.3287 - val_loss: 139.1607\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.4728 - val_loss: 139.7120\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.9257 - val_loss: 138.7444\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.3308 - val_loss: 137.1638\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.3936 - val_loss: 135.2688\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.5109 - val_loss: 134.7697\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.3081 - val_loss: 131.4382\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 664us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  47 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1342.1254 - val_loss: 634.1563\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 655.7849 - val_loss: 568.8286\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 524.2261 - val_loss: 496.4973\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 490.5136 - val_loss: 465.1815\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 460.8446 - val_loss: 429.9373\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 439.6915 - val_loss: 408.6609\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 422.2853 - val_loss: 386.7057\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 401.1258 - val_loss: 367.3983\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 385.4203 - val_loss: 346.4960\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 365.5334 - val_loss: 325.6895\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 340.2442 - val_loss: 294.8084\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 308.0840 - val_loss: 262.9217\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 281.8659 - val_loss: 246.6140\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 271.9585 - val_loss: 241.0027\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 268.8045 - val_loss: 236.2285\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 263.2254 - val_loss: 235.0886\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 260.2676 - val_loss: 229.3696\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.2830 - val_loss: 225.4226\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 251.5835 - val_loss: 224.3047\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 246.9850 - val_loss: 219.0697\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.6060 - val_loss: 221.6976\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.8780 - val_loss: 210.2333\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3530 - val_loss: 208.2561\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 231.4372 - val_loss: 203.7257\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 227.8702 - val_loss: 199.5280\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 221.8044 - val_loss: 197.4316\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 218.0658 - val_loss: 191.9044\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 212.7902 - val_loss: 191.4756\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 209.7715 - val_loss: 187.0715\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 205.1260 - val_loss: 185.2691\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 200.6013 - val_loss: 185.1204\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 204.0790 - val_loss: 181.5025\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 191.7218 - val_loss: 171.6505\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.2367 - val_loss: 167.9053\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.5459 - val_loss: 162.8817\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 175.1314 - val_loss: 168.2320\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.2219 - val_loss: 156.2160\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 166.9706 - val_loss: 153.7171\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.8292 - val_loss: 150.4615\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.2328 - val_loss: 156.1557\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.6939 - val_loss: 144.1611\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.0937 - val_loss: 141.7027\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.0102 - val_loss: 145.4937\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.3315 - val_loss: 136.5408\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.9451 - val_loss: 132.7329\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.6244 - val_loss: 135.7430\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.7300 - val_loss: 127.2981\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.2733 - val_loss: 124.4212\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.1479 - val_loss: 125.7087\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.0181 - val_loss: 122.7045\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 664us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  48 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 436.3375 - val_loss: 317.4107\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 315.8565 - val_loss: 315.3216\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 310.7547 - val_loss: 305.5466\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 304.1508 - val_loss: 300.6973\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 298.3986 - val_loss: 291.8543\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 292.5396 - val_loss: 285.0745\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 286.9167 - val_loss: 277.5074\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 281.0657 - val_loss: 268.4765\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 275.6081 - val_loss: 258.4489\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 270.0450 - val_loss: 251.2404\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 264.3549 - val_loss: 244.0398\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.5363 - val_loss: 234.8670\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.3521 - val_loss: 222.3731\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 227.1399 - val_loss: 207.0667\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 211.1370 - val_loss: 195.3446\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.3826 - val_loss: 186.0248\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.2224 - val_loss: 175.4894\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.5240 - val_loss: 169.7418\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 180.2766 - val_loss: 162.8329\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.4077 - val_loss: 159.0404\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.1252 - val_loss: 153.8418\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.0819 - val_loss: 150.3578\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.9704 - val_loss: 147.5942\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.0020 - val_loss: 143.7616\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.8967 - val_loss: 140.5632\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.4037 - val_loss: 139.5444\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.4830 - val_loss: 137.7604\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.1004 - val_loss: 134.3213\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.6653 - val_loss: 131.8599\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.9252 - val_loss: 133.4787\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.3756 - val_loss: 127.9481\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.4115 - val_loss: 127.9341\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.5154 - val_loss: 126.2008\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.3100 - val_loss: 124.1232\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.1309 - val_loss: 123.0902\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.8481 - val_loss: 122.0846\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.1565 - val_loss: 121.8321\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.9261 - val_loss: 120.9042\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.5822 - val_loss: 120.2783\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.4733 - val_loss: 120.1389\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.7924 - val_loss: 120.3461\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.5873 - val_loss: 118.2107\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.2461 - val_loss: 119.0534\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.7165 - val_loss: 117.8537\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.4463 - val_loss: 116.2603\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.2930 - val_loss: 116.2254\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.5677 - val_loss: 115.3733\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.9410 - val_loss: 115.6224\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.0423 - val_loss: 114.9915\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.9740 - val_loss: 118.1883\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  49 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 28197.9258 - val_loss: 12256.0771\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 6173.4302 - val_loss: 1795.0096\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 823.0941 - val_loss: 323.2002\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 315.1800 - val_loss: 290.7303\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 291.2127 - val_loss: 256.5738\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 261.8523 - val_loss: 250.7080\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 251.8766 - val_loss: 245.2464\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 247.1119 - val_loss: 241.4488\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.7250 - val_loss: 237.8571\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.9168 - val_loss: 234.0343\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.4573 - val_loss: 231.8658\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 233.5257 - val_loss: 228.5843\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 230.5878 - val_loss: 225.1998\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 227.1049 - val_loss: 222.1463\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 223.5353 - val_loss: 220.4905\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 220.7492 - val_loss: 217.4786\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 218.2670 - val_loss: 216.1809\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 214.3917 - val_loss: 211.8021\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 210.9775 - val_loss: 211.2503\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 207.5495 - val_loss: 206.9196\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.6814 - val_loss: 204.2222\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.9265 - val_loss: 200.8585\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 195.8296 - val_loss: 197.8250\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 190.5161 - val_loss: 194.1137\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 186.0941 - val_loss: 189.0782\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 182.1200 - val_loss: 185.2299\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.9036 - val_loss: 181.3748\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.4163 - val_loss: 172.9284\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.9364 - val_loss: 164.5111\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.9548 - val_loss: 157.1528\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.2156 - val_loss: 150.7843\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.2354 - val_loss: 145.6146\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.2172 - val_loss: 139.3105\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.6529 - val_loss: 133.4380\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.3603 - val_loss: 128.4858\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.0268 - val_loss: 124.7168\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.5213 - val_loss: 122.6952\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.5031 - val_loss: 119.2839\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.6647 - val_loss: 116.8070\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.7333 - val_loss: 115.3863\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.1014 - val_loss: 114.4915\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.2368 - val_loss: 115.0894\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.7546 - val_loss: 113.6539\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.5565 - val_loss: 110.6011\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.5133 - val_loss: 111.4754\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.8437 - val_loss: 117.5079\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.6427 - val_loss: 109.8603\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.8830 - val_loss: 108.9680\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.9003 - val_loss: 111.7236\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.1097 - val_loss: 108.6762\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  50 \n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1094.4240 - val_loss: 1022.7569\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 941.3463 - val_loss: 843.4937\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 822.5458 - val_loss: 757.9188\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 724.2125 - val_loss: 670.5167\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 624.0172 - val_loss: 676.8538\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 571.5685 - val_loss: 541.4186\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 508.5239 - val_loss: 491.4415\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 463.1577 - val_loss: 443.6994\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 414.9567 - val_loss: 402.1358\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 388.6928 - val_loss: 371.0704\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 353.9868 - val_loss: 335.7741\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 327.7587 - val_loss: 309.2441\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 295.6645 - val_loss: 294.5522\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 277.9315 - val_loss: 266.6911\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 264.9944 - val_loss: 252.0230\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.7478 - val_loss: 236.5875\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.8079 - val_loss: 261.2694\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 221.2855 - val_loss: 206.7579\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.7189 - val_loss: 212.9203\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 186.1063 - val_loss: 198.1068\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.4110 - val_loss: 179.6171\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 167.1575 - val_loss: 164.4986\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.2264 - val_loss: 160.2190\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.9778 - val_loss: 169.8946\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.5839 - val_loss: 163.9027\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.2449 - val_loss: 141.6853\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.6572 - val_loss: 135.2289\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.2955 - val_loss: 130.6952\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.9075 - val_loss: 126.7344\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.6380 - val_loss: 125.9865\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.7591 - val_loss: 121.3843\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.1221 - val_loss: 121.0163\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.1030 - val_loss: 138.7689\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.3658 - val_loss: 120.7179\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.7799 - val_loss: 113.5576\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.4764 - val_loss: 115.4824\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.0156 - val_loss: 112.2555\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.3027 - val_loss: 109.9039\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.6765 - val_loss: 111.4471\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.0254 - val_loss: 107.7325\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.2836 - val_loss: 108.2988\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.5419 - val_loss: 106.6442\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.7913 - val_loss: 110.2557\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.8854 - val_loss: 111.4561\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.7494 - val_loss: 105.9709\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.1463 - val_loss: 105.4417\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.9135 - val_loss: 104.5086\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.3796 - val_loss: 103.0970\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.7628 - val_loss: 103.4080\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.1201 - val_loss: 106.3831\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "Total Execution Time :  0:01:37.792925\n"
     ]
    }
   ],
   "source": [
    "# Create the for loop to split the data, create, compile & fit model, evaluate & nake predictions, caluclate mse and store\n",
    "# in list_of_mse\n",
    "\n",
    "start_time = datetime.now() # Starting time of the for loop execution\n",
    "\n",
    "for i in range(50) :\n",
    "    # Split the data into train and test set\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\n",
    "    \n",
    "    # Create and compile the regression model using the function regression_model as defined in TASK 1\n",
    "    model = regression_model()\n",
    "\n",
    "    # Fit the model on the train set\n",
    "    print('\\n\\n\\nTraining Model # ' , i+1 , '\\n\\n') # Print the Model Number that is being trained\n",
    "    model.fit(X_train, Y_train, validation_split=0.3, epochs=50)\n",
    "    print('\\n')\n",
    "    \n",
    "    # Make prediction on the test set\n",
    "    Y_predicted = model.predict(X_test)\n",
    "    \n",
    "    # Calculate the mean square error\n",
    "    mse = mean_squared_error(Y_test, Y_predicted)\n",
    "    \n",
    "    # Add the mse to the list_of_mse list\n",
    "    list_of_mse.append(mse)\n",
    "\n",
    "end_time = datetime.now() # Ending time of the for loop execution\n",
    "\n",
    "# Print time taken for fitting 50 models and calucating the Mean and Standard Deviation of MSE of 50 models\n",
    "print('\\n\\nTotal Execution Time : ' , format(end_time - start_time))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Mean of the MSE of 50 Models :  194.45041598952258\n",
      "Standard Deviation of MSE of 50 Models : 126.6524229841716\n"
     ]
    }
   ],
   "source": [
    "# Calculate the Mean of the MSE of 50 models\n",
    "mean_of_mse = stats.mean(list_of_mse)\n",
    "\n",
    "# Calculate the Standard Deviation of the MSE of 50 models\n",
    "std_of_mse = stats.stdev(list_of_mse)\n",
    "\n",
    "# Print the Mean and Standard Deviation of MSE of 50 models\n",
    "print('\\n\\nMean of the MSE of 50 Models : ' , str(mean_of_mse))\n",
    "print('Standard Deviation of MSE of 50 Models : ' + str(std_of_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = green> Comparision of Mean of MSE with Mean of MSE from PART A </font>\n",
    "<p/>\n",
    "\n",
    "<table style=\"width:20%\">\n",
    "  <tr>\n",
    "    <th>Mean of MSE of PART A</th>\n",
    "    <th>Mean of MSE of PART B</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>244.77</td>\n",
    "    <td>126.13</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "The table above compares the Mean of **MSE for PART A** and **Mean of MSE for PART B**. As can be seen, the value of Mean of MSE of PART B is significantly smaller than that of PART A. The mean squared error tells how close a regression line is to a set of points. The smaller the value, the closer the model is to finding the line of best fit. So the smaller value for PART B shows that the model is slightly closer to finding a line of best fit.\n",
    "\n",
    "To summarize, normalizing the features had a significant effect in reducing the MSE and finding the line of best fit\n",
    "\n",
    "<b>Note</b> : Depending on the data, it may be impossible to get a very small value for MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = blue> END OF PART B </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = blue> PART C : BASELINE MODEL WITH 100 EPOCHS </font>\n",
    "\n",
    "\n",
    "In this part, all the tasks from <b>PART B</b> are performed, but this time the number of epochs are increased to 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = darkorange>Task 1 : Train and Test the Baseline Model with Normalized Features and 100 epochs</font>\n",
    "\n",
    "In order to train and test the the baseline model with normalized features and 100 epochs, the following steps are performed :\n",
    "<ol>\n",
    "    <li>Normalize the features (X)</li>\n",
    "    <li>Randomly split the data into <i>X_train, X_test, Y_train, Y_test</i> sets</li>\n",
    "    <li>Create a new model with 100 epochs</li>\n",
    "    <li>Train the model using <i>X_train, Y_train</i> using <b>50</b> epochs</li>\n",
    "    <li>Evaluate the model using <i>X_test, Y_test</i></li>\n",
    "    <li>Get the predictions on the <i>X_test</i> set </li>\n",
    "    <li>Compute the <b>mean squared error</b> on the test set using sklearn</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #2980B9> Step 1 : Normalize the features (X) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note</b> : As the features (X) have already been normalized the features (X), hence this part is skipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #2980B9> Step 2 : Randomly split the data into <i>X_train, X_test, Y_train, Y_test </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X_train, X_test, Y_train and Y_test sets\n",
    "X_train, X_test, Y_train, Y_test = data_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #2980B9> Step 3 : Train the model using <i>X_train, Y_train</i> using 50 epochs </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note </b> : Since the regression model has already been created using `def regression_model` in <b>TASK 1</b> and there are no changes being made to it, hence there is no need to write the code for the model again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 69354.2422 - val_loss: 44521.9570\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 30232.7969 - val_loss: 15288.9619\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 8937.9150 - val_loss: 3865.6746\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2932.3972 - val_loss: 2626.5554\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2420.6802 - val_loss: 2386.0979\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2119.7288 - val_loss: 2049.5044\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1869.6747 - val_loss: 1840.7676\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1682.3641 - val_loss: 1665.4580\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1518.2476 - val_loss: 1506.4469\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1375.3462 - val_loss: 1364.6112\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1249.8822 - val_loss: 1237.1604\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1130.4558 - val_loss: 1090.7435\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 991.5054 - val_loss: 922.3232\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 854.4089 - val_loss: 796.1345\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 765.4644 - val_loss: 683.6593\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 654.9453 - val_loss: 582.2426\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 567.3398 - val_loss: 496.8628\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 492.9579 - val_loss: 427.1186\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 434.3293 - val_loss: 370.0355\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 379.9694 - val_loss: 315.5040\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 337.6182 - val_loss: 276.3868\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 305.7474 - val_loss: 248.5497\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 284.8494 - val_loss: 230.2726\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 273.7149 - val_loss: 217.8160\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 265.8685 - val_loss: 211.5981\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 259.8705 - val_loss: 203.7483\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 254.2444 - val_loss: 203.4203\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 250.2075 - val_loss: 190.2578\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.4368 - val_loss: 183.7822\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 227.9823 - val_loss: 180.1543\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.5997 - val_loss: 175.8201\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 212.4800 - val_loss: 177.0981\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.8878 - val_loss: 149.7656\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.4498 - val_loss: 146.0023\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.1886 - val_loss: 136.5433\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 166.0020 - val_loss: 131.6462\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.1464 - val_loss: 127.0163\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.9415 - val_loss: 130.1337\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.0611 - val_loss: 121.6802\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.2666 - val_loss: 118.7265\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.6716 - val_loss: 118.2149\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.8705 - val_loss: 113.7265\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.8921 - val_loss: 111.9903\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.5990 - val_loss: 109.4796\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.7773 - val_loss: 107.9624\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.5122 - val_loss: 107.3237\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.9071 - val_loss: 105.9423\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.2502 - val_loss: 105.7232\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.3442 - val_loss: 106.7921\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.7608 - val_loss: 110.1846\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.2770 - val_loss: 106.7396\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.7727 - val_loss: 115.9704\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.9967 - val_loss: 104.6249\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.4623 - val_loss: 104.4031\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.5253 - val_loss: 104.5847\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.5702 - val_loss: 103.7450\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.2216 - val_loss: 102.8432\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.3644 - val_loss: 106.8736\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.4050 - val_loss: 102.9802\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.0415 - val_loss: 107.6366\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.6501 - val_loss: 105.4646\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.8958 - val_loss: 108.9913\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.9350 - val_loss: 103.3113\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.7957 - val_loss: 101.7992\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.5656 - val_loss: 101.3568\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.4413 - val_loss: 101.6536\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.3778 - val_loss: 103.7527\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.1881 - val_loss: 103.9224\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.2962 - val_loss: 102.6067\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.2707 - val_loss: 101.1647\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.3401 - val_loss: 104.1579\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.3340 - val_loss: 101.1048\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.0538 - val_loss: 102.0871\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.5721 - val_loss: 101.9159\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.4834 - val_loss: 102.5832\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.4360 - val_loss: 102.7092\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 113.6018 - val_loss: 100.6941\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.4409 - val_loss: 101.7927\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.9803 - val_loss: 100.3414\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.2336 - val_loss: 105.3942\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.1754 - val_loss: 100.4444\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.2425 - val_loss: 100.6051\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.3286 - val_loss: 102.6139\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.3234 - val_loss: 107.6150\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.5150 - val_loss: 101.4947\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.9904 - val_loss: 100.8362\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.3381 - val_loss: 100.5202\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.8415 - val_loss: 101.0728\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.6124 - val_loss: 100.2265\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.0678 - val_loss: 100.7156\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.5044 - val_loss: 100.7470\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.4647 - val_loss: 115.6158\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.1056 - val_loss: 101.8139\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.9839 - val_loss: 100.2583\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.2710 - val_loss: 100.1352\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.4445 - val_loss: 100.2944\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.1185 - val_loss: 100.2202\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.0371 - val_loss: 102.4027\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.6531 - val_loss: 101.7489\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.6411 - val_loss: 100.1266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f8b6161030>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = regression_model()\n",
    "\n",
    "# Fit the model on the train set\n",
    "model.fit(X_train, Y_train, validation_split=0.3, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #2980B9> Step 4 : Get the predictions on the X_test </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 665us/step\n"
     ]
    }
   ],
   "source": [
    "# Store the predictions in a variable Y_Predicted\n",
    "Y_predicted = predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #2980B9> Step 5 : Compute the <i>mean squared error</i> on the test set using sklearn </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error (MSE) of the Baseline Model with Normalized Features is :  112.65548208046148\n"
     ]
    }
   ],
   "source": [
    "# Calculate the mean square error\n",
    "\n",
    "mse = calculate_mse()\n",
    "print('Mean Square Error (MSE) of the Baseline Model with Normalized Features is : ' , str(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = darkorange>Task 2 : Create 50 Models and Calculate the <i>µ</i> & <i>σ</i> of their MSE Errors with 100 Epochs</font>\n",
    "\n",
    "In order to train 50 models with new features (normalized features) and calulate the mean (µ) and standard deviation (σ) of their mean square errors (MSE) the following steps are performed :\n",
    "<ol>\n",
    "    <li>Create an empty list <code>list_of_mse</code> to store the mean square error of each of the models</li>\n",
    "    <li>Define a for loop and perform each of the following steps in the loop</li>\n",
    "        <ol>\n",
    "        <li>Randomly split the data into <i>X_train, X_test, Y_train, Y_test</i> sets</li>\n",
    "        <li>Train the model using <i>X_train, Y_train</i> using <b>50</b> epochs</li>\n",
    "        <li>Evaluate the model using <i>X_test, Y_test</i></li>\n",
    "        <li>Get the predictions on the <i>X_test</i> set </li>\n",
    "        <li>Compute the <b>mean squared error</b> on the test set using sklearn</li>\n",
    "    </ol>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #2980B9> Step 1 : Creating the <i>list_of_mse</i> list</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the empty lists\n",
    "list_of_mse = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #2980B9> Step 2 : Creating 50 Models, Calculating The MSE for Each and <i>µ</i> & <i>σ</i> of the 50 MSE Values in <i>list_of_mse</i> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Training Model #  1 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 77486.1406 - val_loss: 39772.5039\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 21364.6543 - val_loss: 5955.4810\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2772.9600 - val_loss: 1277.0931\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1495.8884 - val_loss: 1426.1152\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1279.3358 - val_loss: 1050.3345\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1091.3325 - val_loss: 955.9625\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1014.6115 - val_loss: 883.6682\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 942.0422 - val_loss: 817.1345\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 874.8395 - val_loss: 759.9186\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 815.2062 - val_loss: 700.3581\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 763.4207 - val_loss: 651.4123\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 713.0803 - val_loss: 615.0571\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 672.6057 - val_loss: 572.6699\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 632.7341 - val_loss: 543.9651\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 599.4930 - val_loss: 514.5045\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 570.5272 - val_loss: 490.1812\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 544.6713 - val_loss: 468.0496\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 522.2241 - val_loss: 448.1583\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 501.7134 - val_loss: 429.3553\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 486.1621 - val_loss: 413.6806\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 468.1283 - val_loss: 404.4832\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 451.9015 - val_loss: 387.7546\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 438.6323 - val_loss: 374.8097\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 427.5171 - val_loss: 363.7209\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 415.5892 - val_loss: 356.9773\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 407.0804 - val_loss: 345.1819\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 394.8228 - val_loss: 339.8265\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 383.3282 - val_loss: 328.1761\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 375.3439 - val_loss: 320.2657\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 367.3962 - val_loss: 316.0936\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 357.8306 - val_loss: 305.6911\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 350.1971 - val_loss: 299.9351\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 343.2422 - val_loss: 294.9553\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 337.0629 - val_loss: 289.6198\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 328.9055 - val_loss: 281.4487\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 323.2409 - val_loss: 277.1564\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 316.3686 - val_loss: 272.1844\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 311.1601 - val_loss: 266.4372\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 305.5600 - val_loss: 261.2827\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 299.4988 - val_loss: 258.2964\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 294.8110 - val_loss: 252.8797\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 290.0426 - val_loss: 248.0332\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 285.8064 - val_loss: 245.6611\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 280.1856 - val_loss: 241.0356\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 274.7229 - val_loss: 237.0431\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 270.2211 - val_loss: 233.6590\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 266.0744 - val_loss: 232.2996\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.3313 - val_loss: 226.6244\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 258.0847 - val_loss: 223.2848\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 254.4259 - val_loss: 221.2249\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 250.8345 - val_loss: 217.9328\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 246.5978 - val_loss: 215.1500\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.0129 - val_loss: 213.7882\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 240.5091 - val_loss: 209.6633\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.4695 - val_loss: 207.7428\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 233.1843 - val_loss: 204.2704\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 230.4962 - val_loss: 202.4579\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 227.7145 - val_loss: 200.0080\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 225.7597 - val_loss: 200.6749\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.3251 - val_loss: 195.3285\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 218.1149 - val_loss: 195.6255\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 215.5787 - val_loss: 191.2421\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 213.4467 - val_loss: 189.6092\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 209.9759 - val_loss: 189.1691\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.0207 - val_loss: 185.4227\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 205.7887 - val_loss: 184.5717\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.4410 - val_loss: 182.2483\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 201.4864 - val_loss: 181.7074\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.4101 - val_loss: 178.9450\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 195.8305 - val_loss: 178.6916\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.3971 - val_loss: 175.7736\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.4273 - val_loss: 174.7157\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 190.5404 - val_loss: 173.4891\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 188.2551 - val_loss: 172.0926\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.6634 - val_loss: 170.7744\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 184.1185 - val_loss: 169.6128\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 182.6709 - val_loss: 167.7705\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 182.1112 - val_loss: 166.3596\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.4728 - val_loss: 167.0958\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 178.0105 - val_loss: 163.8857\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 175.0473 - val_loss: 163.8322\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 174.0746 - val_loss: 161.9524\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.8581 - val_loss: 164.6014\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 169.8927 - val_loss: 159.2502\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 169.2289 - val_loss: 161.4089\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 166.3760 - val_loss: 157.0574\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.8118 - val_loss: 158.6784\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.2235 - val_loss: 154.9476\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.7439 - val_loss: 154.1878\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.6866 - val_loss: 153.7376\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.5142 - val_loss: 152.5803\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.4250 - val_loss: 153.9730\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.6396 - val_loss: 154.7522\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.1691 - val_loss: 150.2893\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.1998 - val_loss: 151.0913\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.2091 - val_loss: 148.4754\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.3392 - val_loss: 148.0174\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.2938 - val_loss: 150.0638\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.7210 - val_loss: 145.8371\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.5870 - val_loss: 147.4106\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 662us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  2 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 281862.2500 - val_loss: 211065.3125\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161481.6719 - val_loss: 115693.6562\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83465.8906 - val_loss: 55554.9844\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 38645.5078 - val_loss: 24482.9668\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 16351.2139 - val_loss: 9434.5303\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5876.8579 - val_loss: 2940.7466\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1698.6930 - val_loss: 895.6179\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 602.2098 - val_loss: 544.8119\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 454.5070 - val_loss: 533.7590\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 451.1674 - val_loss: 528.3065\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 444.0340 - val_loss: 519.0262\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 437.8474 - val_loss: 510.6467\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 432.1738 - val_loss: 504.0639\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 426.7779 - val_loss: 496.8260\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 421.2915 - val_loss: 490.2784\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 415.8431 - val_loss: 483.5248\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 411.2547 - val_loss: 477.1892\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 404.5854 - val_loss: 469.1589\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 399.1154 - val_loss: 462.5236\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 393.4504 - val_loss: 456.2177\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 388.2400 - val_loss: 449.8918\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 383.1479 - val_loss: 443.4874\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 376.6730 - val_loss: 436.0073\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 372.0934 - val_loss: 429.3634\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 366.5988 - val_loss: 423.0449\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 360.9471 - val_loss: 417.7597\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 357.1773 - val_loss: 412.5809\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 351.2677 - val_loss: 404.8453\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 346.8584 - val_loss: 400.5340\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 342.3423 - val_loss: 393.0893\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 337.0414 - val_loss: 389.3475\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 332.1356 - val_loss: 382.8532\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 330.2802 - val_loss: 376.3269\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 323.1114 - val_loss: 374.4946\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 318.7585 - val_loss: 367.6531\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 314.4750 - val_loss: 362.1909\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 311.0535 - val_loss: 357.0350\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 307.1366 - val_loss: 354.3806\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 303.0466 - val_loss: 348.6098\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 298.7338 - val_loss: 344.3872\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 295.4213 - val_loss: 341.4841\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 292.0194 - val_loss: 335.5351\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 287.6561 - val_loss: 332.6892\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 284.9680 - val_loss: 328.2876\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 281.1022 - val_loss: 325.4904\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 278.1212 - val_loss: 320.4507\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 276.1531 - val_loss: 318.7599\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 271.8444 - val_loss: 313.3506\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 271.2124 - val_loss: 310.4027\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 266.3484 - val_loss: 310.9571\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 264.1278 - val_loss: 305.3558\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 261.2535 - val_loss: 302.4121\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 259.1628 - val_loss: 298.6819\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 256.4366 - val_loss: 297.2685\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 254.5569 - val_loss: 294.7603\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 252.5636 - val_loss: 290.6562\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 249.1951 - val_loss: 290.7504\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 247.3953 - val_loss: 286.5284\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.6819 - val_loss: 285.0244\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 242.8740 - val_loss: 281.4235\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.4263 - val_loss: 281.4774\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.8627 - val_loss: 277.3868\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.6522 - val_loss: 275.3087\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 234.8571 - val_loss: 272.9089\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 233.4269 - val_loss: 272.0697\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 231.3193 - val_loss: 268.9190\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 229.7973 - val_loss: 266.6974\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 229.2605 - val_loss: 267.8354\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 225.6966 - val_loss: 262.1631\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 224.5249 - val_loss: 262.3629\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.7193 - val_loss: 260.8388\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 221.2553 - val_loss: 258.5935\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 219.7348 - val_loss: 255.6932\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 218.9119 - val_loss: 256.7237\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 216.5421 - val_loss: 252.6916\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 215.9052 - val_loss: 252.8133\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 214.3016 - val_loss: 248.9168\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 213.0005 - val_loss: 249.2221\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 211.5934 - val_loss: 246.1151\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 210.1198 - val_loss: 245.4788\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.7393 - val_loss: 244.8955\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 207.3204 - val_loss: 243.2834\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 206.7747 - val_loss: 241.6449\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 205.0530 - val_loss: 241.2132\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 204.4108 - val_loss: 240.0559\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.9395 - val_loss: 235.7954\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.0696 - val_loss: 236.8587\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 200.4981 - val_loss: 233.1722\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.4086 - val_loss: 234.0479\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.1992 - val_loss: 231.7670\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 197.8571 - val_loss: 232.3506\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.0312 - val_loss: 229.1037\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 195.5615 - val_loss: 226.3911\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.5421 - val_loss: 230.6109\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.5995 - val_loss: 225.5050\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 193.4254 - val_loss: 226.3633\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.2831 - val_loss: 221.7261\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 190.6273 - val_loss: 221.9241\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 189.2221 - val_loss: 222.9512\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 188.8505 - val_loss: 218.2162\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 554us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  3 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2289.2073 - val_loss: 1001.7289\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 846.8123 - val_loss: 592.1913\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 534.8697 - val_loss: 384.6636\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 363.2240 - val_loss: 313.0143\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 286.8580 - val_loss: 264.6496\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 242.4774 - val_loss: 243.1178\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 218.0468 - val_loss: 234.6183\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 206.3528 - val_loss: 228.5952\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.5934 - val_loss: 225.0975\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.1199 - val_loss: 221.8873\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 190.5074 - val_loss: 220.2368\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 186.1099 - val_loss: 214.5226\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 182.4405 - val_loss: 212.9697\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.1839 - val_loss: 208.3641\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.2934 - val_loss: 204.3186\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.5059 - val_loss: 201.1031\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 171.3812 - val_loss: 198.6467\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 169.4475 - val_loss: 194.7079\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.7695 - val_loss: 191.8727\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.7036 - val_loss: 187.7088\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.6133 - val_loss: 184.9782\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.0931 - val_loss: 181.4167\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.8691 - val_loss: 179.0328\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.3624 - val_loss: 175.8426\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.6824 - val_loss: 173.8858\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.6333 - val_loss: 171.7489\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.0536 - val_loss: 169.6408\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.7701 - val_loss: 167.2033\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.7628 - val_loss: 165.0984\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.4587 - val_loss: 162.3969\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.1977 - val_loss: 159.2017\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.8573 - val_loss: 159.8072\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.4318 - val_loss: 155.4034\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.3650 - val_loss: 151.8647\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.7375 - val_loss: 148.7399\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.6228 - val_loss: 145.9868\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.9204 - val_loss: 148.4855\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.2066 - val_loss: 143.2680\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.8239 - val_loss: 141.7400\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.3625 - val_loss: 138.8026\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.7159 - val_loss: 142.0617\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.6022 - val_loss: 135.6607\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.6197 - val_loss: 137.2761\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.0472 - val_loss: 130.8517\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.8177 - val_loss: 128.5769\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.0256 - val_loss: 128.0611\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.9991 - val_loss: 126.1936\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.5456 - val_loss: 123.3115\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 121.3321 - val_loss: 122.0288\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.0013 - val_loss: 119.6699\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.2429 - val_loss: 118.1515\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.8186 - val_loss: 115.5970\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.8633 - val_loss: 115.5268\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.3620 - val_loss: 112.3940\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.4165 - val_loss: 115.1229\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.2125 - val_loss: 110.0847\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.3160 - val_loss: 119.7024\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.8618 - val_loss: 108.8597\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.5341 - val_loss: 109.1001\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.8828 - val_loss: 111.7333\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.4582 - val_loss: 109.1971\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.3072 - val_loss: 107.6571\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.1826 - val_loss: 106.1786\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.7467 - val_loss: 110.8282\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.8424 - val_loss: 103.6060\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.2352 - val_loss: 107.0653\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.3164 - val_loss: 101.8696\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.9355 - val_loss: 101.3320\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.5453 - val_loss: 102.7892\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.5097 - val_loss: 102.4044\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.5121 - val_loss: 99.3361\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.3460 - val_loss: 98.4855\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.1111 - val_loss: 97.8728\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.9980 - val_loss: 97.4517\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.8255 - val_loss: 98.4094\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.1983 - val_loss: 98.2212\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.0333 - val_loss: 95.7004\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.3439 - val_loss: 96.5565\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.8907 - val_loss: 94.5620\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.3471 - val_loss: 99.8381\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.2946 - val_loss: 102.2917\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.4101 - val_loss: 93.5668\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.2114 - val_loss: 92.1926\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.0100 - val_loss: 92.3127\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.7392 - val_loss: 91.1773\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.7072 - val_loss: 92.6416\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.1196 - val_loss: 91.8868\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.7644 - val_loss: 90.9670\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.8500 - val_loss: 97.5128\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.5130 - val_loss: 88.4246\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.7866 - val_loss: 87.5107\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.2440 - val_loss: 87.0500\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.6046 - val_loss: 87.1399\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.4832 - val_loss: 87.9197\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.4892 - val_loss: 87.1953\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.7430 - val_loss: 85.2359\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.4396 - val_loss: 84.6776\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.0315 - val_loss: 86.9754\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.0813 - val_loss: 83.3019\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.4707 - val_loss: 82.8341\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  4 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 55810.3008 - val_loss: 28713.9102\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 16095.0762 - val_loss: 7250.6484\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3596.2307 - val_loss: 1558.7649\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 786.7794 - val_loss: 579.7776\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 473.5083 - val_loss: 503.6592\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 471.3280 - val_loss: 486.7461\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 451.3748 - val_loss: 476.9529\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 432.4719 - val_loss: 468.7325\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 419.2805 - val_loss: 458.9817\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 407.8136 - val_loss: 443.5552\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 397.2341 - val_loss: 431.5933\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 386.9747 - val_loss: 422.0584\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 377.0135 - val_loss: 412.6764\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 368.3386 - val_loss: 400.9587\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 360.7945 - val_loss: 394.0526\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 352.9113 - val_loss: 382.8047\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 344.9303 - val_loss: 376.6751\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 338.5126 - val_loss: 372.3967\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 331.1154 - val_loss: 359.5111\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 325.1212 - val_loss: 353.4783\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 317.9808 - val_loss: 347.3661\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 312.3300 - val_loss: 339.9874\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 306.0938 - val_loss: 332.2580\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 300.6239 - val_loss: 327.3180\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 294.9084 - val_loss: 320.5426\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 289.9368 - val_loss: 315.9800\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 285.2097 - val_loss: 305.6719\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 279.4257 - val_loss: 305.6653\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 274.8379 - val_loss: 297.3770\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 271.2271 - val_loss: 296.4497\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 265.0960 - val_loss: 284.2292\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 260.9472 - val_loss: 284.7550\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 256.9729 - val_loss: 276.5709\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 251.6966 - val_loss: 271.5233\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 247.2561 - val_loss: 269.0064\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.3439 - val_loss: 265.3845\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.5193 - val_loss: 259.5193\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.8722 - val_loss: 254.0683\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 232.5860 - val_loss: 253.2635\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 228.3096 - val_loss: 245.7397\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 224.9801 - val_loss: 244.2111\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 221.7656 - val_loss: 239.5779\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 218.9772 - val_loss: 237.5713\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 215.1502 - val_loss: 233.4954\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 212.1839 - val_loss: 229.1779\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 209.0504 - val_loss: 225.7275\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 206.3671 - val_loss: 224.8234\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.4823 - val_loss: 218.0403\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 200.7302 - val_loss: 218.0853\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.1784 - val_loss: 213.3589\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 195.2595 - val_loss: 212.5164\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.9114 - val_loss: 207.4588\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 190.8542 - val_loss: 206.9073\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 188.3376 - val_loss: 202.4885\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.8476 - val_loss: 200.3493\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 184.0059 - val_loss: 198.8883\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 182.0534 - val_loss: 199.3999\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.5154 - val_loss: 195.2864\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.4877 - val_loss: 190.4272\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 175.2239 - val_loss: 192.7609\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.1489 - val_loss: 187.6240\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 172.3430 - val_loss: 186.8865\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 169.8632 - val_loss: 182.0774\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 167.3215 - val_loss: 182.4605\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.6893 - val_loss: 178.4548\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.9603 - val_loss: 177.3459\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.1828 - val_loss: 173.7422\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.9826 - val_loss: 173.7024\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.0216 - val_loss: 174.3290\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.4179 - val_loss: 171.3308\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.8824 - val_loss: 170.5822\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.2384 - val_loss: 169.4249\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.5267 - val_loss: 166.0542\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.0343 - val_loss: 164.0316\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.6334 - val_loss: 161.4476\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.0388 - val_loss: 161.8740\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.1617 - val_loss: 160.4840\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.2374 - val_loss: 160.2897\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.5755 - val_loss: 156.7509\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.1518 - val_loss: 156.6776\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.7849 - val_loss: 153.6446\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.9239 - val_loss: 157.3199\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.2336 - val_loss: 153.1780\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.3036 - val_loss: 154.4923\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.2011 - val_loss: 150.4104\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.6090 - val_loss: 151.5313\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.7516 - val_loss: 151.0728\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.0215 - val_loss: 147.1879\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.9276 - val_loss: 146.6990\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.5998 - val_loss: 146.1438\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.1397 - val_loss: 143.7745\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.1023 - val_loss: 145.0770\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.2201 - val_loss: 143.8481\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.5568 - val_loss: 141.5923\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.8350 - val_loss: 142.9252\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.1909 - val_loss: 140.2061\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.7354 - val_loss: 142.3111\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.3433 - val_loss: 140.5419\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.3564 - val_loss: 140.0927\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.2502 - val_loss: 136.8628\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 663us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  5 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 447.4651 - val_loss: 393.7605\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 348.8618 - val_loss: 302.8743\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 290.0532 - val_loss: 254.7269\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 253.3945 - val_loss: 220.0500\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 221.5644 - val_loss: 202.5587\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.4531 - val_loss: 183.6306\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 188.1181 - val_loss: 171.1572\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 175.2859 - val_loss: 161.7928\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.3650 - val_loss: 151.2696\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 153.2758 - val_loss: 136.1762\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.0854 - val_loss: 127.7962\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.3629 - val_loss: 119.6116\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.0604 - val_loss: 109.7563\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.3248 - val_loss: 104.4689\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.5427 - val_loss: 101.4648\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.4447 - val_loss: 102.5929\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.5933 - val_loss: 97.9961\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.8093 - val_loss: 95.1805\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.4930 - val_loss: 93.7659\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.2969 - val_loss: 93.3343\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.1690 - val_loss: 91.8908\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.2381 - val_loss: 92.8569\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.2144 - val_loss: 94.0605\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.7688 - val_loss: 91.1192\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.3838 - val_loss: 89.4280\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.2665 - val_loss: 92.3685\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.2303 - val_loss: 94.4467\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.5339 - val_loss: 87.2642\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.5992 - val_loss: 87.1812\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.2361 - val_loss: 87.0870\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.0490 - val_loss: 85.8488\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.4527 - val_loss: 84.7362\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.4700 - val_loss: 86.4425\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.1791 - val_loss: 83.5307\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.9825 - val_loss: 85.7335\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.0005 - val_loss: 83.3908\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.1249 - val_loss: 81.3485\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 74.7578 - val_loss: 80.3607\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 74.2054 - val_loss: 82.6516\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 74.2369 - val_loss: 81.4634\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 74.3038 - val_loss: 80.1875\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 72.8869 - val_loss: 78.7362\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 73.1464 - val_loss: 78.7786\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 73.1167 - val_loss: 77.9205\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 70.4251 - val_loss: 76.9433\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 69.5105 - val_loss: 79.3904\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 72.0415 - val_loss: 76.2845\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 71.6805 - val_loss: 75.8331\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 69.3566 - val_loss: 75.6403\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 67.8671 - val_loss: 75.0485\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 68.4891 - val_loss: 74.6361\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 67.0242 - val_loss: 74.3077\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 69.2961 - val_loss: 76.2902\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 69.0186 - val_loss: 77.3274\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 67.1194 - val_loss: 76.1323\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 66.5853 - val_loss: 75.1188\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 65.8816 - val_loss: 72.8103\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 65.1626 - val_loss: 74.2989\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 64.2060 - val_loss: 75.9772\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 70.8364 - val_loss: 71.9784\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 64.6275 - val_loss: 72.5637\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 65.6947 - val_loss: 71.5351\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 64.1700 - val_loss: 71.4195\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 63.0966 - val_loss: 71.1458\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 63.5089 - val_loss: 70.5692\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 63.1423 - val_loss: 70.6665\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 62.7362 - val_loss: 71.3244\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 64.9084 - val_loss: 70.3811\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 62.6705 - val_loss: 70.7097\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 62.9711 - val_loss: 70.7291\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 61.6207 - val_loss: 69.6924\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 62.5980 - val_loss: 70.9550\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 63.3579 - val_loss: 70.3075\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 65.7113 - val_loss: 68.8417\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 65.0892 - val_loss: 68.8985\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 63.6537 - val_loss: 79.9679\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 67.9199 - val_loss: 75.5475\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 61.0684 - val_loss: 73.3618\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 64.3293 - val_loss: 72.4706\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 61.9468 - val_loss: 68.5722\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 60.7748 - val_loss: 67.6168\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 60.2115 - val_loss: 67.7272\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 59.5587 - val_loss: 66.9670\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 63.9608 - val_loss: 70.0857\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 62.1181 - val_loss: 68.3560\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 60.4861 - val_loss: 67.2548\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 61.1497 - val_loss: 66.7602\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 60.4823 - val_loss: 67.4255\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 60.1199 - val_loss: 66.1778\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 58.9471 - val_loss: 67.4028\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 61.1869 - val_loss: 70.1155\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 62.8199 - val_loss: 65.8431\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 62.4848 - val_loss: 68.2745\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 58.5381 - val_loss: 65.2385\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 57.9712 - val_loss: 67.6007\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 59.4868 - val_loss: 68.6664\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 61.9493 - val_loss: 65.2477\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 62.8242 - val_loss: 65.8884\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 60.3333 - val_loss: 70.5287\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 64.6050 - val_loss: 72.2352\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 772us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  6 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 120139.0547 - val_loss: 86849.0938\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 66744.5312 - val_loss: 47581.1758\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 36381.2891 - val_loss: 25762.6035\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 19642.9023 - val_loss: 13667.4863\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 10360.6553 - val_loss: 7065.6348\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5369.4941 - val_loss: 3596.9773\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2778.1160 - val_loss: 1909.3102\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1524.9568 - val_loss: 1114.3901\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 926.9740 - val_loss: 732.3445\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 640.7286 - val_loss: 544.6722\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 498.3220 - val_loss: 446.3117\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 424.7567 - val_loss: 393.1944\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 383.2376 - val_loss: 361.6689\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 356.7072 - val_loss: 338.9979\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 337.4408 - val_loss: 320.3740\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 320.9112 - val_loss: 305.5413\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 308.2960 - val_loss: 292.5819\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 297.4666 - val_loss: 280.9467\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 287.5198 - val_loss: 270.8368\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 278.1171 - val_loss: 261.8792\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 269.8977 - val_loss: 253.2648\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.2601 - val_loss: 245.5946\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.7865 - val_loss: 238.5328\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 249.5763 - val_loss: 232.2536\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.1054 - val_loss: 226.6712\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.2067 - val_loss: 222.3473\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.6974 - val_loss: 218.2285\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 232.2823 - val_loss: 214.9239\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 229.3521 - val_loss: 212.0940\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 227.0181 - val_loss: 209.7375\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 224.8896 - val_loss: 207.9164\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 223.1046 - val_loss: 206.5311\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 221.8787 - val_loss: 204.8796\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 220.0665 - val_loss: 203.7061\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 218.6835 - val_loss: 202.6757\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.4702 - val_loss: 201.8288\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 216.4160 - val_loss: 200.9372\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 215.5016 - val_loss: 200.2055\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 214.3398 - val_loss: 199.5132\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 213.4615 - val_loss: 198.8152\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 212.5682 - val_loss: 198.1892\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 211.8806 - val_loss: 197.5639\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 211.0149 - val_loss: 197.0240\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 210.3971 - val_loss: 196.5489\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 209.5304 - val_loss: 195.9803\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.8613 - val_loss: 195.4097\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.0976 - val_loss: 194.9021\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 207.4654 - val_loss: 194.3839\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 206.8004 - val_loss: 193.8850\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 206.1207 - val_loss: 193.3433\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 205.4488 - val_loss: 192.8257\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 204.7574 - val_loss: 192.3938\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 204.1376 - val_loss: 191.9511\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.3748 - val_loss: 191.5026\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.8202 - val_loss: 191.0772\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.1179 - val_loss: 190.6630\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 201.5578 - val_loss: 190.2887\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 200.9286 - val_loss: 189.8594\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 200.2908 - val_loss: 189.4702\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.6281 - val_loss: 189.0286\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.3409 - val_loss: 188.6514\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.4307 - val_loss: 188.1953\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 197.8784 - val_loss: 187.7228\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 197.3413 - val_loss: 187.2958\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.5178 - val_loss: 186.8755\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.0306 - val_loss: 186.4267\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 195.4628 - val_loss: 186.0318\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.7572 - val_loss: 185.5704\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.3488 - val_loss: 185.1595\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.5297 - val_loss: 184.7033\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.9586 - val_loss: 184.3089\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.4290 - val_loss: 183.8477\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 191.7640 - val_loss: 183.4905\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 191.2459 - val_loss: 183.0437\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 190.5834 - val_loss: 182.6421\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 190.0124 - val_loss: 182.3088\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 189.3952 - val_loss: 181.9151\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 188.7692 - val_loss: 181.5411\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 188.1874 - val_loss: 181.1489\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.7133 - val_loss: 180.8448\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.0754 - val_loss: 180.4614\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 186.3713 - val_loss: 180.1662\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.7625 - val_loss: 179.8190\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.1682 - val_loss: 179.5796\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 184.4432 - val_loss: 179.2627\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 183.8291 - val_loss: 178.9460\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 183.2486 - val_loss: 178.5962\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 182.8777 - val_loss: 178.1165\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 181.9593 - val_loss: 177.7588\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.3976 - val_loss: 177.4529\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.7418 - val_loss: 176.9773\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.5621 - val_loss: 176.5618\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.4144 - val_loss: 176.2337\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.0801 - val_loss: 175.9970\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 178.6134 - val_loss: 175.4036\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.7649 - val_loss: 175.0809\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.1397 - val_loss: 174.6975\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.5998 - val_loss: 174.2313\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 175.9862 - val_loss: 173.7956\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 175.4519 - val_loss: 173.3236\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 668us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  7 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3805.4221 - val_loss: 2189.1821\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2144.4087 - val_loss: 1513.6504\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1522.4789 - val_loss: 1127.9879\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1157.7639 - val_loss: 854.1447\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 936.9607 - val_loss: 713.3616\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 800.0597 - val_loss: 636.5996\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 725.6981 - val_loss: 583.2026\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 670.2828 - val_loss: 539.0991\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 622.8892 - val_loss: 499.5258\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 578.6219 - val_loss: 468.6013\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 540.1616 - val_loss: 441.9080\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 511.1038 - val_loss: 414.0808\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 483.5292 - val_loss: 392.3833\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 462.4618 - val_loss: 374.9842\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 439.2305 - val_loss: 364.5886\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 415.7962 - val_loss: 344.7153\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 390.6440 - val_loss: 331.0031\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 376.4821 - val_loss: 317.4134\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 359.8070 - val_loss: 302.3889\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 342.6685 - val_loss: 291.8662\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 330.0877 - val_loss: 281.2206\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 319.5077 - val_loss: 271.8766\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 307.1161 - val_loss: 260.4214\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 294.0764 - val_loss: 251.9267\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 285.8368 - val_loss: 246.1286\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 275.5120 - val_loss: 238.8004\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 273.3524 - val_loss: 245.5941\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 270.2083 - val_loss: 226.6687\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 257.1335 - val_loss: 222.0290\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 247.8873 - val_loss: 221.9859\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.9936 - val_loss: 211.9091\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 232.6834 - val_loss: 204.1286\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 221.8705 - val_loss: 200.2989\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 219.0924 - val_loss: 202.5255\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 212.3497 - val_loss: 192.9137\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.9439 - val_loss: 190.1430\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.3208 - val_loss: 188.2445\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.9110 - val_loss: 186.2048\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.5204 - val_loss: 187.1343\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 193.0965 - val_loss: 200.6846\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 188.4321 - val_loss: 184.1727\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.4543 - val_loss: 178.7633\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.6761 - val_loss: 180.8879\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.8566 - val_loss: 173.1401\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 171.5161 - val_loss: 170.4675\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.7088 - val_loss: 167.6173\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.0527 - val_loss: 168.5317\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.0665 - val_loss: 162.2623\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.7211 - val_loss: 160.3482\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.5590 - val_loss: 157.8705\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.7089 - val_loss: 155.7021\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.2828 - val_loss: 154.6040\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.4559 - val_loss: 151.2989\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.1059 - val_loss: 149.1548\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.1777 - val_loss: 145.7941\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.8457 - val_loss: 143.1642\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.2640 - val_loss: 141.6877\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.8033 - val_loss: 141.6779\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.2029 - val_loss: 136.1118\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.0045 - val_loss: 134.5141\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.9916 - val_loss: 131.5706\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.0376 - val_loss: 131.8151\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.8732 - val_loss: 129.8395\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.1179 - val_loss: 127.2812\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.8328 - val_loss: 127.5244\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.4344 - val_loss: 123.7017\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.5870 - val_loss: 121.5320\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.0745 - val_loss: 120.7479\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.4030 - val_loss: 119.7709\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.6690 - val_loss: 117.8156\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.7371 - val_loss: 118.0978\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.7506 - val_loss: 114.2719\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.0157 - val_loss: 114.0685\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.4415 - val_loss: 113.0251\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.5626 - val_loss: 112.5414\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 106.9303 - val_loss: 118.0433\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.1001 - val_loss: 120.3847\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.9379 - val_loss: 108.2016\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.5474 - val_loss: 109.5628\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.7033 - val_loss: 106.6292\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.6707 - val_loss: 106.4237\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.3757 - val_loss: 105.6182\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.0961 - val_loss: 104.2653\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.2798 - val_loss: 103.8089\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.3540 - val_loss: 102.2726\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.8376 - val_loss: 101.1438\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.3314 - val_loss: 100.4208\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.8714 - val_loss: 100.2287\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.4249 - val_loss: 98.9867\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.1615 - val_loss: 98.0790\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.6833 - val_loss: 98.0883\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.7329 - val_loss: 97.5859\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.5839 - val_loss: 96.8252\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.1870 - val_loss: 101.4795\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.6450 - val_loss: 94.8237\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.5020 - val_loss: 93.6528\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.2899 - val_loss: 92.5713\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.4283 - val_loss: 92.9170\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.5309 - val_loss: 95.6869\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.8839 - val_loss: 93.2694\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 662us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  8 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 40579.7734 - val_loss: 6787.3716\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2422.6265 - val_loss: 1965.5411\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2185.5564 - val_loss: 1186.1179\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1159.6204 - val_loss: 968.1524\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1073.5078 - val_loss: 838.5320\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 948.8613 - val_loss: 764.8583\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 890.2213 - val_loss: 719.5906\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 833.8414 - val_loss: 681.8911\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 794.0577 - val_loss: 656.0526\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 758.9562 - val_loss: 628.4136\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 729.1860 - val_loss: 612.6651\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 698.8483 - val_loss: 590.6304\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 676.3682 - val_loss: 573.8856\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 654.1834 - val_loss: 556.8278\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 630.8816 - val_loss: 542.9530\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 611.1012 - val_loss: 524.1357\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 589.0745 - val_loss: 512.0951\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 571.2128 - val_loss: 494.0546\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 556.8652 - val_loss: 483.9311\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 539.8234 - val_loss: 464.8277\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 516.5692 - val_loss: 454.1618\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 504.4566 - val_loss: 433.9688\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 487.5840 - val_loss: 419.6977\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 472.6437 - val_loss: 405.3660\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 458.4499 - val_loss: 394.2502\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 445.2495 - val_loss: 377.2408\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 431.7701 - val_loss: 363.9528\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 418.1450 - val_loss: 350.8076\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 407.8116 - val_loss: 337.4700\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 396.0845 - val_loss: 325.8704\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 385.9280 - val_loss: 314.4495\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 373.1046 - val_loss: 303.8445\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 365.5701 - val_loss: 290.6477\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 353.3547 - val_loss: 283.4268\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 346.5522 - val_loss: 270.0376\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 338.1702 - val_loss: 259.9795\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 332.2561 - val_loss: 250.3554\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 322.6581 - val_loss: 242.9695\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 311.7389 - val_loss: 233.1266\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 305.6450 - val_loss: 225.3073\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 299.9716 - val_loss: 217.7995\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 293.0026 - val_loss: 211.4707\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 286.3235 - val_loss: 203.3115\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 279.7420 - val_loss: 197.8503\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 274.0565 - val_loss: 190.2082\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 269.7747 - val_loss: 188.6911\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 263.1294 - val_loss: 179.6222\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 256.8125 - val_loss: 177.2511\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 251.6209 - val_loss: 171.8900\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.6417 - val_loss: 165.8202\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 240.0030 - val_loss: 159.4629\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 231.9021 - val_loss: 152.2712\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 227.4569 - val_loss: 147.8849\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 219.6288 - val_loss: 143.2937\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 214.4166 - val_loss: 139.5271\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 210.1818 - val_loss: 135.8218\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 205.4841 - val_loss: 135.5959\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 200.6113 - val_loss: 129.4374\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 200.6618 - val_loss: 133.4807\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.5503 - val_loss: 124.8254\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 190.9051 - val_loss: 121.6483\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.2175 - val_loss: 121.0521\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 182.3646 - val_loss: 124.8754\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.1946 - val_loss: 118.8840\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 178.4058 - val_loss: 119.8568\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 172.3163 - val_loss: 112.9269\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 172.9780 - val_loss: 117.3605\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.4567 - val_loss: 123.8103\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 171.2521 - val_loss: 123.0268\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.5960 - val_loss: 107.4030\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.7071 - val_loss: 119.9478\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.6575 - val_loss: 108.4316\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.5705 - val_loss: 108.9664\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.9956 - val_loss: 123.5082\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.2925 - val_loss: 112.4140\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.0836 - val_loss: 107.8080\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.6624 - val_loss: 106.5818\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.2229 - val_loss: 118.0779\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.4045 - val_loss: 112.4054\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.5169 - val_loss: 122.8492\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.8834 - val_loss: 107.4561\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.2160 - val_loss: 114.6033\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.4757 - val_loss: 106.4779\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.4113 - val_loss: 119.9537\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.2460 - val_loss: 103.2087\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.4865 - val_loss: 103.5795\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.2239 - val_loss: 105.0663\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.4210 - val_loss: 103.3452\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.0000 - val_loss: 108.9222\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.3123 - val_loss: 108.0506\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.0323 - val_loss: 110.8959\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.3831 - val_loss: 106.6849\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.1958 - val_loss: 103.6618\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.2650 - val_loss: 102.7687\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.0877 - val_loss: 107.7099\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.4111 - val_loss: 113.4018\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.4894 - val_loss: 110.8593\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.3228 - val_loss: 116.6041\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.6190 - val_loss: 105.0227\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.6859 - val_loss: 102.2350\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 553us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  9 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 10768.5176 - val_loss: 967.2595\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1773.2585 - val_loss: 1339.2852\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 843.4175 - val_loss: 638.9794\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 674.1029 - val_loss: 483.1841\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 546.1095 - val_loss: 419.3766\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 485.6012 - val_loss: 386.3153\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 463.3484 - val_loss: 371.6400\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 436.8610 - val_loss: 352.2340\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 427.1778 - val_loss: 343.5045\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 405.9879 - val_loss: 330.5847\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 388.6681 - val_loss: 311.6533\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 374.8799 - val_loss: 299.6353\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 357.5199 - val_loss: 289.8311\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 345.0478 - val_loss: 289.8122\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 327.4828 - val_loss: 264.0501\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 313.3557 - val_loss: 252.4534\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 303.2607 - val_loss: 241.7574\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 296.7735 - val_loss: 236.0051\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 278.0661 - val_loss: 225.2647\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.5275 - val_loss: 213.0617\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 253.2156 - val_loss: 216.7896\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.1558 - val_loss: 198.3760\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.5829 - val_loss: 187.1047\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 223.5465 - val_loss: 177.1287\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 215.2561 - val_loss: 175.2408\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 218.5193 - val_loss: 165.5975\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.9332 - val_loss: 161.9508\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.8535 - val_loss: 170.6576\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.2553 - val_loss: 152.9824\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.0235 - val_loss: 145.3313\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 172.2420 - val_loss: 141.6861\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.3170 - val_loss: 139.0583\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.9634 - val_loss: 135.7303\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.9180 - val_loss: 131.4342\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.2395 - val_loss: 129.5638\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.6102 - val_loss: 126.8170\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.4597 - val_loss: 131.8384\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.0665 - val_loss: 123.8009\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.8566 - val_loss: 119.8358\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.7303 - val_loss: 122.6369\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.7235 - val_loss: 118.4352\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.7485 - val_loss: 116.9499\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.3628 - val_loss: 113.3903\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.4752 - val_loss: 115.9466\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.5169 - val_loss: 118.8795\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.7265 - val_loss: 109.0010\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.9066 - val_loss: 108.2418\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.6357 - val_loss: 106.9586\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.6410 - val_loss: 119.7326\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.1128 - val_loss: 115.0836\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.0836 - val_loss: 105.8406\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.5930 - val_loss: 103.2877\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.7015 - val_loss: 103.8287\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.2724 - val_loss: 102.3942\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.5715 - val_loss: 103.7808\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.6028 - val_loss: 106.5096\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.7957 - val_loss: 99.8711\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.8606 - val_loss: 100.4318\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.0945 - val_loss: 101.4239\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.1377 - val_loss: 116.6480\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.3252 - val_loss: 122.1487\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.9005 - val_loss: 99.1108\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.8199 - val_loss: 98.2835\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.4098 - val_loss: 112.7441\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.9641 - val_loss: 97.6775\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.8064 - val_loss: 97.0443\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.5590 - val_loss: 101.9822\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.9702 - val_loss: 108.7163\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.5426 - val_loss: 99.5880\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.3334 - val_loss: 100.4171\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.8430 - val_loss: 98.6148\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.2948 - val_loss: 96.7863\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.7542 - val_loss: 95.8880\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.5847 - val_loss: 113.9238\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.1766 - val_loss: 97.1569\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.1642 - val_loss: 96.3643\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.3771 - val_loss: 99.7840\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.1982 - val_loss: 98.7770\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.3525 - val_loss: 97.2544\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.5991 - val_loss: 96.6885\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.6485 - val_loss: 100.4205\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 97.0621 - val_loss: 97.8435\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.6614 - val_loss: 96.7264\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.2481 - val_loss: 110.2052\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.0074 - val_loss: 95.7214\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.3229 - val_loss: 95.2130\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.0466 - val_loss: 99.6164\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.0315 - val_loss: 95.4484\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.8134 - val_loss: 97.8667\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.9851 - val_loss: 97.9100\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.3117 - val_loss: 110.3999\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.4965 - val_loss: 95.9399\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.7467 - val_loss: 94.7208\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.2026 - val_loss: 101.0895\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.0030 - val_loss: 101.6389\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.6390 - val_loss: 94.8420\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.8115 - val_loss: 95.3595\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.5084 - val_loss: 95.3301\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.8396 - val_loss: 95.6302\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.0782 - val_loss: 95.8306\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 668us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  10 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3088.1609 - val_loss: 753.0176\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 423.9122 - val_loss: 343.2060\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 368.3111 - val_loss: 304.4716\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 300.8884 - val_loss: 286.5590\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 299.9527 - val_loss: 284.7629\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 295.4360 - val_loss: 281.2272\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 292.6609 - val_loss: 280.2690\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 292.6204 - val_loss: 278.6753\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 289.8424 - val_loss: 278.9973\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 287.7653 - val_loss: 276.4286\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 285.8596 - val_loss: 276.1214\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 284.3423 - val_loss: 275.2066\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 283.2697 - val_loss: 273.9803\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 282.9483 - val_loss: 272.9278\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 282.0920 - val_loss: 273.7177\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 281.8517 - val_loss: 271.6366\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 278.6653 - val_loss: 271.8212\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 277.6989 - val_loss: 270.6949\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 277.2530 - val_loss: 269.4998\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 277.9641 - val_loss: 268.6206\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 276.7267 - val_loss: 269.4365\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 275.3971 - val_loss: 267.8279\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 273.5581 - val_loss: 267.6776\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 272.0429 - val_loss: 267.8596\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 271.9797 - val_loss: 266.2904\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 270.3574 - val_loss: 265.4032\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 271.8316 - val_loss: 267.3359\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 270.7406 - val_loss: 263.8627\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 269.4734 - val_loss: 263.5740\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 268.2832 - val_loss: 263.1342\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 267.0309 - val_loss: 264.2285\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 265.9916 - val_loss: 261.9414\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 267.9781 - val_loss: 262.2187\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 265.6167 - val_loss: 260.2992\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 266.5485 - val_loss: 259.0541\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 261.9315 - val_loss: 261.0921\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 263.6071 - val_loss: 257.4485\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 265.4180 - val_loss: 259.7381\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.2675 - val_loss: 257.1947\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 259.4265 - val_loss: 256.4886\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 259.3401 - val_loss: 256.1614\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 258.4922 - val_loss: 253.8631\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 257.7356 - val_loss: 253.0921\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.3802 - val_loss: 252.8457\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 253.6846 - val_loss: 252.4429\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 253.6024 - val_loss: 251.0193\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 252.2470 - val_loss: 251.0399\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 251.1881 - val_loss: 250.1369\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 250.9983 - val_loss: 249.3999\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 249.3042 - val_loss: 248.3980\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 247.7768 - val_loss: 246.6741\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 246.1141 - val_loss: 245.5110\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 246.3983 - val_loss: 246.1247\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.1616 - val_loss: 244.8234\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 242.3191 - val_loss: 244.1926\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 242.7117 - val_loss: 242.2990\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.7727 - val_loss: 243.1206\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.8977 - val_loss: 240.4327\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 240.6770 - val_loss: 244.8592\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 242.5485 - val_loss: 238.4246\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.0451 - val_loss: 240.0305\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.6195 - val_loss: 250.7878\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 242.3962 - val_loss: 235.4577\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 232.7205 - val_loss: 234.4186\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 231.0462 - val_loss: 233.3190\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 231.0932 - val_loss: 232.7479\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 229.2935 - val_loss: 231.2925\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 227.5177 - val_loss: 230.1016\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 227.8966 - val_loss: 229.5372\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 224.4053 - val_loss: 233.8328\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 225.7070 - val_loss: 230.4999\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 223.1424 - val_loss: 240.7922\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 230.0963 - val_loss: 225.7839\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.2830 - val_loss: 223.1549\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.9415 - val_loss: 222.0329\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 216.1794 - val_loss: 220.7160\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.9743 - val_loss: 233.3646\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 226.2105 - val_loss: 221.0879\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 213.8308 - val_loss: 216.6194\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 210.9220 - val_loss: 216.3827\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.5335 - val_loss: 213.4240\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 211.0796 - val_loss: 214.3396\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 208.4437 - val_loss: 212.2591\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 209.3954 - val_loss: 216.6849\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.1658 - val_loss: 207.9716\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.2343 - val_loss: 208.2260\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.8198 - val_loss: 214.4556\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.5687 - val_loss: 203.2178\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 195.7111 - val_loss: 206.9346\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.4344 - val_loss: 199.9599\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.2616 - val_loss: 197.8381\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 190.3944 - val_loss: 197.9176\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 188.8432 - val_loss: 194.6047\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.5699 - val_loss: 192.3312\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.1302 - val_loss: 190.8568\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 184.4944 - val_loss: 190.0868\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 184.5655 - val_loss: 186.8572\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.2694 - val_loss: 185.1698\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.5704 - val_loss: 190.6388\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.5386 - val_loss: 180.9212\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 668us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  11 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 111923.2734 - val_loss: 76218.1406\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 54284.9609 - val_loss: 35154.1367\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 23698.6484 - val_loss: 14090.4893\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 8614.7461 - val_loss: 4549.9033\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2525.5923 - val_loss: 1550.9901\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1009.8865 - val_loss: 1132.2880\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 882.6085 - val_loss: 1112.2587\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 866.6517 - val_loss: 1067.4602\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 831.4209 - val_loss: 1024.3536\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 802.6737 - val_loss: 987.9100\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 776.4371 - val_loss: 952.2311\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 750.4758 - val_loss: 915.7798\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 724.8335 - val_loss: 881.3194\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 701.1343 - val_loss: 846.3812\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 678.3843 - val_loss: 814.4512\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 653.5310 - val_loss: 783.5932\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 632.2004 - val_loss: 753.7369\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 611.4828 - val_loss: 721.7345\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 589.9323 - val_loss: 694.2200\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 571.9544 - val_loss: 666.3305\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 551.5632 - val_loss: 641.1494\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 534.0153 - val_loss: 616.1064\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 517.5994 - val_loss: 592.8782\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 501.9223 - val_loss: 570.6570\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 485.4439 - val_loss: 548.9033\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 471.1823 - val_loss: 527.9300\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 456.5104 - val_loss: 509.9447\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 443.8814 - val_loss: 491.0981\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 431.4193 - val_loss: 473.8423\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 420.6065 - val_loss: 457.0279\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 407.9392 - val_loss: 441.4571\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 398.2456 - val_loss: 427.3188\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 388.5116 - val_loss: 412.9603\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 378.7349 - val_loss: 400.6969\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 370.5822 - val_loss: 388.0074\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 361.0618 - val_loss: 376.9555\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 353.6179 - val_loss: 365.6270\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 345.9679 - val_loss: 355.3824\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 339.0155 - val_loss: 345.4125\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 332.5241 - val_loss: 336.0578\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 325.8893 - val_loss: 328.4888\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 319.4249 - val_loss: 319.7334\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 314.6038 - val_loss: 312.3102\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 308.2850 - val_loss: 305.9639\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 303.2091 - val_loss: 297.6768\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 297.9503 - val_loss: 291.5994\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 294.0091 - val_loss: 286.4568\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 288.8243 - val_loss: 278.7860\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 284.6871 - val_loss: 273.9181\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 279.9492 - val_loss: 268.3284\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 275.9114 - val_loss: 263.1558\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 272.1526 - val_loss: 258.6763\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 268.9574 - val_loss: 253.8709\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 265.7551 - val_loss: 250.0866\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 261.5737 - val_loss: 245.5494\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 258.8885 - val_loss: 241.8313\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 254.9200 - val_loss: 238.4182\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 252.8979 - val_loss: 235.3839\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 248.5284 - val_loss: 230.9585\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 246.8848 - val_loss: 227.8147\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.9125 - val_loss: 224.7815\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.9712 - val_loss: 221.6629\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.0909 - val_loss: 218.4178\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 234.7124 - val_loss: 216.1999\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 231.5171 - val_loss: 213.1136\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 228.0417 - val_loss: 211.3565\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 225.9156 - val_loss: 208.3467\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.7142 - val_loss: 205.6491\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 219.7117 - val_loss: 203.1956\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.6743 - val_loss: 201.2992\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 215.2177 - val_loss: 198.4587\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 211.5290 - val_loss: 197.2377\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 209.1110 - val_loss: 194.7694\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 206.5896 - val_loss: 192.7965\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 204.2534 - val_loss: 190.7780\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.5517 - val_loss: 188.8019\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.3235 - val_loss: 187.6653\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 197.0663 - val_loss: 185.6878\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 195.0722 - val_loss: 184.3502\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.7213 - val_loss: 182.8092\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.2530 - val_loss: 181.5277\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 191.2418 - val_loss: 181.7738\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.4967 - val_loss: 178.6306\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 184.3872 - val_loss: 177.4641\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 182.6798 - val_loss: 176.0013\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.5806 - val_loss: 174.7816\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.1978 - val_loss: 174.3901\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.8851 - val_loss: 172.3188\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 175.3212 - val_loss: 171.3208\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.6412 - val_loss: 170.5663\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 172.1124 - val_loss: 170.9055\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.4831 - val_loss: 168.5661\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.7773 - val_loss: 167.8305\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 167.0686 - val_loss: 166.4670\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 166.0228 - val_loss: 166.2883\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.6881 - val_loss: 164.7245\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.3971 - val_loss: 164.3707\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.0062 - val_loss: 162.7626\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.2977 - val_loss: 161.9836\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.5077 - val_loss: 161.1633\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 664us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  12 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2858.7815 - val_loss: 2825.7881\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1975.5302 - val_loss: 2199.4067\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1531.1372 - val_loss: 1763.1608\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1246.8365 - val_loss: 1380.7419\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 974.3510 - val_loss: 1079.2961\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 774.7343 - val_loss: 856.3668\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 648.4534 - val_loss: 756.3790\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 587.6593 - val_loss: 640.5560\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 519.8778 - val_loss: 567.1103\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 466.9231 - val_loss: 506.7872\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 427.4333 - val_loss: 458.9586\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 397.8045 - val_loss: 413.1314\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 367.3571 - val_loss: 374.2073\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 337.4004 - val_loss: 343.1133\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 308.8216 - val_loss: 323.4179\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 283.5000 - val_loss: 286.7635\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 267.3724 - val_loss: 265.0017\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 246.5706 - val_loss: 240.8319\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 231.2705 - val_loss: 221.9223\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 216.1866 - val_loss: 210.5998\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 206.3380 - val_loss: 197.8006\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 201.6050 - val_loss: 187.2256\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.0975 - val_loss: 199.3401\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.2039 - val_loss: 171.2366\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 172.5353 - val_loss: 170.4758\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.7879 - val_loss: 161.7447\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 165.5392 - val_loss: 159.9937\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.9464 - val_loss: 156.4166\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.5079 - val_loss: 155.3888\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.8072 - val_loss: 150.8122\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.0120 - val_loss: 149.1322\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.2957 - val_loss: 145.4756\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.1054 - val_loss: 144.1291\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.3472 - val_loss: 142.2912\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.0037 - val_loss: 142.6256\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.7956 - val_loss: 138.0062\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.3878 - val_loss: 141.0031\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.5527 - val_loss: 135.8838\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.4838 - val_loss: 136.3479\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.9527 - val_loss: 133.6007\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.0592 - val_loss: 130.3359\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.7300 - val_loss: 130.6417\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.2043 - val_loss: 134.3059\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.2868 - val_loss: 136.9365\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.1279 - val_loss: 126.0108\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.4006 - val_loss: 124.6327\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.4774 - val_loss: 124.9606\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.9641 - val_loss: 131.7043\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.7579 - val_loss: 127.3560\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.9037 - val_loss: 124.6306\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.3209 - val_loss: 120.7960\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.4291 - val_loss: 151.8112\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.1511 - val_loss: 119.1429\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 132.6570 - val_loss: 121.9446\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.8138 - val_loss: 117.5138\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.3656 - val_loss: 117.6296\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.7538 - val_loss: 119.4975\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.0520 - val_loss: 116.4395\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.8571 - val_loss: 126.0617\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.6008 - val_loss: 131.0806\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.1340 - val_loss: 117.0388\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.6046 - val_loss: 116.7123\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.3509 - val_loss: 114.5940\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.4034 - val_loss: 119.7864\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.1919 - val_loss: 130.3719\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.7205 - val_loss: 113.3300\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.5583 - val_loss: 114.4393\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.2095 - val_loss: 118.0229\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.5393 - val_loss: 115.9541\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.2682 - val_loss: 111.1814\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.8646 - val_loss: 113.5626\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.1172 - val_loss: 111.7828\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.0194 - val_loss: 111.4526\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.2854 - val_loss: 110.4082\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.8296 - val_loss: 113.7182\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.2050 - val_loss: 122.3406\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.1863 - val_loss: 112.2072\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.0212 - val_loss: 116.5351\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.5814 - val_loss: 109.2080\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.0007 - val_loss: 118.0017\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.0210 - val_loss: 108.9141\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.0589 - val_loss: 111.7833\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.7888 - val_loss: 112.1673\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.4435 - val_loss: 119.9622\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.2807 - val_loss: 110.1756\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.4802 - val_loss: 117.9052\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.7395 - val_loss: 117.7417\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.6058 - val_loss: 109.1785\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.8584 - val_loss: 108.8849\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.5124 - val_loss: 109.0607\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.6011 - val_loss: 108.7732\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.9914 - val_loss: 110.5592\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.1188 - val_loss: 117.0009\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.2279 - val_loss: 112.2876\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.4308 - val_loss: 109.1441\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.3961 - val_loss: 119.7829\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.3303 - val_loss: 118.6111\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.5247 - val_loss: 118.9644\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.7123 - val_loss: 108.4295\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.8690 - val_loss: 107.7073\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 557us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  13 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 532.4346 - val_loss: 397.5939\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 365.0888 - val_loss: 302.2147\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 292.8783 - val_loss: 256.9405\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 253.6807 - val_loss: 227.4055\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.2482 - val_loss: 220.5196\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.8370 - val_loss: 188.5690\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 190.6202 - val_loss: 183.4947\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 183.8730 - val_loss: 168.1929\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 172.5915 - val_loss: 169.1071\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.8528 - val_loss: 154.3013\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.1839 - val_loss: 154.5874\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.3846 - val_loss: 144.2033\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.6885 - val_loss: 139.6408\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.6134 - val_loss: 134.1778\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.8773 - val_loss: 141.7063\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.5414 - val_loss: 155.7032\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.1337 - val_loss: 136.2330\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.0480 - val_loss: 155.3707\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.2001 - val_loss: 106.8389\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.4570 - val_loss: 102.0091\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.3870 - val_loss: 119.1404\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.7572 - val_loss: 114.0205\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.5445 - val_loss: 98.1855\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.1942 - val_loss: 94.9280\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.6015 - val_loss: 106.8103\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.6344 - val_loss: 111.3360\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.6357 - val_loss: 88.4655\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.0309 - val_loss: 95.0953\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.8440 - val_loss: 86.1052\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.5461 - val_loss: 91.7197\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.7071 - val_loss: 88.8835\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.9525 - val_loss: 81.3288\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.7773 - val_loss: 85.1682\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.7874 - val_loss: 78.3765\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.4958 - val_loss: 78.3139\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.0883 - val_loss: 76.8456\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.4850 - val_loss: 94.5891\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 80.9831 - val_loss: 79.2777\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.6294 - val_loss: 77.5491\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.3940 - val_loss: 81.7985\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.9715 - val_loss: 82.7575\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.9205 - val_loss: 77.0843\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 74.5866 - val_loss: 76.3444\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 73.4286 - val_loss: 74.4308\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 72.6595 - val_loss: 71.3240\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 74.0372 - val_loss: 70.3530\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 72.3536 - val_loss: 75.0970\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.6453 - val_loss: 78.6685\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 72.9051 - val_loss: 68.2105\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 69.9438 - val_loss: 67.6908\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 70.2292 - val_loss: 72.1108\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 71.6019 - val_loss: 75.6359\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 69.6748 - val_loss: 67.6873\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 67.8218 - val_loss: 66.4538\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 67.5201 - val_loss: 67.2891\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 64.9936 - val_loss: 73.1524\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 68.9577 - val_loss: 74.2943\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 69.2808 - val_loss: 65.1097\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 68.3528 - val_loss: 63.9862\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 63.7451 - val_loss: 63.1185\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 63.4230 - val_loss: 67.7940\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 63.2069 - val_loss: 66.2781\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 62.7740 - val_loss: 61.8995\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 61.2308 - val_loss: 61.1936\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 63.3946 - val_loss: 62.7160\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 63.0919 - val_loss: 63.4073\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 60.5187 - val_loss: 59.2133\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 61.6505 - val_loss: 58.8493\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 61.7465 - val_loss: 58.4882\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 59.0816 - val_loss: 60.2418\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 60.2312 - val_loss: 57.7670\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 59.7936 - val_loss: 57.4291\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 62.6114 - val_loss: 68.6336\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 61.9401 - val_loss: 57.1744\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 57.9578 - val_loss: 56.2707\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 59.5427 - val_loss: 58.3030\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 58.5122 - val_loss: 56.1748\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 60.0111 - val_loss: 56.5586\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 59.9892 - val_loss: 55.5659\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 57.0117 - val_loss: 56.2131\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 55.7139 - val_loss: 56.4020\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 56.0390 - val_loss: 56.9827\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 57.0930 - val_loss: 54.4541\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 57.5896 - val_loss: 64.6885\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 58.6927 - val_loss: 53.8139\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 55.3079 - val_loss: 53.6923\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 56.2718 - val_loss: 53.2237\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 57.2890 - val_loss: 66.9664\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 57.9575 - val_loss: 64.6164\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 57.7227 - val_loss: 79.7705\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 58.5928 - val_loss: 52.3399\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 59.7799 - val_loss: 62.3885\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 65.7109 - val_loss: 62.3271\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 58.6248 - val_loss: 55.4697\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 54.7445 - val_loss: 55.1767\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 55.7810 - val_loss: 56.2426\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 56.1562 - val_loss: 54.1725\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 55.6624 - val_loss: 64.9815\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 55.8222 - val_loss: 53.8360\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 55.0346 - val_loss: 59.1497\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 662us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  14 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4957.5127 - val_loss: 1622.4407\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1662.1416 - val_loss: 1655.5603\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1372.1138 - val_loss: 1223.8589\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1184.5549 - val_loss: 1083.5945\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1023.3867 - val_loss: 945.3162\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 895.4720 - val_loss: 840.5067\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 800.1277 - val_loss: 776.4388\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 737.1286 - val_loss: 719.9961\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 683.8420 - val_loss: 680.2427\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 642.2841 - val_loss: 641.2437\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 608.1354 - val_loss: 605.1947\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 573.3854 - val_loss: 574.4955\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 542.2263 - val_loss: 544.4166\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 515.9138 - val_loss: 518.0076\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 490.8132 - val_loss: 488.6911\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 472.7928 - val_loss: 470.1068\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 450.7979 - val_loss: 439.2534\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 426.9037 - val_loss: 426.7036\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 414.7285 - val_loss: 399.0224\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 390.0893 - val_loss: 385.6185\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 375.8816 - val_loss: 363.7072\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 361.0452 - val_loss: 353.5969\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 347.3342 - val_loss: 334.1425\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 335.2196 - val_loss: 322.2671\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 328.3473 - val_loss: 305.5593\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 315.0112 - val_loss: 299.5686\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 306.6927 - val_loss: 290.4398\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 297.0614 - val_loss: 272.3279\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 288.7057 - val_loss: 269.8822\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 279.4691 - val_loss: 253.6626\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 277.7677 - val_loss: 249.4743\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 266.2214 - val_loss: 237.0747\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 261.2279 - val_loss: 235.2789\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 256.1541 - val_loss: 233.0572\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 249.5174 - val_loss: 217.8662\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.5289 - val_loss: 215.9571\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.1962 - val_loss: 206.6480\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 240.6944 - val_loss: 202.3241\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 233.9889 - val_loss: 202.8918\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 226.5067 - val_loss: 194.8306\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.8723 - val_loss: 189.2679\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 218.5032 - val_loss: 187.0291\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 215.4717 - val_loss: 190.3394\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 215.9051 - val_loss: 180.6439\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 213.1402 - val_loss: 182.1612\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 209.6823 - val_loss: 177.5254\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 204.6053 - val_loss: 174.0146\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.1370 - val_loss: 167.2105\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.6301 - val_loss: 169.4267\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.5851 - val_loss: 161.7873\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.3684 - val_loss: 169.6431\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 190.0082 - val_loss: 157.2572\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 186.9020 - val_loss: 154.1531\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.2982 - val_loss: 159.1592\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.2488 - val_loss: 150.2287\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 178.5195 - val_loss: 152.6016\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 178.2733 - val_loss: 148.8062\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.3194 - val_loss: 143.4519\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.7197 - val_loss: 141.4536\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 172.5784 - val_loss: 139.9986\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 171.8189 - val_loss: 138.7753\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.0207 - val_loss: 137.1883\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.6578 - val_loss: 136.6375\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.5749 - val_loss: 135.5452\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.9480 - val_loss: 136.2351\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.8169 - val_loss: 132.8871\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.1286 - val_loss: 133.0807\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.7331 - val_loss: 129.7210\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.9230 - val_loss: 129.9307\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.0530 - val_loss: 128.5200\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 158.7545 - val_loss: 127.5528\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.2371 - val_loss: 137.0828\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.2612 - val_loss: 126.0194\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.8210 - val_loss: 124.2647\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.4835 - val_loss: 133.3612\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.4353 - val_loss: 121.9017\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.2039 - val_loss: 121.4555\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.7144 - val_loss: 124.6177\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.3251 - val_loss: 124.2394\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.1028 - val_loss: 128.7337\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.6313 - val_loss: 119.3386\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.8539 - val_loss: 117.2647\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.4765 - val_loss: 119.5406\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.1255 - val_loss: 115.6477\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.4752 - val_loss: 130.3918\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.6194 - val_loss: 124.9632\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.6901 - val_loss: 115.8518\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.1136 - val_loss: 118.8865\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.3037 - val_loss: 113.1525\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.3605 - val_loss: 116.4981\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.8580 - val_loss: 115.0204\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.8267 - val_loss: 113.7488\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.4648 - val_loss: 111.2337\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.8778 - val_loss: 111.6813\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.5595 - val_loss: 111.2946\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.5308 - val_loss: 121.0777\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.5473 - val_loss: 115.0379\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.9040 - val_loss: 114.2998\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.4622 - val_loss: 111.0792\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.5096 - val_loss: 108.8989\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  15 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 17900.3438 - val_loss: 10534.0381\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 6085.9536 - val_loss: 5114.8745\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4606.5713 - val_loss: 4157.8042\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3642.0410 - val_loss: 3467.2910\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2941.2083 - val_loss: 2995.1165\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2502.8215 - val_loss: 2601.3521\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2190.0376 - val_loss: 2274.3530\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1930.8361 - val_loss: 2034.2266\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1684.4683 - val_loss: 1758.5226\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1472.5184 - val_loss: 1502.5883\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1239.0966 - val_loss: 1244.8104\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 959.8168 - val_loss: 905.1719\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 718.1036 - val_loss: 686.8731\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 547.1030 - val_loss: 564.5767\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 476.0478 - val_loss: 464.5252\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 416.7260 - val_loss: 415.5336\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 376.8560 - val_loss: 380.4619\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 349.7445 - val_loss: 354.1976\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 332.3234 - val_loss: 333.1607\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 312.4673 - val_loss: 318.2234\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 297.6404 - val_loss: 303.0721\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 285.6158 - val_loss: 290.1249\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 279.5331 - val_loss: 279.7983\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 267.8498 - val_loss: 270.7530\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 257.0230 - val_loss: 262.4214\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 249.0627 - val_loss: 253.8492\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.0620 - val_loss: 247.3978\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.2786 - val_loss: 242.1397\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 227.9287 - val_loss: 234.5267\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 219.9992 - val_loss: 228.6606\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 214.4228 - val_loss: 223.1881\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.6820 - val_loss: 218.0304\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 205.3320 - val_loss: 212.4463\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.8818 - val_loss: 208.0459\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 197.9482 - val_loss: 203.4182\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 190.6797 - val_loss: 203.7712\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 188.9989 - val_loss: 198.1258\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.1005 - val_loss: 197.0319\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.6538 - val_loss: 189.4634\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.4781 - val_loss: 184.9388\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.1586 - val_loss: 183.7284\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.0462 - val_loss: 178.9127\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.2437 - val_loss: 175.9012\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.8141 - val_loss: 172.6951\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.0624 - val_loss: 169.7641\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.7793 - val_loss: 165.4625\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.7867 - val_loss: 164.6690\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.5957 - val_loss: 160.7401\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.9458 - val_loss: 159.6269\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.6254 - val_loss: 158.5787\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.1869 - val_loss: 154.8324\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 136.6363 - val_loss: 156.5215\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.6341 - val_loss: 150.2218\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.3519 - val_loss: 147.6157\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.0455 - val_loss: 145.5943\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.9740 - val_loss: 142.7222\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.2430 - val_loss: 143.9604\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.7266 - val_loss: 140.9759\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.0963 - val_loss: 137.9282\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.2226 - val_loss: 135.3980\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.9517 - val_loss: 137.4093\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.3378 - val_loss: 132.6469\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.5755 - val_loss: 131.1462\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.9114 - val_loss: 135.0600\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.3066 - val_loss: 129.5959\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.1257 - val_loss: 129.2731\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.9838 - val_loss: 125.6098\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.5186 - val_loss: 124.6474\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.4951 - val_loss: 125.0997\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.3883 - val_loss: 124.0601\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.4901 - val_loss: 121.0855\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.1701 - val_loss: 119.6031\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.4344 - val_loss: 124.7853\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.5801 - val_loss: 118.4053\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.6698 - val_loss: 116.1641\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.0651 - val_loss: 120.0210\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.5823 - val_loss: 114.5731\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.7311 - val_loss: 118.4061\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.7301 - val_loss: 112.2933\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.4617 - val_loss: 113.0196\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.1103 - val_loss: 110.9573\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.1813 - val_loss: 111.1021\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.2540 - val_loss: 110.1680\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.1132 - val_loss: 108.3062\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.6811 - val_loss: 107.5297\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.2795 - val_loss: 108.0482\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.0527 - val_loss: 109.1549\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.3413 - val_loss: 105.4149\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.5028 - val_loss: 105.9422\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.1365 - val_loss: 104.0554\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.3394 - val_loss: 103.7754\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.8470 - val_loss: 109.4682\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.0127 - val_loss: 101.3165\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.2424 - val_loss: 101.5992\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.7090 - val_loss: 103.7509\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.9824 - val_loss: 100.7375\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.0002 - val_loss: 99.9910\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.4215 - val_loss: 98.4337\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.6344 - val_loss: 99.5864\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.2028 - val_loss: 101.6046\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 664us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  16 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 80158.1406 - val_loss: 53070.7188\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 39947.0781 - val_loss: 23307.8711\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 16018.8984 - val_loss: 8734.2314\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 6408.2798 - val_loss: 3994.3772\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4065.7903 - val_loss: 3144.8203\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3333.4392 - val_loss: 2646.7566\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2842.1399 - val_loss: 2315.2559\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2494.1182 - val_loss: 2085.9131\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2235.6121 - val_loss: 1902.0742\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2020.3472 - val_loss: 1756.8131\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1840.6722 - val_loss: 1631.5330\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1688.1946 - val_loss: 1524.6392\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1558.5717 - val_loss: 1432.2686\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1450.6599 - val_loss: 1345.1049\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1352.2161 - val_loss: 1274.4279\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1272.6351 - val_loss: 1209.9498\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1204.5776 - val_loss: 1155.6448\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1142.2411 - val_loss: 1101.3751\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1085.3160 - val_loss: 1054.1383\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1036.1454 - val_loss: 1013.3710\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 991.8702 - val_loss: 973.3016\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 951.2579 - val_loss: 936.0622\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 913.4443 - val_loss: 902.9944\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 880.4459 - val_loss: 870.9572\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 849.2941 - val_loss: 841.8058\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 820.1624 - val_loss: 814.2341\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 793.2012 - val_loss: 787.3582\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 767.3829 - val_loss: 764.2620\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 743.1829 - val_loss: 740.6289\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 722.0944 - val_loss: 719.5208\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 701.5840 - val_loss: 698.6724\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 680.8300 - val_loss: 679.4907\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 663.3824 - val_loss: 661.6429\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 645.2120 - val_loss: 644.3870\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 629.0131 - val_loss: 628.3970\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 613.4470 - val_loss: 613.9716\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 598.7893 - val_loss: 599.4061\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 584.5261 - val_loss: 585.0067\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 571.6078 - val_loss: 570.9691\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 559.2272 - val_loss: 559.3667\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 546.2343 - val_loss: 545.9626\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 535.5634 - val_loss: 534.2457\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 525.0269 - val_loss: 524.4013\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 512.6532 - val_loss: 512.2287\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 502.2286 - val_loss: 500.9889\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 491.7171 - val_loss: 491.9436\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 479.1115 - val_loss: 479.9235\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 468.5350 - val_loss: 469.5295\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 458.1260 - val_loss: 458.3682\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 447.4346 - val_loss: 447.5419\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 437.6062 - val_loss: 437.9984\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 428.4929 - val_loss: 427.4409\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 418.5755 - val_loss: 418.3885\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 410.5890 - val_loss: 409.4779\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 403.3858 - val_loss: 401.2819\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 394.9562 - val_loss: 393.5355\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 388.1001 - val_loss: 383.8390\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 378.8307 - val_loss: 377.8027\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 372.2547 - val_loss: 369.1614\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 364.2638 - val_loss: 361.6035\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 357.1956 - val_loss: 354.9488\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 350.3532 - val_loss: 347.9485\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 344.9219 - val_loss: 340.9678\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 337.4309 - val_loss: 334.0040\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 331.8355 - val_loss: 327.1691\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 328.0173 - val_loss: 320.5857\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 322.7047 - val_loss: 315.3710\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 313.6670 - val_loss: 309.6193\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 310.5497 - val_loss: 303.6617\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 304.0391 - val_loss: 297.2694\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 298.0022 - val_loss: 290.8184\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 292.2161 - val_loss: 284.2296\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 286.7940 - val_loss: 278.0058\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 281.5695 - val_loss: 272.0260\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 276.6735 - val_loss: 266.3717\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 272.1031 - val_loss: 260.9927\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 266.4263 - val_loss: 254.4444\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.4374 - val_loss: 248.5394\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 257.1773 - val_loss: 243.3522\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 253.5095 - val_loss: 237.6345\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 248.0408 - val_loss: 233.9330\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.8713 - val_loss: 227.3911\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.7022 - val_loss: 223.3551\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.5586 - val_loss: 217.6104\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 231.9343 - val_loss: 213.7498\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 228.1823 - val_loss: 209.1577\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 224.7042 - val_loss: 205.5450\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 220.2359 - val_loss: 200.3022\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.0125 - val_loss: 197.1638\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 213.4045 - val_loss: 192.7840\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 210.0500 - val_loss: 189.5497\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 206.8624 - val_loss: 185.2059\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.7831 - val_loss: 182.2048\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 200.9204 - val_loss: 179.1210\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.5558 - val_loss: 174.7450\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.0206 - val_loss: 172.1441\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.3082 - val_loss: 169.0479\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 190.3965 - val_loss: 166.3565\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.2746 - val_loss: 162.9476\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 184.6728 - val_loss: 161.9034\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 668us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  17 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 28719.9316 - val_loss: 20400.8887\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 15266.3916 - val_loss: 9709.1162\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 6029.8325 - val_loss: 2641.5977\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1303.4849 - val_loss: 479.7066\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 460.0053 - val_loss: 414.3049\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 403.0027 - val_loss: 328.8831\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 328.2583 - val_loss: 296.7902\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 295.3096 - val_loss: 269.3459\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 270.2096 - val_loss: 249.7452\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 251.8181 - val_loss: 236.7510\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.9416 - val_loss: 225.9627\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 225.3208 - val_loss: 218.9621\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.2938 - val_loss: 213.7811\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 210.9212 - val_loss: 209.1138\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.5765 - val_loss: 203.6843\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 197.9641 - val_loss: 201.0312\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 191.9744 - val_loss: 196.7983\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.5517 - val_loss: 193.2410\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 182.7870 - val_loss: 191.7744\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.1676 - val_loss: 190.0669\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.2170 - val_loss: 188.0914\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.4938 - val_loss: 188.4094\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.9608 - val_loss: 186.0960\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.3688 - val_loss: 185.6271\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 167.2637 - val_loss: 185.0689\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.3937 - val_loss: 182.9438\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.3904 - val_loss: 182.9228\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.4105 - val_loss: 181.6960\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.2380 - val_loss: 179.4769\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.5199 - val_loss: 179.8186\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.9785 - val_loss: 181.3710\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.9937 - val_loss: 186.5212\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.1998 - val_loss: 189.8237\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.8759 - val_loss: 185.5626\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.7486 - val_loss: 188.2740\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.1171 - val_loss: 187.9625\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.4329 - val_loss: 186.1164\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.5701 - val_loss: 185.9819\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.8415 - val_loss: 186.7941\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.9686 - val_loss: 184.8444\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.6994 - val_loss: 182.3530\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.5062 - val_loss: 182.6276\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.5563 - val_loss: 182.7269\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.1416 - val_loss: 181.2064\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 142.9978 - val_loss: 182.0059\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.0655 - val_loss: 180.2186\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.8065 - val_loss: 179.4750\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.1118 - val_loss: 178.4173\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.2078 - val_loss: 177.5423\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.8854 - val_loss: 177.5028\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.0164 - val_loss: 177.4799\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.9208 - val_loss: 175.7583\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.8552 - val_loss: 175.7704\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.1925 - val_loss: 175.0222\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.6155 - val_loss: 174.7959\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.4176 - val_loss: 173.3406\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.7647 - val_loss: 173.1866\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.9898 - val_loss: 172.4848\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.6643 - val_loss: 171.7800\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.7351 - val_loss: 172.0555\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.3098 - val_loss: 170.1910\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.7966 - val_loss: 170.6537\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.4599 - val_loss: 170.7115\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.9507 - val_loss: 168.9505\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.5006 - val_loss: 169.2961\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.6989 - val_loss: 166.9779\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.6459 - val_loss: 168.2116\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.7548 - val_loss: 167.1042\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.1283 - val_loss: 167.5427\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.0863 - val_loss: 165.4410\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.9871 - val_loss: 168.2819\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.9239 - val_loss: 164.9560\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.9048 - val_loss: 164.3319\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.5731 - val_loss: 164.3998\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.0253 - val_loss: 162.6780\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.8357 - val_loss: 163.1161\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.2345 - val_loss: 162.0706\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.2868 - val_loss: 161.4572\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.8182 - val_loss: 160.8104\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.4381 - val_loss: 161.6796\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.2465 - val_loss: 160.9202\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.3456 - val_loss: 159.2134\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.5408 - val_loss: 159.2023\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.8753 - val_loss: 159.3678\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.8657 - val_loss: 157.8538\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.4082 - val_loss: 157.9940\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.9861 - val_loss: 157.5574\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.6813 - val_loss: 160.0397\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.8109 - val_loss: 156.3435\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.3383 - val_loss: 155.3920\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.3693 - val_loss: 155.7555\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.0660 - val_loss: 154.1202\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.5677 - val_loss: 154.0224\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.0245 - val_loss: 153.5716\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.4506 - val_loss: 153.7502\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.8942 - val_loss: 152.6579\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.8350 - val_loss: 152.5279\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.5579 - val_loss: 151.6929\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.5096 - val_loss: 152.5963\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.6132 - val_loss: 149.1317\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 557us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  18 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2434.2358 - val_loss: 726.8362\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 823.5672 - val_loss: 842.6865\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 659.4366 - val_loss: 515.6846\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 517.1587 - val_loss: 431.9186\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 417.3822 - val_loss: 403.2857\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 359.2059 - val_loss: 366.4049\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 312.9391 - val_loss: 336.2682\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 277.7259 - val_loss: 322.2250\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 251.0199 - val_loss: 286.4326\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 229.3630 - val_loss: 266.3327\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 220.7882 - val_loss: 265.3923\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 204.0429 - val_loss: 249.5055\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.9498 - val_loss: 237.0260\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.4553 - val_loss: 223.5517\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.0261 - val_loss: 214.0152\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 178.2901 - val_loss: 209.6569\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.5068 - val_loss: 206.0837\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 169.6724 - val_loss: 201.7556\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 166.9345 - val_loss: 193.0309\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.0783 - val_loss: 191.8634\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.7683 - val_loss: 191.6362\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.2300 - val_loss: 181.4204\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.2439 - val_loss: 182.5973\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.7255 - val_loss: 166.0045\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.8844 - val_loss: 168.9812\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.4857 - val_loss: 162.7538\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.5307 - val_loss: 156.0082\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.4912 - val_loss: 167.8684\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.7773 - val_loss: 162.9057\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.3720 - val_loss: 148.5655\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.5891 - val_loss: 153.5397\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.2701 - val_loss: 152.7665\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.4981 - val_loss: 145.8504\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.4483 - val_loss: 138.1786\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.5467 - val_loss: 138.5831\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.7139 - val_loss: 134.0213\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.8152 - val_loss: 148.6041\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.9485 - val_loss: 131.6002\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.8297 - val_loss: 148.0933\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.8077 - val_loss: 130.6216\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.0039 - val_loss: 127.0785\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.7631 - val_loss: 131.8690\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.7707 - val_loss: 133.1931\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.0602 - val_loss: 132.3095\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.0538 - val_loss: 123.8894\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.8112 - val_loss: 119.4915\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.8793 - val_loss: 117.9431\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.6428 - val_loss: 117.5681\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.8445 - val_loss: 133.1512\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.5041 - val_loss: 129.2210\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.1017 - val_loss: 119.4505\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.3945 - val_loss: 117.3437\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.1612 - val_loss: 111.3161\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.9872 - val_loss: 111.0857\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.9102 - val_loss: 109.6592\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.2885 - val_loss: 110.2557\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.7897 - val_loss: 115.4611\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.1325 - val_loss: 113.2060\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.3412 - val_loss: 106.9527\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.9345 - val_loss: 115.3277\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.7066 - val_loss: 108.2126\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.1696 - val_loss: 106.1680\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.1467 - val_loss: 113.6193\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.6232 - val_loss: 112.9366\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.7048 - val_loss: 109.2691\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.0873 - val_loss: 111.3006\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.9028 - val_loss: 108.0003\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.0426 - val_loss: 112.4029\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.6485 - val_loss: 110.6062\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.5868 - val_loss: 105.1102\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.0112 - val_loss: 116.3172\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.0836 - val_loss: 109.6457\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.8048 - val_loss: 103.8212\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.8972 - val_loss: 106.1257\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.3268 - val_loss: 106.2934\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.0662 - val_loss: 107.2169\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.1106 - val_loss: 105.5728\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.1238 - val_loss: 102.5352\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.3858 - val_loss: 102.2854\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.7859 - val_loss: 105.7652\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.3697 - val_loss: 103.0459\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.5877 - val_loss: 104.7812\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.5791 - val_loss: 104.0638\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.0064 - val_loss: 106.9180\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.1587 - val_loss: 101.6373\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 90.0234 - val_loss: 104.4250\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.2076 - val_loss: 100.6299\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.2762 - val_loss: 100.5289\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.7628 - val_loss: 105.0026\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.6800 - val_loss: 108.2461\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.6684 - val_loss: 101.2427\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.1063 - val_loss: 104.8253\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.5580 - val_loss: 101.1242\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.4414 - val_loss: 110.7519\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.5864 - val_loss: 108.5350\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.6912 - val_loss: 116.3983\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.5697 - val_loss: 102.8715\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.7840 - val_loss: 103.8308\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.1184 - val_loss: 106.8651\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.8452 - val_loss: 122.8346\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 666us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  19 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 343.1270 - val_loss: 217.1964\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.1892 - val_loss: 212.2157\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.7660 - val_loss: 200.9553\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 227.9312 - val_loss: 198.4491\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 221.7177 - val_loss: 195.0322\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 218.4593 - val_loss: 193.5035\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 213.2894 - val_loss: 189.4453\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.3979 - val_loss: 186.7683\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 204.1114 - val_loss: 184.4514\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 200.2599 - val_loss: 182.0744\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 197.0312 - val_loss: 182.2420\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.9454 - val_loss: 177.3665\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 190.0752 - val_loss: 178.3817\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 188.1115 - val_loss: 172.8441\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.8609 - val_loss: 170.3621\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.0614 - val_loss: 173.4015\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.2277 - val_loss: 165.7675\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.1472 - val_loss: 163.3962\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 167.0596 - val_loss: 161.1210\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.7135 - val_loss: 166.1311\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 166.7775 - val_loss: 156.8267\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.5550 - val_loss: 163.3327\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.3807 - val_loss: 153.9223\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.2437 - val_loss: 154.3484\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.9316 - val_loss: 151.4699\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 148.2863 - val_loss: 148.3121\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.8736 - val_loss: 147.3834\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.2804 - val_loss: 147.9046\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.3390 - val_loss: 146.4935\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.5206 - val_loss: 144.2056\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.5148 - val_loss: 143.1053\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.4201 - val_loss: 142.4434\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.8485 - val_loss: 138.4724\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.2707 - val_loss: 138.9997\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.4646 - val_loss: 137.2398\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.4343 - val_loss: 138.0965\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.0816 - val_loss: 137.5300\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.7459 - val_loss: 136.1725\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.7861 - val_loss: 131.9233\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.7122 - val_loss: 139.2508\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.7450 - val_loss: 130.0111\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.7699 - val_loss: 130.6826\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.2316 - val_loss: 141.0389\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.3373 - val_loss: 127.0293\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.6752 - val_loss: 130.6868\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.1125 - val_loss: 137.0446\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.8593 - val_loss: 123.0460\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.6531 - val_loss: 127.0726\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.0157 - val_loss: 124.2276\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.7290 - val_loss: 121.3999\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.0137 - val_loss: 118.6798\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.2923 - val_loss: 120.4778\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.5215 - val_loss: 119.0915\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.2898 - val_loss: 118.5066\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.2184 - val_loss: 122.1729\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.9165 - val_loss: 117.2674\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.0030 - val_loss: 125.5874\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.8618 - val_loss: 113.6217\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.2847 - val_loss: 112.6482\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.0368 - val_loss: 114.4420\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.1477 - val_loss: 111.8979\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.9974 - val_loss: 110.0190\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.4245 - val_loss: 111.1547\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.1148 - val_loss: 126.1639\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.1412 - val_loss: 115.3926\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.6857 - val_loss: 117.1171\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.9633 - val_loss: 107.1446\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.4765 - val_loss: 104.6466\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.0595 - val_loss: 104.1328\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.4381 - val_loss: 104.8864\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.7459 - val_loss: 104.7450\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.8158 - val_loss: 103.1718\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.4327 - val_loss: 101.1730\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.1555 - val_loss: 108.7448\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 85.6098 - val_loss: 108.2816\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.5218 - val_loss: 102.3255\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.4020 - val_loss: 99.7343\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.9169 - val_loss: 97.8574\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.6196 - val_loss: 98.5565\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.5215 - val_loss: 101.6462\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.1390 - val_loss: 102.6081\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.6607 - val_loss: 110.8914\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.7179 - val_loss: 99.5828\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.1979 - val_loss: 100.2824\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.2549 - val_loss: 94.9124\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.1676 - val_loss: 94.5378\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.4732 - val_loss: 99.2411\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.3018 - val_loss: 94.1407\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.5930 - val_loss: 93.5513\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.4493 - val_loss: 95.4515\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.6489 - val_loss: 99.7428\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.5025 - val_loss: 109.1803\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.8603 - val_loss: 94.3210\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.2564 - val_loss: 90.9895\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.3555 - val_loss: 92.5857\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.3504 - val_loss: 90.9490\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.5743 - val_loss: 91.9842\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.7279 - val_loss: 92.1010\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.7640 - val_loss: 96.2814\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.7223 - val_loss: 90.7378\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  20 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 361.2840 - val_loss: 277.0555\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 248.9370 - val_loss: 231.7996\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 213.2346 - val_loss: 205.2764\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.6610 - val_loss: 191.7990\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.6549 - val_loss: 176.6510\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 167.3989 - val_loss: 162.7444\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.6300 - val_loss: 158.5001\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.5085 - val_loss: 149.8763\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.4326 - val_loss: 143.8655\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.9932 - val_loss: 141.0267\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.6015 - val_loss: 139.5961\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.6902 - val_loss: 136.3983\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.6355 - val_loss: 133.2896\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.7033 - val_loss: 138.5622\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.4702 - val_loss: 132.0531\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.3371 - val_loss: 126.8431\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.9022 - val_loss: 125.6699\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.3866 - val_loss: 124.6448\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.4174 - val_loss: 127.4638\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.7515 - val_loss: 132.4609\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.0334 - val_loss: 123.9594\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.5146 - val_loss: 121.2719\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.0663 - val_loss: 121.9674\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.2744 - val_loss: 125.7734\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.8389 - val_loss: 123.6742\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.2654 - val_loss: 126.2242\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 111.5903 - val_loss: 119.9078\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.2898 - val_loss: 119.1727\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.5528 - val_loss: 120.5829\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.8984 - val_loss: 119.1653\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.8278 - val_loss: 119.3275\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.7114 - val_loss: 121.7830\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.0738 - val_loss: 118.5307\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.4286 - val_loss: 116.9760\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.3975 - val_loss: 135.3721\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.5044 - val_loss: 117.1716\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.6292 - val_loss: 115.7429\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.4365 - val_loss: 119.8356\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.6738 - val_loss: 119.4909\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.6739 - val_loss: 115.9063\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.7079 - val_loss: 121.2816\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.2865 - val_loss: 118.7124\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.1497 - val_loss: 114.5723\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.2731 - val_loss: 114.1498\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.9178 - val_loss: 114.3433\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.8779 - val_loss: 115.4805\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.9311 - val_loss: 120.0537\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.5385 - val_loss: 119.3710\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.0043 - val_loss: 118.8912\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.3319 - val_loss: 119.9220\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.2149 - val_loss: 117.3318\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.0415 - val_loss: 115.2484\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.4394 - val_loss: 116.8436\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.5578 - val_loss: 116.6490\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.4859 - val_loss: 120.2839\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.7359 - val_loss: 125.7969\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.1773 - val_loss: 130.4740\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.6147 - val_loss: 123.8431\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.8698 - val_loss: 114.5990\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.0652 - val_loss: 114.6053\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.0286 - val_loss: 116.7363\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.9258 - val_loss: 116.4836\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.3014 - val_loss: 116.7078\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.7075 - val_loss: 113.8249\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.2635 - val_loss: 113.2986\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.5016 - val_loss: 113.9412\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.7520 - val_loss: 114.2247\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.5613 - val_loss: 113.2396\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.2880 - val_loss: 115.6714\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.5162 - val_loss: 114.0841\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.0408 - val_loss: 113.0008\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.4311 - val_loss: 111.1489\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.2160 - val_loss: 111.9057\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.2465 - val_loss: 126.5029\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.8206 - val_loss: 116.5676\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.8570 - val_loss: 112.0679\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.8489 - val_loss: 110.5396\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.3195 - val_loss: 112.0623\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.4517 - val_loss: 113.4995\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.1642 - val_loss: 110.4790\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.1093 - val_loss: 111.5237\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.8837 - val_loss: 113.3041\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.3182 - val_loss: 111.9100\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.6373 - val_loss: 114.8803\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.4638 - val_loss: 111.8730\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.4911 - val_loss: 111.8298\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.5724 - val_loss: 109.0109\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.4960 - val_loss: 109.7649\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.3278 - val_loss: 107.5633\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.1966 - val_loss: 109.1236\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.9062 - val_loss: 108.5445\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.3051 - val_loss: 108.5380\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.5918 - val_loss: 109.1757\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.4802 - val_loss: 108.3934\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.0272 - val_loss: 108.7630\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.1949 - val_loss: 114.3449\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.5247 - val_loss: 108.9178\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.5667 - val_loss: 107.7490\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.6199 - val_loss: 108.5219\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.3013 - val_loss: 108.6053\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 556us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  21 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 58662.9414 - val_loss: 41832.0000\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 27810.1094 - val_loss: 17517.6445\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 10407.1621 - val_loss: 5430.9780\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2739.5972 - val_loss: 1239.4352\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 826.8859 - val_loss: 609.9526\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 670.6888 - val_loss: 555.4213\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 608.4498 - val_loss: 507.4421\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 547.8773 - val_loss: 488.9996\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 515.1121 - val_loss: 485.9605\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 499.3479 - val_loss: 477.8501\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 489.6272 - val_loss: 472.9616\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 483.3531 - val_loss: 466.4802\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 481.6091 - val_loss: 454.3399\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 468.9846 - val_loss: 446.5488\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 462.0267 - val_loss: 439.9948\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 460.2316 - val_loss: 432.9065\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 449.6823 - val_loss: 422.3299\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 440.7882 - val_loss: 412.2002\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 436.1938 - val_loss: 407.5454\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 427.3595 - val_loss: 397.1945\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 424.5105 - val_loss: 390.3332\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 418.2684 - val_loss: 385.1216\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 414.6117 - val_loss: 380.4837\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 409.8824 - val_loss: 371.6216\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 403.7789 - val_loss: 371.7556\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 402.9348 - val_loss: 361.0417\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 399.0043 - val_loss: 359.0136\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 393.8681 - val_loss: 355.8952\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 390.6096 - val_loss: 349.9772\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 387.9598 - val_loss: 350.4270\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 383.9951 - val_loss: 341.7716\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 381.0572 - val_loss: 342.1041\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 376.3380 - val_loss: 338.9786\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 370.1066 - val_loss: 332.4502\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 370.4754 - val_loss: 329.2123\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 362.9449 - val_loss: 329.6375\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 361.3231 - val_loss: 324.6318\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 357.4467 - val_loss: 323.3043\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 355.2264 - val_loss: 319.6682\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 352.0127 - val_loss: 317.2992\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 349.6242 - val_loss: 313.5113\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 346.8624 - val_loss: 313.3954\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 343.1514 - val_loss: 308.5026\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 340.4516 - val_loss: 306.2876\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 335.8879 - val_loss: 303.1411\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 334.4117 - val_loss: 301.1656\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 334.2047 - val_loss: 302.7540\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 329.6965 - val_loss: 296.8999\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 329.5764 - val_loss: 295.4310\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 324.7379 - val_loss: 298.9229\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 320.9546 - val_loss: 291.5301\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 320.0078 - val_loss: 291.1848\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 314.7300 - val_loss: 287.1589\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 313.9257 - val_loss: 286.0390\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 309.2077 - val_loss: 283.1155\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 307.5267 - val_loss: 280.6144\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 308.7466 - val_loss: 278.4088\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 306.6033 - val_loss: 278.5440\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 302.8153 - val_loss: 275.9253\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 302.3109 - val_loss: 274.3617\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 300.2801 - val_loss: 274.8218\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 298.1425 - val_loss: 271.1022\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 298.4488 - val_loss: 271.4026\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 297.6707 - val_loss: 269.6409\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 295.2710 - val_loss: 270.5203\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 294.4135 - val_loss: 266.0863\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 294.8620 - val_loss: 266.3695\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 290.2368 - val_loss: 263.8368\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 288.0035 - val_loss: 265.1772\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 288.2155 - val_loss: 261.8411\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 285.9693 - val_loss: 261.7810\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 284.3577 - val_loss: 261.6708\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 283.4887 - val_loss: 260.7920\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 281.8697 - val_loss: 257.6795\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 283.2256 - val_loss: 262.1559\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 280.6793 - val_loss: 256.3862\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 279.0479 - val_loss: 255.5291\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 279.1593 - val_loss: 254.0617\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 277.7763 - val_loss: 254.9722\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 275.7233 - val_loss: 252.9365\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 278.9705 - val_loss: 251.8814\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 274.9801 - val_loss: 253.1710\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 272.5697 - val_loss: 250.5603\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 272.7980 - val_loss: 250.4396\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 271.1308 - val_loss: 252.5263\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 270.2663 - val_loss: 250.5113\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 267.7170 - val_loss: 246.6794\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 270.3947 - val_loss: 248.1310\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 271.2979 - val_loss: 245.4489\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 266.9910 - val_loss: 247.8751\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 265.6339 - val_loss: 247.5688\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 267.3881 - val_loss: 243.5799\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.9135 - val_loss: 248.0518\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 263.1153 - val_loss: 241.6457\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 261.7064 - val_loss: 249.3590\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 263.5290 - val_loss: 240.2547\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 263.1645 - val_loss: 239.2383\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 259.3909 - val_loss: 251.4445\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 259.4930 - val_loss: 237.9714\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 257.3110 - val_loss: 239.4631\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  22 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 111097.7812 - val_loss: 41273.2773\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 20274.3848 - val_loss: 4602.2681\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3002.0935 - val_loss: 2351.2837\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2472.7532 - val_loss: 2449.5286\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2190.9456 - val_loss: 1992.8949\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2016.3314 - val_loss: 1879.1797\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1934.9756 - val_loss: 1817.1241\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1856.6328 - val_loss: 1766.0782\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1788.2556 - val_loss: 1695.0465\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1718.8953 - val_loss: 1631.9718\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1652.2944 - val_loss: 1569.9686\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1590.2222 - val_loss: 1508.6932\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1537.0132 - val_loss: 1456.3579\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1475.2549 - val_loss: 1397.5256\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1435.0657 - val_loss: 1335.8579\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1370.1875 - val_loss: 1304.2692\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1327.8718 - val_loss: 1254.2163\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1275.1329 - val_loss: 1199.4578\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1234.0074 - val_loss: 1154.6793\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1190.8895 - val_loss: 1116.9675\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1149.1465 - val_loss: 1079.2775\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1113.8973 - val_loss: 1035.9802\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1072.7324 - val_loss: 1010.5549\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1041.2118 - val_loss: 973.8881\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1012.1979 - val_loss: 941.9554\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 976.8563 - val_loss: 913.5385\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 946.9741 - val_loss: 882.6976\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 919.4710 - val_loss: 857.2394\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 895.2473 - val_loss: 839.3445\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 867.1968 - val_loss: 805.7162\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 845.6749 - val_loss: 783.6370\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 820.1282 - val_loss: 768.6836\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 802.5846 - val_loss: 745.1030\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 780.2750 - val_loss: 723.0410\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 760.8950 - val_loss: 707.0677\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 741.4299 - val_loss: 693.1626\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 725.0208 - val_loss: 674.9088\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 708.6032 - val_loss: 654.8066\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 697.3073 - val_loss: 646.6451\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 680.2841 - val_loss: 624.4422\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 659.4631 - val_loss: 619.3083\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 647.2251 - val_loss: 597.9915\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 628.1672 - val_loss: 590.1650\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 616.3446 - val_loss: 571.7289\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 608.1707 - val_loss: 558.8226\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 587.2109 - val_loss: 556.3623\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 576.6047 - val_loss: 533.4352\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 562.3408 - val_loss: 526.3738\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 550.9556 - val_loss: 515.6980\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 539.1347 - val_loss: 501.5435\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 525.4571 - val_loss: 498.1969\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 517.1188 - val_loss: 483.0537\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 503.2616 - val_loss: 476.0200\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 493.6528 - val_loss: 465.7328\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 482.7487 - val_loss: 458.6484\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 473.7022 - val_loss: 450.8554\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 462.8184 - val_loss: 439.5838\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 454.2548 - val_loss: 432.7924\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 443.5909 - val_loss: 421.4398\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 437.1610 - val_loss: 416.9760\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 426.5245 - val_loss: 407.8295\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 418.3470 - val_loss: 400.6441\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 411.8944 - val_loss: 391.4822\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 404.5331 - val_loss: 386.4709\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 395.8341 - val_loss: 377.5160\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 387.0520 - val_loss: 371.0759\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 380.2473 - val_loss: 363.6390\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 372.6972 - val_loss: 359.5873\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 367.6610 - val_loss: 350.4589\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 359.0960 - val_loss: 346.9845\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 352.8216 - val_loss: 340.4479\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 345.8716 - val_loss: 333.3098\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 341.0177 - val_loss: 328.2754\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 334.4036 - val_loss: 325.2271\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 328.4442 - val_loss: 318.0730\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 321.5302 - val_loss: 312.3476\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 314.0042 - val_loss: 313.1508\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 312.1496 - val_loss: 302.1235\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 306.9521 - val_loss: 300.1422\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 299.1977 - val_loss: 295.5431\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 296.6959 - val_loss: 295.0047\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 287.6062 - val_loss: 285.3352\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 283.8505 - val_loss: 281.3294\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 278.3855 - val_loss: 279.2351\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 273.4492 - val_loss: 276.2011\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 269.3885 - val_loss: 271.8828\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 264.5940 - val_loss: 268.1250\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.3814 - val_loss: 265.4992\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 260.7245 - val_loss: 259.3175\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 254.1626 - val_loss: 257.4981\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 248.2711 - val_loss: 254.9938\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.9587 - val_loss: 253.2794\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 240.4370 - val_loss: 246.2216\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.7597 - val_loss: 243.0211\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 232.3291 - val_loss: 242.5187\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 227.5234 - val_loss: 235.7622\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 226.0673 - val_loss: 236.2745\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 220.2242 - val_loss: 230.8453\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 216.8409 - val_loss: 227.9753\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 212.9611 - val_loss: 224.6827\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 668us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  23 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 21483.1797 - val_loss: 14642.8379\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 10019.0869 - val_loss: 6579.4937\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3919.7915 - val_loss: 2458.9998\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1333.8053 - val_loss: 1065.8657\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 695.7760 - val_loss: 747.1924\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 500.4567 - val_loss: 532.3588\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 361.2018 - val_loss: 400.5773\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 295.9967 - val_loss: 336.9046\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 257.0462 - val_loss: 293.9231\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 226.4814 - val_loss: 261.7207\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 204.6991 - val_loss: 238.1341\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.4642 - val_loss: 223.7402\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 169.6808 - val_loss: 212.5247\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.9800 - val_loss: 206.1143\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.4750 - val_loss: 197.9369\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.6324 - val_loss: 192.4877\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.8259 - val_loss: 187.6609\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.1350 - val_loss: 184.9118\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.6139 - val_loss: 183.9825\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.1662 - val_loss: 180.8860\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.2846 - val_loss: 179.7402\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.2514 - val_loss: 177.6140\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.6611 - val_loss: 175.9818\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.4532 - val_loss: 174.5438\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.6693 - val_loss: 174.7752\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.5658 - val_loss: 172.4477\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.7921 - val_loss: 171.9726\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.2583 - val_loss: 169.9460\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.4756 - val_loss: 169.9958\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.1034 - val_loss: 168.3114\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.2007 - val_loss: 167.4984\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.2841 - val_loss: 166.2831\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.8645 - val_loss: 167.5467\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.1710 - val_loss: 165.5957\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.4875 - val_loss: 164.5361\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.0729 - val_loss: 164.8249\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.0569 - val_loss: 162.9121\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.3136 - val_loss: 162.6470\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.4913 - val_loss: 162.3439\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.9008 - val_loss: 161.2005\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.2330 - val_loss: 161.3996\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.2467 - val_loss: 161.4007\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.4188 - val_loss: 160.6803\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.7840 - val_loss: 159.9327\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.7267 - val_loss: 158.6018\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.1695 - val_loss: 157.8215\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.0579 - val_loss: 158.8406\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.4084 - val_loss: 157.1493\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.9959 - val_loss: 157.8816\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.6052 - val_loss: 155.9443\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.1383 - val_loss: 155.9734\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.9726 - val_loss: 156.1169\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.4512 - val_loss: 155.1756\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.0609 - val_loss: 154.1265\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.7329 - val_loss: 154.3759\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.6094 - val_loss: 152.8912\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.2555 - val_loss: 153.1712\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.2785 - val_loss: 152.7086\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.0626 - val_loss: 152.2065\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.4337 - val_loss: 151.0487\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.2407 - val_loss: 152.1046\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.1672 - val_loss: 151.8182\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.9977 - val_loss: 152.1492\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.4492 - val_loss: 149.9220\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.8976 - val_loss: 150.3285\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 115.2568 - val_loss: 148.4590\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.2448 - val_loss: 148.1885\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.2520 - val_loss: 148.0112\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.9061 - val_loss: 146.8569\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.0476 - val_loss: 146.5256\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.8243 - val_loss: 146.5656\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.2923 - val_loss: 147.4343\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.2141 - val_loss: 144.9297\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.2865 - val_loss: 144.5317\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.0703 - val_loss: 144.7246\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.1211 - val_loss: 144.0060\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.0676 - val_loss: 142.1194\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.8899 - val_loss: 146.5985\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.6673 - val_loss: 141.9261\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.2289 - val_loss: 143.5482\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.8327 - val_loss: 140.0568\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.6034 - val_loss: 142.1376\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.9592 - val_loss: 139.1538\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.6897 - val_loss: 141.0071\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.5111 - val_loss: 138.3729\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.8571 - val_loss: 139.4084\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.9229 - val_loss: 137.8207\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.3213 - val_loss: 137.7393\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.1304 - val_loss: 137.0451\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.5405 - val_loss: 135.5506\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.8432 - val_loss: 136.2737\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.0722 - val_loss: 135.0145\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.7431 - val_loss: 137.2174\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.4041 - val_loss: 133.5351\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.0365 - val_loss: 134.4300\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.2937 - val_loss: 134.7961\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.8045 - val_loss: 133.0662\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.5517 - val_loss: 131.2029\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.1788 - val_loss: 131.6658\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.9828 - val_loss: 133.3192\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 671us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  24 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 70593.7891 - val_loss: 41710.0664\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 30693.3047 - val_loss: 16060.5254\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 11379.0166 - val_loss: 5452.2944\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3897.6794 - val_loss: 1984.8998\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1548.3784 - val_loss: 1095.7335\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 902.4178 - val_loss: 898.5972\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 713.7212 - val_loss: 825.9131\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 637.9036 - val_loss: 776.1602\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 595.1421 - val_loss: 731.5484\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 558.6575 - val_loss: 684.5997\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 525.0200 - val_loss: 645.7162\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 493.0168 - val_loss: 604.9344\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 463.9003 - val_loss: 564.1772\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 434.9927 - val_loss: 527.1523\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 408.5056 - val_loss: 491.3884\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 388.1244 - val_loss: 467.1915\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 362.9387 - val_loss: 431.1000\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 344.0163 - val_loss: 409.2512\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 327.8732 - val_loss: 384.2242\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 313.4985 - val_loss: 360.3676\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 299.9342 - val_loss: 350.8688\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 287.5548 - val_loss: 327.0864\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 276.7317 - val_loss: 318.4926\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 266.7486 - val_loss: 300.1324\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.9986 - val_loss: 292.1646\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 247.9592 - val_loss: 278.0163\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.9537 - val_loss: 266.5911\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 232.1513 - val_loss: 257.1654\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 226.0912 - val_loss: 247.6767\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 220.7234 - val_loss: 240.4338\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 215.2445 - val_loss: 236.1320\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 214.9538 - val_loss: 229.3214\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 207.6568 - val_loss: 226.3869\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.9781 - val_loss: 218.3576\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 201.1350 - val_loss: 214.4728\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.4221 - val_loss: 212.3074\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 195.4814 - val_loss: 207.9376\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.0637 - val_loss: 208.4220\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 189.7826 - val_loss: 202.0585\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 188.7046 - val_loss: 203.5505\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.3155 - val_loss: 199.8503\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 184.4708 - val_loss: 200.7402\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 183.3149 - val_loss: 199.3864\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.9966 - val_loss: 199.6173\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.5734 - val_loss: 196.0392\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.1297 - val_loss: 194.7054\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 178.4693 - val_loss: 195.3521\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.1972 - val_loss: 192.0948\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 175.7916 - val_loss: 194.1444\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.8234 - val_loss: 193.9645\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.0615 - val_loss: 189.8531\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.1067 - val_loss: 188.9050\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 172.4956 - val_loss: 191.4382\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 171.6239 - val_loss: 189.3018\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.8118 - val_loss: 188.0870\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.0632 - val_loss: 190.4170\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 169.8378 - val_loss: 187.3796\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.2375 - val_loss: 186.6600\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 167.8427 - val_loss: 185.8511\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 167.1586 - val_loss: 183.1266\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 166.7256 - val_loss: 183.4709\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 165.2984 - val_loss: 182.4682\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.7680 - val_loss: 182.5639\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.1541 - val_loss: 179.8669\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.4875 - val_loss: 186.3184\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.7161 - val_loss: 178.8629\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.4557 - val_loss: 180.6706\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.5192 - val_loss: 179.6820\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.9975 - val_loss: 178.9368\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.6418 - val_loss: 177.5633\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.7197 - val_loss: 178.5424\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.4014 - val_loss: 177.1786\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.5858 - val_loss: 173.1619\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.5229 - val_loss: 173.6600\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.3930 - val_loss: 173.5197\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.2027 - val_loss: 173.0927\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.9636 - val_loss: 172.9179\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.4081 - val_loss: 173.1640\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.8021 - val_loss: 171.4201\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.1116 - val_loss: 170.9238\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.4405 - val_loss: 170.2735\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.0104 - val_loss: 168.2569\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.2169 - val_loss: 166.2731\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.5701 - val_loss: 167.0345\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.1334 - val_loss: 166.8661\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.1510 - val_loss: 167.2562\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.9404 - val_loss: 164.9119\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.4456 - val_loss: 170.1054\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.8643 - val_loss: 161.6236\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.8344 - val_loss: 167.5308\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.1095 - val_loss: 163.2172\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.2674 - val_loss: 164.8322\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.6344 - val_loss: 161.6538\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.4091 - val_loss: 165.6190\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.7811 - val_loss: 159.0095\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.0878 - val_loss: 163.5895\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.7695 - val_loss: 161.6223\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.4160 - val_loss: 162.4469\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.2108 - val_loss: 160.1899\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.8206 - val_loss: 160.6903\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  25 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2176.9402 - val_loss: 1396.2196\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1179.3260 - val_loss: 822.2601\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 726.0346 - val_loss: 526.6407\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 467.5202 - val_loss: 337.2917\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 319.2526 - val_loss: 239.3272\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.0099 - val_loss: 190.1751\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.2363 - val_loss: 179.1432\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 172.3552 - val_loss: 158.1346\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.5094 - val_loss: 151.4074\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.8665 - val_loss: 157.2508\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.5801 - val_loss: 157.1742\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.1268 - val_loss: 139.6130\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.1134 - val_loss: 135.5314\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.4796 - val_loss: 151.5705\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.1686 - val_loss: 142.8761\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.4415 - val_loss: 132.1914\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.5136 - val_loss: 130.5083\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.4284 - val_loss: 138.1968\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.2005 - val_loss: 131.8878\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.0501 - val_loss: 131.3156\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.8945 - val_loss: 146.3490\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.7489 - val_loss: 130.5682\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.9665 - val_loss: 130.5587\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.0871 - val_loss: 130.5778\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.7599 - val_loss: 129.2725\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.3226 - val_loss: 133.9666\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.5233 - val_loss: 130.2931\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.1578 - val_loss: 133.0406\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.9520 - val_loss: 132.1281\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.0124 - val_loss: 148.1754\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.6405 - val_loss: 127.3474\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.8408 - val_loss: 136.0159\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.3798 - val_loss: 134.1749\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.7202 - val_loss: 141.7182\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.7464 - val_loss: 127.5576\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.7201 - val_loss: 126.0221\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.6497 - val_loss: 125.9503\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.3572 - val_loss: 123.3737\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.1493 - val_loss: 125.6641\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.4005 - val_loss: 123.8212\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.3279 - val_loss: 126.3611\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.7955 - val_loss: 121.4971\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.3752 - val_loss: 127.6369\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.4315 - val_loss: 122.5592\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.1146 - val_loss: 120.2484\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.7990 - val_loss: 126.1643\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.3488 - val_loss: 123.4954\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.3834 - val_loss: 120.8984\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.1905 - val_loss: 120.2884\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.2121 - val_loss: 122.2313\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.8640 - val_loss: 120.6944\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.4188 - val_loss: 118.1966\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.7989 - val_loss: 117.3284\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.2726 - val_loss: 116.3911\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.8028 - val_loss: 115.4337\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.6645 - val_loss: 114.1053\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.1660 - val_loss: 122.0219\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 96.4508 - val_loss: 114.1095\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.8722 - val_loss: 113.6297\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.4825 - val_loss: 115.1141\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.2965 - val_loss: 112.5952\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.8504 - val_loss: 115.6276\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.2587 - val_loss: 111.2750\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.4769 - val_loss: 111.2848\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.6960 - val_loss: 112.4468\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.0371 - val_loss: 109.7145\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.9876 - val_loss: 114.3929\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.0258 - val_loss: 117.0662\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.4014 - val_loss: 112.9825\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.4207 - val_loss: 117.4168\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.7188 - val_loss: 117.0575\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.5286 - val_loss: 112.5350\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.0284 - val_loss: 111.7813\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.8046 - val_loss: 106.6409\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.5090 - val_loss: 105.6220\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.6977 - val_loss: 109.5219\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.6970 - val_loss: 105.0313\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.1858 - val_loss: 108.1486\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.5488 - val_loss: 112.7224\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.6779 - val_loss: 106.2163\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.3787 - val_loss: 99.5607\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.3970 - val_loss: 104.3710\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.7484 - val_loss: 99.5627\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.0783 - val_loss: 98.1095\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.8683 - val_loss: 101.3011\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.4528 - val_loss: 101.2377\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.1026 - val_loss: 104.0188\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.0197 - val_loss: 96.2963\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.8957 - val_loss: 99.0407\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.4438 - val_loss: 94.7147\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.9199 - val_loss: 93.5822\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.1338 - val_loss: 92.4952\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.2505 - val_loss: 91.7575\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.5911 - val_loss: 92.9630\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.8546 - val_loss: 95.9574\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.9359 - val_loss: 89.8037\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.5167 - val_loss: 93.0151\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.1842 - val_loss: 87.8099\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 73.7419 - val_loss: 90.7786\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 73.1160 - val_loss: 87.4106\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  26 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 8623.1621 - val_loss: 4211.3833\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3618.0330 - val_loss: 3592.3105\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2948.7485 - val_loss: 2630.4243\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2388.7117 - val_loss: 2168.7200\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2038.2089 - val_loss: 1849.8220\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1724.9084 - val_loss: 1485.3127\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1408.4558 - val_loss: 1168.5370\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1157.4878 - val_loss: 924.8113\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 967.1223 - val_loss: 773.1866\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 823.3303 - val_loss: 634.1367\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 690.8577 - val_loss: 514.7493\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 567.2401 - val_loss: 410.5863\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 454.0334 - val_loss: 337.5070\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 358.4268 - val_loss: 276.5291\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 299.3385 - val_loss: 230.2300\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 252.4212 - val_loss: 194.9716\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 209.5098 - val_loss: 166.2606\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.1537 - val_loss: 147.1871\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 169.4142 - val_loss: 137.6191\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.6253 - val_loss: 132.7614\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.7761 - val_loss: 124.7530\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.2408 - val_loss: 123.1969\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.1024 - val_loss: 116.9589\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.2665 - val_loss: 113.2276\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.2643 - val_loss: 110.4831\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.5097 - val_loss: 111.4217\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.4753 - val_loss: 106.5097\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.0845 - val_loss: 104.7788\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.2651 - val_loss: 102.0749\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.7133 - val_loss: 104.1889\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.4365 - val_loss: 100.7639\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.2747 - val_loss: 98.1666\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.2604 - val_loss: 96.6529\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.1434 - val_loss: 96.7791\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.9278 - val_loss: 94.1982\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.0202 - val_loss: 92.8512\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.8237 - val_loss: 94.0900\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.7570 - val_loss: 92.3644\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.8625 - val_loss: 94.6946\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.1668 - val_loss: 89.1823\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.8536 - val_loss: 91.6144\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.1022 - val_loss: 89.6390\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.8744 - val_loss: 85.7314\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.3153 - val_loss: 87.6769\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.5470 - val_loss: 85.8478\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.0216 - val_loss: 83.3056\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.0959 - val_loss: 83.7307\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.9887 - val_loss: 81.2965\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.9540 - val_loss: 84.8599\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.0082 - val_loss: 79.6731\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.8144 - val_loss: 79.4478\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.8952 - val_loss: 78.0951\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.5912 - val_loss: 78.9335\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.7974 - val_loss: 77.6976\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.7577 - val_loss: 77.4812\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.0259 - val_loss: 75.9461\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.6863 - val_loss: 75.1464\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.1984 - val_loss: 80.2031\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.7075 - val_loss: 73.9975\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.7928 - val_loss: 73.6546\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.8315 - val_loss: 85.1386\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.9462 - val_loss: 72.9843\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.5605 - val_loss: 73.6965\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.5092 - val_loss: 75.6832\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.5746 - val_loss: 72.0712\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.5676 - val_loss: 72.8250\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 82.2782 - val_loss: 71.5657\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.0521 - val_loss: 74.1182\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.6880 - val_loss: 72.8337\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.5903 - val_loss: 70.7290\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.7800 - val_loss: 70.1350\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.9940 - val_loss: 74.8266\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.5863 - val_loss: 71.2994\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.3629 - val_loss: 71.2887\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.2544 - val_loss: 70.3282\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.3342 - val_loss: 71.3193\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.6629 - val_loss: 69.1476\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.4034 - val_loss: 68.7899\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.0961 - val_loss: 78.6327\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.8968 - val_loss: 71.0261\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.0418 - val_loss: 69.7079\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.2495 - val_loss: 69.7081\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.5776 - val_loss: 69.2417\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.7408 - val_loss: 71.3592\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.3428 - val_loss: 67.9836\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.7666 - val_loss: 67.9088\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.0972 - val_loss: 68.8478\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.4090 - val_loss: 67.3735\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.0570 - val_loss: 69.6261\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.2273 - val_loss: 68.7000\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.1241 - val_loss: 67.9451\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.3714 - val_loss: 67.3342\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.2977 - val_loss: 66.5696\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.7605 - val_loss: 66.9629\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.0770 - val_loss: 72.2294\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.0084 - val_loss: 70.3633\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.3959 - val_loss: 67.1662\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.2061 - val_loss: 66.1100\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.2546 - val_loss: 74.9202\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.2469 - val_loss: 65.8737\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 667us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  27 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 35107.1758 - val_loss: 21577.7910\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 16105.1650 - val_loss: 9806.0791\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 7576.8970 - val_loss: 4911.4541\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4047.5305 - val_loss: 2827.0422\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 2498.5229 - val_loss: 1866.8889\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1723.1642 - val_loss: 1357.9818\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1286.6929 - val_loss: 1057.7046\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1024.8455 - val_loss: 864.7740\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 850.6106 - val_loss: 743.9263\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 734.3858 - val_loss: 666.8069\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 660.8347 - val_loss: 610.4565\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 606.9212 - val_loss: 572.4954\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 567.3575 - val_loss: 547.5075\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 540.0550 - val_loss: 527.7610\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 519.0363 - val_loss: 512.8643\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 502.1765 - val_loss: 500.6671\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 489.7907 - val_loss: 491.1036\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 479.4413 - val_loss: 482.9235\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 470.3000 - val_loss: 475.4676\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 462.7686 - val_loss: 468.5042\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 456.0088 - val_loss: 461.8874\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 449.8654 - val_loss: 455.2446\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 444.4396 - val_loss: 448.8836\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 439.3851 - val_loss: 443.1382\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 434.5019 - val_loss: 437.4631\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 429.9090 - val_loss: 432.7198\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 425.7000 - val_loss: 428.0476\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 421.7345 - val_loss: 423.4408\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 418.1294 - val_loss: 418.8956\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 414.2069 - val_loss: 415.3418\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 410.6759 - val_loss: 411.3675\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 406.9334 - val_loss: 407.0300\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 403.1307 - val_loss: 402.8487\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 399.4783 - val_loss: 398.5677\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 395.8633 - val_loss: 394.5893\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 392.6599 - val_loss: 390.3719\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 389.2575 - val_loss: 387.0214\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 386.2372 - val_loss: 383.2810\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 383.1345 - val_loss: 379.5103\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 380.0374 - val_loss: 375.7548\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 377.2684 - val_loss: 372.6577\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 374.7539 - val_loss: 368.4748\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 371.9765 - val_loss: 364.9464\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 369.2497 - val_loss: 362.2837\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 366.8687 - val_loss: 359.1372\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 364.5608 - val_loss: 355.1871\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 361.8326 - val_loss: 352.2069\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 359.5363 - val_loss: 349.5013\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 357.3007 - val_loss: 346.1465\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 355.0032 - val_loss: 343.5280\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 352.9579 - val_loss: 341.0375\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 351.2269 - val_loss: 338.7890\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 349.0219 - val_loss: 335.7460\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 346.9757 - val_loss: 333.0251\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 345.0092 - val_loss: 330.8503\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 343.1722 - val_loss: 328.5369\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 341.2131 - val_loss: 325.4370\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 339.3394 - val_loss: 323.4243\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 337.3061 - val_loss: 321.2974\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 335.5012 - val_loss: 319.0997\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 333.7682 - val_loss: 317.4174\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 331.9897 - val_loss: 314.9714\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 330.2759 - val_loss: 312.9401\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 328.6290 - val_loss: 310.7547\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 326.8689 - val_loss: 308.8349\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 325.3295 - val_loss: 307.4914\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 323.6812 - val_loss: 305.4143\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 322.2144 - val_loss: 303.5824\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 320.7229 - val_loss: 301.2140\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 319.2130 - val_loss: 299.5303\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 317.6861 - val_loss: 298.1410\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 316.4222 - val_loss: 296.5580\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 315.0876 - val_loss: 295.4641\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 313.5635 - val_loss: 293.4195\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 312.2676 - val_loss: 291.6966\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 310.8895 - val_loss: 290.4951\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 309.5580 - val_loss: 288.5492\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 308.5580 - val_loss: 287.4368\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 306.9168 - val_loss: 285.5161\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 306.3201 - val_loss: 284.8257\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 304.4652 - val_loss: 282.8976\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 303.4565 - val_loss: 280.9001\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 302.2288 - val_loss: 280.4919\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 300.9773 - val_loss: 279.4194\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 299.9662 - val_loss: 277.1332\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 298.6107 - val_loss: 276.1437\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 297.5111 - val_loss: 275.2346\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 296.4292 - val_loss: 274.3485\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 295.2762 - val_loss: 272.0230\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 294.0542 - val_loss: 271.1619\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 292.8745 - val_loss: 270.0778\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 291.8306 - val_loss: 268.9187\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 291.1199 - val_loss: 267.9433\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 290.2573 - val_loss: 266.1158\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 288.7057 - val_loss: 265.4250\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 287.6794 - val_loss: 264.7788\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 286.6789 - val_loss: 263.6361\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 285.9340 - val_loss: 261.3717\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 284.7458 - val_loss: 261.0827\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 283.5346 - val_loss: 259.6742\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  28 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7646.6211 - val_loss: 1488.1826\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 690.2258 - val_loss: 669.7247\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 599.9024 - val_loss: 480.2394\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 380.8872 - val_loss: 366.9487\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 346.1944 - val_loss: 329.8071\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 302.6735 - val_loss: 299.6690\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 267.9078 - val_loss: 270.0017\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.4293 - val_loss: 254.0586\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 226.6448 - val_loss: 240.0610\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 216.4629 - val_loss: 229.4494\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 207.4455 - val_loss: 224.2586\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 204.0052 - val_loss: 216.0366\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.0678 - val_loss: 211.9872\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.3542 - val_loss: 207.5232\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 189.8568 - val_loss: 202.6263\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.4433 - val_loss: 200.6798\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 182.5573 - val_loss: 196.0909\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.9218 - val_loss: 192.4418\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.5686 - val_loss: 189.6505\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.7382 - val_loss: 187.7447\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 171.1534 - val_loss: 185.0124\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.7673 - val_loss: 182.0337\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 166.5145 - val_loss: 179.0423\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.4525 - val_loss: 174.9633\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.1803 - val_loss: 174.4100\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.5190 - val_loss: 169.6596\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.5355 - val_loss: 167.9288\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.3820 - val_loss: 165.0271\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.5966 - val_loss: 169.0685\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.2047 - val_loss: 159.7418\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.3085 - val_loss: 159.7781\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.9403 - val_loss: 157.9594\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.9449 - val_loss: 155.7510\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.3758 - val_loss: 151.7552\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.9939 - val_loss: 151.2523\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.9666 - val_loss: 147.7965\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.9906 - val_loss: 148.3720\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.5338 - val_loss: 144.3971\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.7070 - val_loss: 143.8792\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.2302 - val_loss: 147.1455\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.6052 - val_loss: 141.2922\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.4494 - val_loss: 139.9124\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.5199 - val_loss: 136.5158\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.1411 - val_loss: 135.1532\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.6745 - val_loss: 132.9617\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.9926 - val_loss: 130.7532\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.6688 - val_loss: 128.1556\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.7942 - val_loss: 128.7370\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.7831 - val_loss: 125.0458\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.1257 - val_loss: 123.4205\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.6926 - val_loss: 120.9304\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.7122 - val_loss: 122.6733\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.5259 - val_loss: 117.8163\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.0050 - val_loss: 117.4479\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.3893 - val_loss: 119.3981\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.8515 - val_loss: 115.5454\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.5911 - val_loss: 121.9208\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.9877 - val_loss: 114.0241\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.8055 - val_loss: 112.1357\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.6791 - val_loss: 112.8584\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.5208 - val_loss: 110.8375\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.8502 - val_loss: 111.5806\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.1400 - val_loss: 109.1265\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.3978 - val_loss: 108.6664\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.9837 - val_loss: 109.2708\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.1107 - val_loss: 107.1707\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.7067 - val_loss: 105.1252\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.5243 - val_loss: 104.9199\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.4088 - val_loss: 103.5754\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.7717 - val_loss: 103.9720\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.5007 - val_loss: 104.4106\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.0049 - val_loss: 102.1743\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.3932 - val_loss: 101.3077\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.4182 - val_loss: 99.3783\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.3736 - val_loss: 99.6170\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.1194 - val_loss: 102.3249\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.0178 - val_loss: 98.0556\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.0814 - val_loss: 96.9979\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.3836 - val_loss: 97.3323\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.7151 - val_loss: 96.3617\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.3141 - val_loss: 94.9170\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.1475 - val_loss: 93.6984\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.9511 - val_loss: 96.9921\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.4231 - val_loss: 93.5279\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.2207 - val_loss: 93.8916\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.4199 - val_loss: 91.8367\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.6534 - val_loss: 94.1240\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.3646 - val_loss: 90.2331\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.8399 - val_loss: 90.4402\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.6078 - val_loss: 89.2410\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.4023 - val_loss: 88.4570\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.0154 - val_loss: 92.9500\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.7870 - val_loss: 87.0592\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.8921 - val_loss: 87.3031\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.3699 - val_loss: 85.3459\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.5773 - val_loss: 91.5301\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.8768 - val_loss: 86.1822\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.3234 - val_loss: 84.9167\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.5315 - val_loss: 83.8654\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.9534 - val_loss: 83.0088\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 664us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  29 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 725.0922 - val_loss: 508.8412\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 346.1580 - val_loss: 320.5081\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 265.0733 - val_loss: 247.8457\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 229.5502 - val_loss: 225.9657\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 211.3588 - val_loss: 206.1322\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 205.7213 - val_loss: 209.4562\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.4314 - val_loss: 191.6194\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 184.1686 - val_loss: 186.8501\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 178.5807 - val_loss: 181.7024\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.7531 - val_loss: 187.7822\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 169.6867 - val_loss: 175.3670\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.2650 - val_loss: 178.5369\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.1632 - val_loss: 166.6955\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.8891 - val_loss: 163.0464\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.2894 - val_loss: 178.8559\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.2760 - val_loss: 155.0073\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.1740 - val_loss: 151.3197\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.1610 - val_loss: 151.3841\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.1990 - val_loss: 155.0913\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.0323 - val_loss: 146.9604\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.8279 - val_loss: 146.9413\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.9174 - val_loss: 139.8279\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.3976 - val_loss: 135.7432\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.8590 - val_loss: 132.6438\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.4202 - val_loss: 140.9062\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.8622 - val_loss: 125.0384\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.7084 - val_loss: 123.2806\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.3687 - val_loss: 122.6088\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.7931 - val_loss: 114.2917\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.9636 - val_loss: 112.4898\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.9154 - val_loss: 111.0978\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 101.3754 - val_loss: 106.8911\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.5980 - val_loss: 104.6701\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.6757 - val_loss: 105.0845\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.4731 - val_loss: 103.4342\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.8669 - val_loss: 99.8300\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.2353 - val_loss: 103.4687\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.5279 - val_loss: 98.9493\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.0292 - val_loss: 97.8365\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.5249 - val_loss: 95.1375\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.5943 - val_loss: 94.8077\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.1513 - val_loss: 96.8173\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.5908 - val_loss: 93.3848\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.5596 - val_loss: 93.6312\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.1484 - val_loss: 98.7041\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.2094 - val_loss: 91.3179\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.3494 - val_loss: 91.0256\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.1434 - val_loss: 89.7048\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.5546 - val_loss: 87.6719\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.0613 - val_loss: 86.1847\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.5606 - val_loss: 85.8914\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.9476 - val_loss: 86.6101\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.0780 - val_loss: 85.5201\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.8563 - val_loss: 85.0074\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.3515 - val_loss: 85.3461\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.5676 - val_loss: 88.2058\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.3637 - val_loss: 83.9403\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.8985 - val_loss: 84.5267\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.1400 - val_loss: 82.5155\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.8889 - val_loss: 85.0698\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.2804 - val_loss: 82.4267\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.3973 - val_loss: 90.5286\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.8830 - val_loss: 83.5094\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.8989 - val_loss: 82.0399\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.4566 - val_loss: 80.8284\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.4458 - val_loss: 80.5159\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 74.9587 - val_loss: 81.2051\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 73.9334 - val_loss: 81.2303\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.2008 - val_loss: 83.8947\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 74.8570 - val_loss: 80.5007\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 73.6103 - val_loss: 81.4011\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 72.5156 - val_loss: 80.5325\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 71.7514 - val_loss: 83.7358\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 73.1831 - val_loss: 81.1325\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 71.6983 - val_loss: 82.9383\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 70.1927 - val_loss: 80.2937\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 69.8403 - val_loss: 79.9138\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 68.8263 - val_loss: 80.3918\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 68.1345 - val_loss: 79.0622\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 72.1999 - val_loss: 79.0618\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 67.5911 - val_loss: 78.0490\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 66.2573 - val_loss: 77.4364\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 67.0124 - val_loss: 78.0093\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 65.7149 - val_loss: 77.9557\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 66.9986 - val_loss: 77.0995\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 65.3320 - val_loss: 76.4098\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 64.3507 - val_loss: 74.9915\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 63.5549 - val_loss: 74.2194\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 63.9465 - val_loss: 73.9385\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 64.3690 - val_loss: 73.2958\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 62.3812 - val_loss: 76.7925\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 65.3408 - val_loss: 72.5984\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 63.5238 - val_loss: 73.0123\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 62.3226 - val_loss: 71.8091\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 62.4838 - val_loss: 71.6464\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 61.2491 - val_loss: 71.7329\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 63.1851 - val_loss: 73.6862\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 62.0709 - val_loss: 70.6623\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 60.9863 - val_loss: 72.2465\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 60.3047 - val_loss: 69.8256\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 664us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  30 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 141947.3750 - val_loss: 89323.9531\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 58143.6953 - val_loss: 32784.0156\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 20869.4355 - val_loss: 12003.2734\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 7651.3047 - val_loss: 3935.4043\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2437.2771 - val_loss: 1154.9655\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 856.0555 - val_loss: 555.4548\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 597.5567 - val_loss: 489.5902\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 560.4601 - val_loss: 474.0301\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 540.3018 - val_loss: 458.9863\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 522.2687 - val_loss: 447.7382\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 506.6155 - val_loss: 436.2282\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 492.6399 - val_loss: 426.5989\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 479.3181 - val_loss: 417.9163\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 468.3369 - val_loss: 409.0004\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 456.1800 - val_loss: 402.2455\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 446.9376 - val_loss: 393.8532\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 436.7267 - val_loss: 387.3785\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 429.2195 - val_loss: 380.2354\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 419.6831 - val_loss: 374.1862\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 411.6013 - val_loss: 369.5208\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 404.0976 - val_loss: 362.8975\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 396.9413 - val_loss: 357.5357\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 390.7639 - val_loss: 353.6186\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 385.1459 - val_loss: 349.6160\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 379.8768 - val_loss: 346.1349\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 374.5480 - val_loss: 343.3352\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 370.1516 - val_loss: 339.7979\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 366.9416 - val_loss: 337.5701\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 361.5855 - val_loss: 332.8572\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 357.7604 - val_loss: 329.9758\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 354.0104 - val_loss: 326.6845\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 349.5560 - val_loss: 324.2933\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 346.1031 - val_loss: 321.3480\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 342.4811 - val_loss: 319.4014\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 339.1600 - val_loss: 316.9163\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 335.9207 - val_loss: 313.2734\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 332.4413 - val_loss: 310.6384\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 329.3656 - val_loss: 309.0343\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 326.3846 - val_loss: 306.0219\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 323.8030 - val_loss: 303.1301\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 319.7563 - val_loss: 301.8767\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 316.8723 - val_loss: 299.7523\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 314.3755 - val_loss: 296.3812\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 311.0800 - val_loss: 294.8304\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 309.5458 - val_loss: 292.1180\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 305.5646 - val_loss: 291.7569\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 303.4823 - val_loss: 288.3288\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 300.4052 - val_loss: 285.5712\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 298.0453 - val_loss: 283.7639\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 296.0378 - val_loss: 281.9343\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 293.1357 - val_loss: 279.5457\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 290.6755 - val_loss: 278.3109\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 289.0508 - val_loss: 276.6743\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 286.0629 - val_loss: 274.3904\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 284.2651 - val_loss: 272.3431\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 281.5381 - val_loss: 271.9110\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 279.1094 - val_loss: 269.3895\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 277.0063 - val_loss: 266.9993\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 274.6649 - val_loss: 266.1106\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 273.8671 - val_loss: 263.9647\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 271.7632 - val_loss: 263.6300\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 269.3877 - val_loss: 260.1263\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 266.5919 - val_loss: 259.4377\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 265.1069 - val_loss: 257.9331\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.1359 - val_loss: 255.3623\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 260.9463 - val_loss: 253.5885\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 258.3635 - val_loss: 252.0147\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 256.5412 - val_loss: 250.4776\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 254.8142 - val_loss: 250.0293\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 253.5716 - val_loss: 246.9292\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 250.0075 - val_loss: 246.9170\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 248.5405 - val_loss: 244.7371\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 246.5141 - val_loss: 242.9606\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.5377 - val_loss: 242.0477\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.3734 - val_loss: 241.7150\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.0077 - val_loss: 238.6501\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.4951 - val_loss: 237.6931\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.7218 - val_loss: 236.0394\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3796 - val_loss: 234.5334\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 233.9506 - val_loss: 232.9315\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 232.3124 - val_loss: 231.8940\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 230.6152 - val_loss: 229.9822\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 228.9513 - val_loss: 229.5502\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 228.3554 - val_loss: 227.1579\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 226.1033 - val_loss: 225.7916\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 223.3660 - val_loss: 224.6492\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 221.4723 - val_loss: 222.5026\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 219.4369 - val_loss: 220.7387\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.2821 - val_loss: 219.5526\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 215.5355 - val_loss: 218.6403\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 213.0868 - val_loss: 214.7815\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 211.7857 - val_loss: 212.8508\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 207.9047 - val_loss: 210.8387\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 204.8932 - val_loss: 208.4754\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.4857 - val_loss: 206.8700\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.5784 - val_loss: 204.5150\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 197.0912 - val_loss: 203.0675\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 195.6460 - val_loss: 200.5621\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.2844 - val_loss: 198.2375\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 191.3897 - val_loss: 197.6305\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 668us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  31 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3108.9473 - val_loss: 1626.1348\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 871.1056 - val_loss: 336.9179\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 308.2509 - val_loss: 276.2484\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 287.2669 - val_loss: 245.0928\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 249.9431 - val_loss: 227.9537\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 232.6390 - val_loss: 224.0603\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 225.3667 - val_loss: 221.2425\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 221.4950 - val_loss: 217.9044\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.3636 - val_loss: 214.6245\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 212.8754 - val_loss: 211.6053\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 211.3570 - val_loss: 208.4456\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 205.6416 - val_loss: 205.1895\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.0299 - val_loss: 201.8306\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.4766 - val_loss: 198.8483\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 195.6930 - val_loss: 195.5453\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 191.9734 - val_loss: 192.6444\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 188.6926 - val_loss: 190.2616\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 186.2082 - val_loss: 188.1750\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 183.1098 - val_loss: 185.6095\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.7216 - val_loss: 183.0031\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.8020 - val_loss: 180.6510\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 175.7794 - val_loss: 178.8604\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 172.6685 - val_loss: 175.3076\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.3282 - val_loss: 173.6106\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 167.8909 - val_loss: 172.1593\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.7446 - val_loss: 170.0669\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.8726 - val_loss: 167.3312\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.8949 - val_loss: 164.6517\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.8887 - val_loss: 165.6040\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.7219 - val_loss: 161.2502\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.7302 - val_loss: 160.1366\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.0930 - val_loss: 157.4214\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.1563 - val_loss: 157.3678\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.8875 - val_loss: 154.3685\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.2358 - val_loss: 153.5246\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.8407 - val_loss: 150.7181\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.5323 - val_loss: 148.8714\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.2621 - val_loss: 146.5558\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.2289 - val_loss: 144.5183\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.6985 - val_loss: 144.0495\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.0061 - val_loss: 140.6585\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.6877 - val_loss: 138.9220\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.9953 - val_loss: 139.0886\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.9669 - val_loss: 135.2556\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.3933 - val_loss: 135.2825\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.1280 - val_loss: 131.9558\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.9079 - val_loss: 131.5275\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.2300 - val_loss: 129.2905\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.3861 - val_loss: 127.2472\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.5526 - val_loss: 126.1784\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.4488 - val_loss: 125.4078\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.8959 - val_loss: 123.5998\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.4448 - val_loss: 121.5132\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.5001 - val_loss: 122.8863\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.5611 - val_loss: 119.1221\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.3271 - val_loss: 118.4011\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.7402 - val_loss: 119.9286\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.1957 - val_loss: 115.7966\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.6120 - val_loss: 115.6564\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.3881 - val_loss: 114.8962\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.5690 - val_loss: 112.4399\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.2071 - val_loss: 111.2425\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 93.9739 - val_loss: 112.0840\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.0256 - val_loss: 109.6419\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.5794 - val_loss: 107.9035\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.6472 - val_loss: 107.0839\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.3067 - val_loss: 109.2803\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.7337 - val_loss: 104.7575\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.0275 - val_loss: 104.2645\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.6112 - val_loss: 103.1849\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.9357 - val_loss: 102.8828\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.4469 - val_loss: 103.0219\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.9657 - val_loss: 100.9753\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.9006 - val_loss: 100.2690\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.0028 - val_loss: 102.7275\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.0942 - val_loss: 99.2598\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.8840 - val_loss: 100.8152\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.2854 - val_loss: 99.9386\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.7451 - val_loss: 98.4876\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.3162 - val_loss: 97.6172\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.4449 - val_loss: 98.6094\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.4645 - val_loss: 96.8295\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.7519 - val_loss: 108.2234\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.9385 - val_loss: 101.5729\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.9358 - val_loss: 96.8106\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.6222 - val_loss: 99.4173\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.2530 - val_loss: 95.6352\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.7538 - val_loss: 98.4578\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.3810 - val_loss: 94.9859\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.9760 - val_loss: 100.1078\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.0395 - val_loss: 94.7269\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.1708 - val_loss: 97.3391\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.3553 - val_loss: 97.1527\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.8357 - val_loss: 95.2047\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.9681 - val_loss: 93.8741\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.4167 - val_loss: 93.3055\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.9309 - val_loss: 95.2252\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.7142 - val_loss: 95.3917\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.6270 - val_loss: 93.1394\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.6884 - val_loss: 92.6801\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 668us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  32 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 170945.6250 - val_loss: 105880.8281\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 68473.1172 - val_loss: 33924.7930\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 18886.0332 - val_loss: 6499.4175\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3468.9490 - val_loss: 1416.2455\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1527.8728 - val_loss: 1418.8822\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1402.1257 - val_loss: 1146.0569\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1197.3212 - val_loss: 1001.9247\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1099.1665 - val_loss: 911.0828\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1000.8545 - val_loss: 833.4296\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 917.6202 - val_loss: 762.9155\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 826.0693 - val_loss: 692.6674\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 754.2564 - val_loss: 635.3542\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 687.3228 - val_loss: 590.2537\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 633.1992 - val_loss: 548.4592\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 588.7653 - val_loss: 512.7125\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 546.5121 - val_loss: 481.9267\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 512.6545 - val_loss: 458.2931\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 486.7205 - val_loss: 438.1038\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 464.9752 - val_loss: 420.6707\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 447.3864 - val_loss: 405.0981\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 431.0199 - val_loss: 393.0403\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 417.7125 - val_loss: 381.4879\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 406.2787 - val_loss: 371.7758\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 395.2910 - val_loss: 364.7520\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 386.7491 - val_loss: 357.7460\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 379.0749 - val_loss: 349.6508\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 372.1401 - val_loss: 343.3521\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 364.8728 - val_loss: 339.3234\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 359.6442 - val_loss: 331.7760\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 353.2897 - val_loss: 326.8838\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 348.7698 - val_loss: 321.9674\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 343.5965 - val_loss: 317.9519\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 338.8826 - val_loss: 313.2252\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 336.6484 - val_loss: 309.5071\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 331.7353 - val_loss: 303.9231\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 327.3430 - val_loss: 302.0219\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 322.0866 - val_loss: 296.1280\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 318.7438 - val_loss: 291.4707\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 314.7067 - val_loss: 288.6716\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 310.9004 - val_loss: 284.9837\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 308.0256 - val_loss: 281.0128\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 304.9680 - val_loss: 278.2713\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 300.5461 - val_loss: 273.7681\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 297.2835 - val_loss: 270.2991\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 294.0441 - val_loss: 266.8147\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 290.8032 - val_loss: 263.6087\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 288.3236 - val_loss: 259.9559\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 283.7874 - val_loss: 255.0112\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 280.9613 - val_loss: 253.3843\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 278.0170 - val_loss: 248.9675\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 274.8760 - val_loss: 246.3541\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 271.3481 - val_loss: 241.5970\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 270.5465 - val_loss: 239.9008\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 265.8202 - val_loss: 236.2413\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.9496 - val_loss: 233.1371\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 260.3689 - val_loss: 230.0410\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 258.0984 - val_loss: 227.3535\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.6573 - val_loss: 226.8220\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 252.8513 - val_loss: 222.0535\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 250.9466 - val_loss: 219.1974\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 248.0536 - val_loss: 217.4319\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.6507 - val_loss: 216.4775\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.7544 - val_loss: 212.1915\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.0792 - val_loss: 212.2078\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.9486 - val_loss: 206.8513\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.8698 - val_loss: 206.4006\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.0352 - val_loss: 205.4469\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 232.7903 - val_loss: 201.3415\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 230.8401 - val_loss: 199.8761\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 229.0140 - val_loss: 197.2315\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 227.1466 - val_loss: 194.9898\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 224.9311 - val_loss: 195.3216\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 223.4910 - val_loss: 191.8747\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 221.3470 - val_loss: 190.4454\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 219.1210 - val_loss: 187.2246\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.4841 - val_loss: 186.3174\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 218.7451 - val_loss: 184.8907\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.6334 - val_loss: 181.9238\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 213.0270 - val_loss: 183.6609\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 210.6010 - val_loss: 178.4473\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 209.0311 - val_loss: 176.8883\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 207.2854 - val_loss: 177.1980\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 206.8105 - val_loss: 175.4574\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.7140 - val_loss: 172.2529\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.8288 - val_loss: 172.0035\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.5332 - val_loss: 170.2446\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.1080 - val_loss: 169.3089\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.0273 - val_loss: 168.1797\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 195.9613 - val_loss: 165.0734\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 195.2116 - val_loss: 163.8778\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.2123 - val_loss: 162.6635\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 191.8344 - val_loss: 161.8223\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 190.6052 - val_loss: 161.9592\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 189.8245 - val_loss: 158.6650\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.2102 - val_loss: 160.7566\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 186.2961 - val_loss: 156.2660\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 184.3322 - val_loss: 156.7476\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 184.3444 - val_loss: 153.9240\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.7647 - val_loss: 154.9868\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.4451 - val_loss: 151.3155\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 556us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  33 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 200122.2188 - val_loss: 162627.7969\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133109.5781 - val_loss: 107676.2031\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84308.8281 - val_loss: 63141.0156\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 41624.7344 - val_loss: 24533.8281\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 13071.7461 - val_loss: 6993.9395\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4803.1597 - val_loss: 4228.3789\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3859.0552 - val_loss: 3689.0518\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3319.6777 - val_loss: 3286.3865\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2890.3743 - val_loss: 2972.3018\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2571.0605 - val_loss: 2697.4978\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2320.7419 - val_loss: 2458.6167\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2092.4404 - val_loss: 2242.7336\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1896.0902 - val_loss: 2047.2745\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1731.0554 - val_loss: 1868.5933\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1575.4139 - val_loss: 1704.1023\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1433.7968 - val_loss: 1552.5400\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1303.1586 - val_loss: 1411.2517\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1177.2814 - val_loss: 1277.1812\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1067.1484 - val_loss: 1150.7340\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 962.1564 - val_loss: 1038.6404\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 870.3723 - val_loss: 931.8975\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 785.4933 - val_loss: 837.5833\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 711.9694 - val_loss: 757.3939\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 652.0398 - val_loss: 682.4967\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 597.9408 - val_loss: 615.8823\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 549.5936 - val_loss: 558.9003\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 505.6296 - val_loss: 509.5916\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 468.1747 - val_loss: 465.8365\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 432.4478 - val_loss: 425.7034\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 395.3957 - val_loss: 383.3275\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 356.1931 - val_loss: 340.2948\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 319.1274 - val_loss: 304.5068\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 287.1952 - val_loss: 276.0409\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 260.6685 - val_loss: 253.9681\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.9003 - val_loss: 237.0941\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 222.8399 - val_loss: 223.3289\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.8472 - val_loss: 211.5381\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.8756 - val_loss: 203.3130\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.6172 - val_loss: 194.6358\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.7312 - val_loss: 187.8084\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 172.6219 - val_loss: 180.9801\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.6143 - val_loss: 175.0528\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.0813 - val_loss: 169.7014\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.6738 - val_loss: 165.6537\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.3135 - val_loss: 162.7626\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.4461 - val_loss: 159.0269\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.8043 - val_loss: 155.4353\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.2197 - val_loss: 156.6534\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.8346 - val_loss: 150.5386\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.3972 - val_loss: 148.2765\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.5513 - val_loss: 145.8715\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.5307 - val_loss: 143.2719\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.9210 - val_loss: 142.7589\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.1015 - val_loss: 142.1983\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.8446 - val_loss: 139.3483\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.1668 - val_loss: 146.0047\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.7904 - val_loss: 138.5625\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.3862 - val_loss: 138.5375\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.8026 - val_loss: 135.5591\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.6299 - val_loss: 139.4208\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.1778 - val_loss: 132.6840\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.9723 - val_loss: 130.4143\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.0631 - val_loss: 130.0230\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.7119 - val_loss: 131.0430\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.7920 - val_loss: 127.2435\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.7515 - val_loss: 127.8583\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.4139 - val_loss: 132.0427\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.4626 - val_loss: 124.9284\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.7985 - val_loss: 123.8208\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.1337 - val_loss: 127.5466\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.9075 - val_loss: 122.3529\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.4442 - val_loss: 129.3723\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.4443 - val_loss: 122.7413\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.9530 - val_loss: 121.3170\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.9674 - val_loss: 123.0150\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.2181 - val_loss: 120.8518\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.1304 - val_loss: 118.7710\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.3620 - val_loss: 120.1888\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.3483 - val_loss: 119.4684\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.2157 - val_loss: 118.0707\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.7745 - val_loss: 123.4417\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.4899 - val_loss: 116.3824\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.7643 - val_loss: 121.5008\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.0783 - val_loss: 119.1443\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.5047 - val_loss: 115.0605\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.2698 - val_loss: 124.6248\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.0627 - val_loss: 114.7960\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.1433 - val_loss: 113.9863\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.4718 - val_loss: 123.3012\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.0851 - val_loss: 122.3347\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.2995 - val_loss: 114.4828\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.1261 - val_loss: 124.0951\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.1823 - val_loss: 112.0404\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.4327 - val_loss: 111.5708\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.0961 - val_loss: 115.9382\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.0005 - val_loss: 117.7313\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.4624 - val_loss: 112.4868\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.2224 - val_loss: 111.7414\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.5536 - val_loss: 110.9448\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.9313 - val_loss: 116.8700\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 555us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  34 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2985.2654 - val_loss: 1749.7435\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1390.3157 - val_loss: 831.5822\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 959.8484 - val_loss: 619.0977\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 672.1179 - val_loss: 405.9895\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 457.0015 - val_loss: 319.6147\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 353.9104 - val_loss: 302.1989\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 301.6158 - val_loss: 244.7152\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 266.5496 - val_loss: 225.4952\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.4563 - val_loss: 225.1313\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 221.0490 - val_loss: 207.0322\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 201.7382 - val_loss: 182.7338\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 190.9894 - val_loss: 177.8747\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 184.1803 - val_loss: 163.0994\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.2683 - val_loss: 156.9561\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.6995 - val_loss: 155.9583\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.3376 - val_loss: 160.0895\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.2546 - val_loss: 159.3997\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.9169 - val_loss: 132.5567\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.5649 - val_loss: 128.5563\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.8505 - val_loss: 123.5443\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.3490 - val_loss: 124.0220\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.0449 - val_loss: 117.5523\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.6649 - val_loss: 117.8603\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.7909 - val_loss: 132.4398\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.5741 - val_loss: 132.6002\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.6064 - val_loss: 116.2753\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.5762 - val_loss: 109.6826\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.2972 - val_loss: 116.2494\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.5466 - val_loss: 111.7898\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.4197 - val_loss: 106.8330\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.6340 - val_loss: 116.7857\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.3476 - val_loss: 127.5172\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.3694 - val_loss: 135.6495\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.0577 - val_loss: 106.1084\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.6468 - val_loss: 112.3429\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.8661 - val_loss: 104.3595\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.8264 - val_loss: 107.4617\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.3312 - val_loss: 104.2473\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.9846 - val_loss: 107.9474\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.4643 - val_loss: 116.4283\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.4191 - val_loss: 108.5732\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.7759 - val_loss: 113.2904\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.5145 - val_loss: 103.9133\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.8059 - val_loss: 119.1686\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.4239 - val_loss: 114.9904\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.5039 - val_loss: 104.6880\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.1539 - val_loss: 112.7271\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.9212 - val_loss: 103.3914\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.8303 - val_loss: 102.3754\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.6639 - val_loss: 121.5430\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.3433 - val_loss: 107.4069\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.5585 - val_loss: 102.5222\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 107.3453 - val_loss: 109.6972\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.2357 - val_loss: 109.6184\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.9184 - val_loss: 111.2970\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.0530 - val_loss: 103.3535\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.8332 - val_loss: 104.3956\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.6869 - val_loss: 103.5189\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.9571 - val_loss: 102.6163\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.5394 - val_loss: 110.1168\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.8669 - val_loss: 103.2221\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.4010 - val_loss: 105.8722\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.5010 - val_loss: 103.8301\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.3173 - val_loss: 116.3501\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.7244 - val_loss: 106.4923\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.8301 - val_loss: 122.1625\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.8316 - val_loss: 105.8701\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.0751 - val_loss: 105.9125\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.3966 - val_loss: 103.3650\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.7377 - val_loss: 102.9773\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.3883 - val_loss: 103.2024\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.0888 - val_loss: 102.0569\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.2523 - val_loss: 109.6755\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.1988 - val_loss: 104.0539\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.6242 - val_loss: 102.5419\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.6998 - val_loss: 104.1557\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.7729 - val_loss: 111.3454\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.9182 - val_loss: 115.8079\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.2115 - val_loss: 104.9456\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.2395 - val_loss: 131.1633\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.4748 - val_loss: 102.1491\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.1282 - val_loss: 102.8595\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.8551 - val_loss: 109.3456\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.2300 - val_loss: 101.8188\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.5187 - val_loss: 102.5979\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.7324 - val_loss: 113.5345\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.4641 - val_loss: 103.4002\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.0016 - val_loss: 118.4497\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.7345 - val_loss: 110.2695\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.8846 - val_loss: 102.3913\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.0672 - val_loss: 104.5257\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.0705 - val_loss: 111.1145\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.2996 - val_loss: 102.3050\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.6250 - val_loss: 108.7728\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.8528 - val_loss: 119.6011\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.2431 - val_loss: 103.3051\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.8571 - val_loss: 103.3114\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.0048 - val_loss: 103.5641\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.0127 - val_loss: 101.9698\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.5862 - val_loss: 102.5782\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 671us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  35 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 244715.6875 - val_loss: 190949.6250\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155817.4531 - val_loss: 123512.6797\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101396.0859 - val_loss: 80671.6562\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 65845.7188 - val_loss: 51917.8164\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 41862.9492 - val_loss: 32215.9082\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 25491.2148 - val_loss: 19015.9922\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 14719.7383 - val_loss: 10529.7305\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 7940.8257 - val_loss: 5558.5225\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4131.3877 - val_loss: 2893.7063\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2208.1987 - val_loss: 1687.0814\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1394.5945 - val_loss: 1221.0585\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1077.4543 - val_loss: 1079.7683\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 981.1166 - val_loss: 1033.9923\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 952.0942 - val_loss: 1013.4420\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 934.4358 - val_loss: 997.7119\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 920.0359 - val_loss: 983.0373\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 905.7963 - val_loss: 967.2051\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 891.3217 - val_loss: 951.1167\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 876.8804 - val_loss: 935.0848\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 862.3712 - val_loss: 919.0728\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 847.8048 - val_loss: 903.7541\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 833.9135 - val_loss: 888.2536\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 819.1612 - val_loss: 873.0131\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 805.0497 - val_loss: 857.6786\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 792.2674 - val_loss: 842.2382\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 777.3237 - val_loss: 827.8480\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 764.4960 - val_loss: 812.6575\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 750.8173 - val_loss: 798.3770\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 737.7557 - val_loss: 783.7905\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 725.3486 - val_loss: 770.3718\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 712.6663 - val_loss: 755.6190\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 699.1217 - val_loss: 742.2307\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 688.3861 - val_loss: 728.7272\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 675.0912 - val_loss: 715.8146\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 663.3696 - val_loss: 703.5873\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 651.5401 - val_loss: 690.7259\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 640.2918 - val_loss: 678.4537\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 629.0631 - val_loss: 666.6459\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 618.2864 - val_loss: 654.8946\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 607.7817 - val_loss: 643.9400\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 597.6279 - val_loss: 632.2295\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 587.3693 - val_loss: 621.3290\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 577.6128 - val_loss: 611.3493\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 567.9162 - val_loss: 600.6696\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 558.7362 - val_loss: 590.3621\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 549.2184 - val_loss: 580.3044\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 539.6400 - val_loss: 570.8388\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 531.3289 - val_loss: 561.3609\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 522.4213 - val_loss: 552.7751\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 514.2615 - val_loss: 543.0076\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 505.5153 - val_loss: 534.1080\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 497.5472 - val_loss: 525.3347\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 489.6791 - val_loss: 517.0924\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 482.3873 - val_loss: 508.7009\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 475.2880 - val_loss: 500.6656\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 467.8328 - val_loss: 492.9348\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 460.8315 - val_loss: 484.6079\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 453.7159 - val_loss: 477.2097\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 446.3686 - val_loss: 470.2869\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 440.1560 - val_loss: 462.8486\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 433.6881 - val_loss: 455.6852\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 427.4207 - val_loss: 449.0891\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 421.1844 - val_loss: 442.3114\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 415.2606 - val_loss: 435.8896\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 409.4982 - val_loss: 429.3726\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 403.5661 - val_loss: 423.2252\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 398.1116 - val_loss: 417.1729\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 392.9533 - val_loss: 411.0923\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 387.0703 - val_loss: 405.4534\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 382.1795 - val_loss: 399.8318\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 377.0137 - val_loss: 394.2820\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 371.9815 - val_loss: 388.7132\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 367.2355 - val_loss: 383.2931\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 363.0824 - val_loss: 378.4361\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 357.9846 - val_loss: 373.1212\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 353.4704 - val_loss: 368.3837\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 349.1730 - val_loss: 363.5424\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 345.3391 - val_loss: 358.7442\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 340.5877 - val_loss: 354.4497\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 336.9650 - val_loss: 349.9347\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 332.5031 - val_loss: 345.7665\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 328.6468 - val_loss: 341.3285\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 325.2542 - val_loss: 336.9561\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 321.0695 - val_loss: 333.4769\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 317.4953 - val_loss: 329.2167\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 314.5321 - val_loss: 325.1590\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 310.4232 - val_loss: 321.4192\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 307.4411 - val_loss: 317.6790\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 303.7936 - val_loss: 313.9453\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 300.5065 - val_loss: 310.3940\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 297.6722 - val_loss: 307.5090\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 294.3328 - val_loss: 303.5056\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 291.2607 - val_loss: 300.2300\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 288.2231 - val_loss: 297.1212\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 285.3730 - val_loss: 294.0462\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 282.7626 - val_loss: 290.9853\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 279.8401 - val_loss: 287.6687\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 277.2502 - val_loss: 284.8301\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 274.5891 - val_loss: 282.0730\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 271.7817 - val_loss: 278.9661\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  36 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2225.4937 - val_loss: 1138.8915\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 734.6497 - val_loss: 523.3705\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 461.4671 - val_loss: 287.9078\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 274.6069 - val_loss: 263.2112\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 230.5492 - val_loss: 243.1156\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 207.1168 - val_loss: 231.3269\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 191.4553 - val_loss: 222.4863\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 184.4874 - val_loss: 221.0544\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.3324 - val_loss: 214.9430\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.8450 - val_loss: 213.9228\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 166.1202 - val_loss: 223.3001\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.3592 - val_loss: 198.7350\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.5861 - val_loss: 202.4471\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.2415 - val_loss: 236.4222\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.2010 - val_loss: 189.6589\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.3024 - val_loss: 187.2339\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.1345 - val_loss: 187.4059\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.3083 - val_loss: 187.2142\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.0026 - val_loss: 184.9982\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.1910 - val_loss: 180.2934\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.1084 - val_loss: 186.2476\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.8685 - val_loss: 172.3304\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.4732 - val_loss: 172.7009\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.1397 - val_loss: 167.6030\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.5125 - val_loss: 167.9594\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.1629 - val_loss: 164.8470\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.4669 - val_loss: 164.5040\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.2188 - val_loss: 171.0405\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.6250 - val_loss: 159.6323\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.3537 - val_loss: 158.9121\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.4523 - val_loss: 155.9718\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.1667 - val_loss: 153.0755\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.7146 - val_loss: 155.6870\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.7557 - val_loss: 150.5986\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.0440 - val_loss: 153.0260\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.3533 - val_loss: 159.3450\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.2588 - val_loss: 146.5443\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.5235 - val_loss: 151.8943\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.8515 - val_loss: 149.6361\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.4234 - val_loss: 146.8317\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.1314 - val_loss: 143.5837\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.0114 - val_loss: 159.6899\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.1704 - val_loss: 142.0033\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.7669 - val_loss: 139.5835\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.7053 - val_loss: 156.8846\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.1101 - val_loss: 140.6598\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.1005 - val_loss: 144.7900\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.0805 - val_loss: 137.4437\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.2493 - val_loss: 136.5998\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.1428 - val_loss: 136.7618\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.8484 - val_loss: 138.0184\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.2559 - val_loss: 137.0459\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.7848 - val_loss: 134.6673\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.7108 - val_loss: 144.9062\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.6507 - val_loss: 135.1186\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.8261 - val_loss: 135.3273\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.2743 - val_loss: 140.1723\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.1622 - val_loss: 132.1751\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.0635 - val_loss: 133.3690\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.7977 - val_loss: 131.2806\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.7035 - val_loss: 132.0145\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.7513 - val_loss: 134.4387\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.0155 - val_loss: 136.7909\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.9782 - val_loss: 135.7464\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.7525 - val_loss: 133.5540\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.1460 - val_loss: 145.6736\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.7994 - val_loss: 132.1384\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.1052 - val_loss: 135.1394\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.0481 - val_loss: 133.4412\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.0059 - val_loss: 127.9926\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.8565 - val_loss: 143.1205\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.1685 - val_loss: 133.6796\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.0278 - val_loss: 127.2708\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.5235 - val_loss: 136.9914\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.5563 - val_loss: 126.1466\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.3194 - val_loss: 125.2531\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.0831 - val_loss: 125.7522\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.7762 - val_loss: 124.8065\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.6745 - val_loss: 129.5747\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.7503 - val_loss: 122.7249\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.8485 - val_loss: 124.4884\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.6660 - val_loss: 121.0136\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.5861 - val_loss: 121.4558\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.9666 - val_loss: 123.4963\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.5936 - val_loss: 120.9154\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.8272 - val_loss: 120.5466\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.9359 - val_loss: 123.2806\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.7072 - val_loss: 120.2134\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.3346 - val_loss: 120.2250\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.2791 - val_loss: 118.5050\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.1608 - val_loss: 119.6718\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.4570 - val_loss: 120.9246\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.6014 - val_loss: 119.2643\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.0809 - val_loss: 117.9209\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.5540 - val_loss: 133.5656\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.9935 - val_loss: 129.1263\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.5898 - val_loss: 117.0022\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.1160 - val_loss: 119.0797\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.8527 - val_loss: 114.8852\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.0832 - val_loss: 114.7456\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 557us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  37 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 114855.7422 - val_loss: 58562.1523\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32089.3203 - val_loss: 10790.9121\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4251.2207 - val_loss: 1119.1901\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1095.2002 - val_loss: 1221.9456\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1087.9089 - val_loss: 921.4202\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 844.6013 - val_loss: 852.4316\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 797.7329 - val_loss: 797.5273\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 758.4586 - val_loss: 747.1819\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 724.8024 - val_loss: 710.4682\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 696.0246 - val_loss: 677.6734\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 667.4370 - val_loss: 645.2084\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 641.5079 - val_loss: 618.0782\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 615.2037 - val_loss: 588.1618\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 591.2080 - val_loss: 562.0027\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 569.0790 - val_loss: 538.4218\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 545.7172 - val_loss: 512.8353\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 525.3229 - val_loss: 490.6420\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 504.4155 - val_loss: 471.6276\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 486.6078 - val_loss: 451.8544\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 467.3943 - val_loss: 430.7119\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 450.9706 - val_loss: 413.8448\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 433.7393 - val_loss: 397.2818\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 418.7675 - val_loss: 381.9969\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 403.2153 - val_loss: 365.9065\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 389.0469 - val_loss: 351.8579\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 375.4873 - val_loss: 339.0361\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 362.8369 - val_loss: 326.8083\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 350.2822 - val_loss: 312.9465\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 338.8329 - val_loss: 301.7107\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 326.8150 - val_loss: 291.7438\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 317.3927 - val_loss: 280.2850\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 307.5267 - val_loss: 271.7328\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 297.7158 - val_loss: 260.8090\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 286.5858 - val_loss: 256.1450\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 278.3828 - val_loss: 244.1626\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 270.0161 - val_loss: 236.8348\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.5093 - val_loss: 229.4479\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 254.5463 - val_loss: 222.9254\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 247.1374 - val_loss: 216.8979\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 240.0357 - val_loss: 208.8963\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 233.0861 - val_loss: 203.7166\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 228.4790 - val_loss: 197.1107\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 221.4312 - val_loss: 192.3499\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.1431 - val_loss: 188.0232\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 210.3432 - val_loss: 182.2878\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 204.6735 - val_loss: 177.7612\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 201.2703 - val_loss: 174.2470\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 195.8751 - val_loss: 169.1360\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 191.0476 - val_loss: 166.5564\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.9924 - val_loss: 161.5906\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 183.0232 - val_loss: 158.5906\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.6176 - val_loss: 155.5608\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.2309 - val_loss: 152.2996\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 171.0961 - val_loss: 149.5953\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.1311 - val_loss: 147.2126\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.7224 - val_loss: 143.9724\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.1166 - val_loss: 141.5038\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.0718 - val_loss: 140.8371\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.3367 - val_loss: 137.2397\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.9064 - val_loss: 135.1380\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.2692 - val_loss: 133.0548\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.2644 - val_loss: 132.1366\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.6355 - val_loss: 129.5864\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.2050 - val_loss: 127.8383\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.6652 - val_loss: 127.3774\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.1223 - val_loss: 124.4495\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.2232 - val_loss: 123.5633\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.9072 - val_loss: 121.7265\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.6895 - val_loss: 120.9312\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.7202 - val_loss: 119.4963\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.7767 - val_loss: 117.9731\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.0606 - val_loss: 117.4302\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.2267 - val_loss: 116.5521\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.5450 - val_loss: 114.9887\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.7018 - val_loss: 113.8653\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.7277 - val_loss: 112.7739\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.7539 - val_loss: 111.7434\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.4625 - val_loss: 110.8971\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.6158 - val_loss: 110.2793\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.8249 - val_loss: 109.1507\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.6841 - val_loss: 108.5708\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.0402 - val_loss: 108.0255\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.7905 - val_loss: 107.1169\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.4451 - val_loss: 106.4149\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.8081 - val_loss: 105.7864\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.9861 - val_loss: 105.7574\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.8121 - val_loss: 104.2005\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.7191 - val_loss: 104.5611\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.6836 - val_loss: 103.1815\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.6550 - val_loss: 102.6035\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.5891 - val_loss: 102.0518\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.1919 - val_loss: 101.7124\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.7226 - val_loss: 101.1298\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.1790 - val_loss: 102.1431\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.9999 - val_loss: 100.7540\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.5004 - val_loss: 100.1782\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.5904 - val_loss: 99.2256\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.2871 - val_loss: 98.6127\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.7141 - val_loss: 98.5723\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.9655 - val_loss: 97.6879\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 662us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  38 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1767.7279 - val_loss: 1744.8959\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1432.9005 - val_loss: 1503.0319\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1204.6924 - val_loss: 1253.0721\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1055.4080 - val_loss: 1100.1544\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 927.7614 - val_loss: 1003.4857\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 811.3464 - val_loss: 860.0477\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 712.6599 - val_loss: 780.9877\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 641.1961 - val_loss: 713.8312\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 600.0270 - val_loss: 680.8041\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 557.1992 - val_loss: 666.4026\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 491.6701 - val_loss: 577.6630\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 439.5229 - val_loss: 522.4191\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 416.9815 - val_loss: 493.9210\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 392.9849 - val_loss: 460.0127\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 356.2384 - val_loss: 438.6110\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 337.1985 - val_loss: 415.6595\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 323.1678 - val_loss: 391.1663\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 301.5345 - val_loss: 372.4484\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 290.4888 - val_loss: 356.0494\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 271.3258 - val_loss: 335.5226\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 256.8739 - val_loss: 329.3011\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 248.0791 - val_loss: 309.4358\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.8528 - val_loss: 297.2060\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 224.3484 - val_loss: 287.6652\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 215.6314 - val_loss: 277.0405\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.4045 - val_loss: 265.6662\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 195.5390 - val_loss: 256.8795\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 190.0515 - val_loss: 256.9367\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 189.9139 - val_loss: 238.6593\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.9878 - val_loss: 234.3979\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 172.0280 - val_loss: 224.9548\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 166.6349 - val_loss: 220.7626\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.6731 - val_loss: 215.3524\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.9021 - val_loss: 209.4556\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.3696 - val_loss: 206.1214\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.0477 - val_loss: 199.0424\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.6056 - val_loss: 195.1437\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.0865 - val_loss: 206.6506\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.4338 - val_loss: 191.1634\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.5288 - val_loss: 192.0822\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.8030 - val_loss: 181.9754\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.4841 - val_loss: 176.8473\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.3341 - val_loss: 172.5813\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.9556 - val_loss: 173.0970\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.7325 - val_loss: 167.6459\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.9701 - val_loss: 167.9880\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.9908 - val_loss: 169.6750\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.0292 - val_loss: 157.5837\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.3737 - val_loss: 159.0299\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.2538 - val_loss: 154.3166\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.6622 - val_loss: 154.1702\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.2009 - val_loss: 157.3384\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.7426 - val_loss: 148.3221\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.2177 - val_loss: 147.1676\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.1617 - val_loss: 147.8222\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.8158 - val_loss: 145.4788\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.3458 - val_loss: 142.6398\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.8971 - val_loss: 141.2225\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.4316 - val_loss: 142.0067\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.7483 - val_loss: 140.3277\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.6039 - val_loss: 139.7988\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.9225 - val_loss: 135.5695\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.1975 - val_loss: 134.2540\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.9620 - val_loss: 132.9700\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.5393 - val_loss: 143.9669\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.9281 - val_loss: 132.3479\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.5491 - val_loss: 132.5181\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.5150 - val_loss: 130.6187\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.8962 - val_loss: 128.8687\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.3482 - val_loss: 136.9234\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.3108 - val_loss: 129.1851\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.8077 - val_loss: 131.2099\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.9134 - val_loss: 127.2173\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.8585 - val_loss: 127.1294\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.8609 - val_loss: 125.8929\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.7891 - val_loss: 123.3103\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.7751 - val_loss: 129.4229\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.9738 - val_loss: 124.4500\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.9571 - val_loss: 123.6652\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.8710 - val_loss: 129.8356\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.7195 - val_loss: 120.7431\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.5356 - val_loss: 119.5794\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.0710 - val_loss: 119.2459\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.8324 - val_loss: 118.7528\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.9786 - val_loss: 123.9376\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.4688 - val_loss: 118.8794\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.5454 - val_loss: 117.1867\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.2102 - val_loss: 118.5569\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.6746 - val_loss: 114.5639\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.3649 - val_loss: 120.4038\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 81.0965 - val_loss: 115.4627\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.3600 - val_loss: 126.9759\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.8486 - val_loss: 112.0037\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.9000 - val_loss: 111.9455\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.7237 - val_loss: 109.6331\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.8614 - val_loss: 110.4341\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.4310 - val_loss: 114.0968\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.5275 - val_loss: 118.0514\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.2445 - val_loss: 116.1897\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.3340 - val_loss: 118.7416\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 550us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  39 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3081.0537 - val_loss: 1633.6251\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 878.3804 - val_loss: 363.6249\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 274.4590 - val_loss: 267.7505\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 256.2343 - val_loss: 248.8876\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.5176 - val_loss: 246.0108\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 233.9056 - val_loss: 240.6482\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 230.0546 - val_loss: 237.2904\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 228.0374 - val_loss: 234.2103\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 224.2851 - val_loss: 231.4942\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.2560 - val_loss: 228.8944\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 221.6975 - val_loss: 227.4720\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 216.6653 - val_loss: 223.8490\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 215.2160 - val_loss: 221.8455\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 212.5356 - val_loss: 218.5575\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 209.1020 - val_loss: 216.0279\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 206.5829 - val_loss: 213.1155\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 205.3254 - val_loss: 210.5001\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.1456 - val_loss: 208.1549\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.4788 - val_loss: 205.3985\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 197.3060 - val_loss: 202.9455\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.8969 - val_loss: 200.1814\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.8360 - val_loss: 197.6429\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 191.1774 - val_loss: 195.1754\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.7315 - val_loss: 192.6210\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 186.9823 - val_loss: 190.3243\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 184.1969 - val_loss: 189.3040\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.6675 - val_loss: 185.8557\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.1434 - val_loss: 184.0292\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 178.9474 - val_loss: 181.6402\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 178.3717 - val_loss: 181.1404\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.3997 - val_loss: 177.7151\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 172.5629 - val_loss: 176.8214\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.6148 - val_loss: 173.5916\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.1444 - val_loss: 171.4759\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.1242 - val_loss: 172.4693\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.3711 - val_loss: 167.6234\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.4511 - val_loss: 165.6281\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.5157 - val_loss: 164.9981\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.4709 - val_loss: 162.4624\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.1898 - val_loss: 163.6243\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.7123 - val_loss: 159.0289\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 153.8619 - val_loss: 159.6324\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.1031 - val_loss: 155.5668\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.6131 - val_loss: 154.9986\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.5744 - val_loss: 153.3268\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.0481 - val_loss: 151.4403\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.0203 - val_loss: 149.2865\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.5807 - val_loss: 149.2387\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.5490 - val_loss: 147.2357\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.7657 - val_loss: 145.4205\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.3850 - val_loss: 145.3118\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.2277 - val_loss: 143.5768\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.4949 - val_loss: 141.8339\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.8524 - val_loss: 140.8110\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.8905 - val_loss: 139.7349\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.9644 - val_loss: 139.7174\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.8500 - val_loss: 140.6228\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.5648 - val_loss: 136.5584\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.4995 - val_loss: 137.2283\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.9498 - val_loss: 135.1414\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.2933 - val_loss: 134.4610\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.3148 - val_loss: 132.9387\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.1450 - val_loss: 135.6942\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.1826 - val_loss: 133.2065\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.7749 - val_loss: 131.9814\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.6367 - val_loss: 132.2280\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.1373 - val_loss: 129.2534\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.1425 - val_loss: 128.6335\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.7526 - val_loss: 128.8138\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.5302 - val_loss: 127.3482\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.1914 - val_loss: 127.3520\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.5617 - val_loss: 127.3945\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.1738 - val_loss: 124.8841\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.7164 - val_loss: 125.0927\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.3517 - val_loss: 127.9603\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.4546 - val_loss: 123.9926\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.8701 - val_loss: 123.0261\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.4150 - val_loss: 123.3070\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.2437 - val_loss: 122.9387\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.3152 - val_loss: 123.2770\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.1123 - val_loss: 122.1410\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.4819 - val_loss: 122.1786\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.1938 - val_loss: 120.9050\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.7455 - val_loss: 121.6144\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.6179 - val_loss: 123.5066\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.4124 - val_loss: 121.8916\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.5403 - val_loss: 120.1865\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.6015 - val_loss: 118.5011\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.0436 - val_loss: 118.7094\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.9426 - val_loss: 118.3361\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.9025 - val_loss: 119.0786\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.0291 - val_loss: 117.4700\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.5686 - val_loss: 118.5994\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.0113 - val_loss: 123.5178\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.0773 - val_loss: 116.2389\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.8137 - val_loss: 117.3141\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.2757 - val_loss: 113.4913\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.0095 - val_loss: 124.1859\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.8761 - val_loss: 112.5170\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.5685 - val_loss: 111.0171\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 661us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  40 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 8725.2744 - val_loss: 530.8743\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 665.6749 - val_loss: 833.6489\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 404.9406 - val_loss: 256.5558\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 295.4775 - val_loss: 244.2528\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.7144 - val_loss: 248.7307\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3346 - val_loss: 229.4435\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.3379 - val_loss: 224.0778\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 211.5225 - val_loss: 211.4727\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 201.7197 - val_loss: 205.2072\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.9677 - val_loss: 191.4207\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.5204 - val_loss: 190.0798\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.2997 - val_loss: 178.5074\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.4332 - val_loss: 173.8080\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.6644 - val_loss: 167.1493\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.9656 - val_loss: 163.4569\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.6240 - val_loss: 156.3183\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.1185 - val_loss: 154.0502\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.6506 - val_loss: 151.1611\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.6910 - val_loss: 150.3887\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.0240 - val_loss: 142.9831\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.6546 - val_loss: 142.6940\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.2041 - val_loss: 138.6327\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.2088 - val_loss: 138.0612\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.3183 - val_loss: 132.4200\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.4868 - val_loss: 133.4546\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.1393 - val_loss: 130.5707\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.3604 - val_loss: 126.2842\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.1676 - val_loss: 130.0553\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.2561 - val_loss: 123.2932\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.1121 - val_loss: 126.0049\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.6915 - val_loss: 122.1456\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.2651 - val_loss: 122.0364\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.6514 - val_loss: 119.3997\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.4678 - val_loss: 121.8357\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.6783 - val_loss: 116.9580\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.4552 - val_loss: 121.8100\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.5158 - val_loss: 114.7136\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.4937 - val_loss: 117.7339\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.4160 - val_loss: 113.8055\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.1936 - val_loss: 115.0879\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.6204 - val_loss: 112.8358\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.8616 - val_loss: 112.0560\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.3901 - val_loss: 113.6002\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.8409 - val_loss: 111.2519\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.2503 - val_loss: 112.9740\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.5280 - val_loss: 109.7851\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.9030 - val_loss: 109.2136\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.8506 - val_loss: 117.3696\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.8170 - val_loss: 108.8360\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.0737 - val_loss: 111.2338\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.4810 - val_loss: 110.5678\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.6520 - val_loss: 109.8330\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.0903 - val_loss: 108.7347\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.0210 - val_loss: 111.2588\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.6339 - val_loss: 106.6424\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.8529 - val_loss: 107.2448\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.9595 - val_loss: 109.3426\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.9341 - val_loss: 112.7906\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.7901 - val_loss: 106.1904\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.3929 - val_loss: 115.6869\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.5189 - val_loss: 105.3795\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.5211 - val_loss: 106.5791\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.9024 - val_loss: 105.1507\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.0844 - val_loss: 114.2952\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.5751 - val_loss: 104.7267\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.1665 - val_loss: 108.5022\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.2088 - val_loss: 112.0884\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.5958 - val_loss: 107.7718\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.3267 - val_loss: 108.6205\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.3499 - val_loss: 107.7014\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.5379 - val_loss: 109.0179\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.2495 - val_loss: 105.6921\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.4603 - val_loss: 102.7227\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.9874 - val_loss: 103.6647\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 95.7208 - val_loss: 107.2250\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.2357 - val_loss: 101.9549\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.7723 - val_loss: 104.8766\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.2564 - val_loss: 102.9186\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.6641 - val_loss: 120.7913\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.4706 - val_loss: 101.2969\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.4919 - val_loss: 102.3097\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.9485 - val_loss: 101.8098\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.8409 - val_loss: 114.9860\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.7923 - val_loss: 104.9775\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.9947 - val_loss: 102.0686\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.8229 - val_loss: 101.9770\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.3343 - val_loss: 105.2163\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.3691 - val_loss: 103.7689\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.8336 - val_loss: 100.1677\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.8020 - val_loss: 100.2524\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.3178 - val_loss: 100.4136\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.2172 - val_loss: 99.4666\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.7919 - val_loss: 99.2663\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.2040 - val_loss: 100.9858\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.2397 - val_loss: 99.1482\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.4386 - val_loss: 99.0774\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.3655 - val_loss: 99.2718\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.1211 - val_loss: 99.8795\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.3088 - val_loss: 99.8208\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.2556 - val_loss: 98.8790\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 557us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  41 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 20711.2344 - val_loss: 12456.1660\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 8393.3945 - val_loss: 5430.8921\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3949.0015 - val_loss: 3213.0071\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2479.8953 - val_loss: 2211.2129\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1686.4874 - val_loss: 1671.4464\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1247.5015 - val_loss: 1339.9685\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 970.7327 - val_loss: 1061.8563\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 754.8644 - val_loss: 839.9443\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 599.2769 - val_loss: 684.5025\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 501.7105 - val_loss: 547.4277\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 402.8590 - val_loss: 439.8489\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 329.9741 - val_loss: 363.9913\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 284.6202 - val_loss: 328.4312\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 264.7716 - val_loss: 306.2372\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 256.0711 - val_loss: 291.3530\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 248.0761 - val_loss: 281.5613\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 240.2012 - val_loss: 273.5903\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.3020 - val_loss: 268.2765\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 233.0709 - val_loss: 265.5762\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 229.2922 - val_loss: 260.1722\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 228.9068 - val_loss: 260.0438\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 223.6960 - val_loss: 253.9621\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 221.3658 - val_loss: 251.4055\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.9034 - val_loss: 248.3900\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 216.1656 - val_loss: 245.6679\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 213.3812 - val_loss: 243.3185\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 211.0251 - val_loss: 240.5618\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 209.9933 - val_loss: 237.7442\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 207.0090 - val_loss: 235.3394\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 204.5211 - val_loss: 232.5617\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.0730 - val_loss: 230.3582\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.8186 - val_loss: 228.3023\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.0060 - val_loss: 225.7930\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 195.1175 - val_loss: 222.5294\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.5193 - val_loss: 220.1124\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 190.2996 - val_loss: 219.4108\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 188.9777 - val_loss: 215.7947\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 186.8398 - val_loss: 212.8691\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 183.5768 - val_loss: 210.4840\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.6889 - val_loss: 207.9049\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 178.0157 - val_loss: 204.8862\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.8161 - val_loss: 202.5023\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.2015 - val_loss: 200.5395\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.7392 - val_loss: 197.0161\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.4902 - val_loss: 196.8640\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.9520 - val_loss: 192.3817\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.5831 - val_loss: 190.8460\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.9700 - val_loss: 187.3595\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.6652 - val_loss: 184.7612\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.9640 - val_loss: 181.9961\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.2682 - val_loss: 179.7382\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.9197 - val_loss: 179.1997\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.8153 - val_loss: 174.6804\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.0854 - val_loss: 172.6184\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.2447 - val_loss: 175.5486\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.8125 - val_loss: 166.2729\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.0895 - val_loss: 164.1831\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.9308 - val_loss: 164.4010\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.4813 - val_loss: 160.4712\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.0085 - val_loss: 159.9041\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.3930 - val_loss: 155.7704\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.4350 - val_loss: 153.1567\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.4316 - val_loss: 151.3466\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.9890 - val_loss: 151.3539\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.4407 - val_loss: 147.5061\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.1714 - val_loss: 147.5768\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.5141 - val_loss: 146.8026\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.4555 - val_loss: 143.1172\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.8873 - val_loss: 142.0874\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.9450 - val_loss: 140.4156\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.5945 - val_loss: 138.7986\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.0340 - val_loss: 138.9331\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.8419 - val_loss: 135.2633\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.6523 - val_loss: 138.7758\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.9193 - val_loss: 133.9194\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.4733 - val_loss: 132.9884\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.2512 - val_loss: 131.9014\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.9957 - val_loss: 131.4662\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.3316 - val_loss: 130.8405\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.5157 - val_loss: 130.5537\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.5705 - val_loss: 130.9712\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.1349 - val_loss: 131.5638\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.3102 - val_loss: 126.8298\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.3853 - val_loss: 126.1438\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.6856 - val_loss: 126.1546\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.2547 - val_loss: 131.4948\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.4061 - val_loss: 125.4826\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.8118 - val_loss: 124.5808\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.4788 - val_loss: 124.7145\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.4548 - val_loss: 122.8405\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.2996 - val_loss: 122.9395\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.0452 - val_loss: 122.9906\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.2446 - val_loss: 123.8018\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.8763 - val_loss: 123.0251\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.4113 - val_loss: 121.2197\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.1256 - val_loss: 122.5372\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.8208 - val_loss: 121.4400\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.3849 - val_loss: 121.2305\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.1343 - val_loss: 126.8392\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 100.3283 - val_loss: 121.6307\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 662us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  42 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 165516.2812 - val_loss: 109683.4375\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77629.8984 - val_loss: 45596.7969\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 30592.5684 - val_loss: 15478.9297\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 9918.3691 - val_loss: 4721.1919\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3075.1018 - val_loss: 1487.5856\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1016.4733 - val_loss: 505.2062\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 403.2317 - val_loss: 252.2191\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.2778 - val_loss: 196.5059\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 200.9130 - val_loss: 178.2931\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 184.4350 - val_loss: 166.7307\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.0174 - val_loss: 160.1813\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 166.7514 - val_loss: 154.9059\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.7684 - val_loss: 149.6282\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.0317 - val_loss: 145.8060\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.9959 - val_loss: 143.0932\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.1582 - val_loss: 140.8611\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.4154 - val_loss: 139.2578\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.3227 - val_loss: 137.6945\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.7672 - val_loss: 136.5083\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.0199 - val_loss: 135.4673\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.3413 - val_loss: 134.6691\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.4953 - val_loss: 134.0332\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.8135 - val_loss: 132.8723\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.1192 - val_loss: 132.0827\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.6443 - val_loss: 131.6334\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.8209 - val_loss: 130.7107\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.1132 - val_loss: 129.7588\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.5133 - val_loss: 129.0942\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.1391 - val_loss: 128.2207\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.3974 - val_loss: 127.6047\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.7217 - val_loss: 126.7188\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.0632 - val_loss: 126.1673\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.3917 - val_loss: 125.3441\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.8776 - val_loss: 124.3661\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.1866 - val_loss: 123.8560\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.3899 - val_loss: 123.1921\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.7742 - val_loss: 122.5098\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.1857 - val_loss: 121.8092\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.4814 - val_loss: 121.1854\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.9776 - val_loss: 120.5978\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.8688 - val_loss: 119.9990\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.6486 - val_loss: 119.9044\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.5317 - val_loss: 119.0394\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.8827 - val_loss: 118.5193\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.2753 - val_loss: 118.0700\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.8774 - val_loss: 117.3755\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.5133 - val_loss: 117.0050\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.9721 - val_loss: 116.6755\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.9100 - val_loss: 116.1501\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.9176 - val_loss: 115.8168\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.5031 - val_loss: 115.2066\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.9468 - val_loss: 114.6072\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.8778 - val_loss: 114.4107\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.0189 - val_loss: 113.7295\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.6365 - val_loss: 113.2165\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.1828 - val_loss: 112.9665\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.2717 - val_loss: 112.3115\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.6188 - val_loss: 111.6430\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.4404 - val_loss: 111.0346\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.7168 - val_loss: 110.5974\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 125.1440 - val_loss: 110.0573\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.4607 - val_loss: 109.9950\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.1696 - val_loss: 109.1440\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.7124 - val_loss: 108.7374\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.0764 - val_loss: 108.3303\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.9653 - val_loss: 108.1062\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.2266 - val_loss: 107.3256\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.5321 - val_loss: 106.7120\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.1194 - val_loss: 106.1503\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.9057 - val_loss: 105.7353\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.6916 - val_loss: 104.9513\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.3160 - val_loss: 104.1510\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.4275 - val_loss: 104.1598\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.9638 - val_loss: 103.1292\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.7609 - val_loss: 102.8289\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.8430 - val_loss: 101.7045\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.9464 - val_loss: 101.2618\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.5611 - val_loss: 101.1459\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.0378 - val_loss: 100.2339\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.2902 - val_loss: 100.0402\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.8954 - val_loss: 98.5402\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.5637 - val_loss: 98.3233\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.5062 - val_loss: 97.1720\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.6629 - val_loss: 97.0226\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.2297 - val_loss: 96.2279\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.9134 - val_loss: 95.7136\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.1152 - val_loss: 95.6480\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.3803 - val_loss: 94.8738\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.7081 - val_loss: 94.4610\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.2673 - val_loss: 93.8021\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.8048 - val_loss: 93.8849\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.1476 - val_loss: 93.0691\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.8083 - val_loss: 92.7658\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.1184 - val_loss: 92.2674\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.7364 - val_loss: 91.8021\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.7053 - val_loss: 91.2999\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.5300 - val_loss: 90.9683\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.7970 - val_loss: 90.3764\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.1990 - val_loss: 89.9705\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.4360 - val_loss: 89.4204\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  43 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 21437.9492 - val_loss: 15775.1348\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 12555.7041 - val_loss: 8958.8193\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 6798.6455 - val_loss: 4642.4150\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3211.7996 - val_loss: 2324.7000\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1603.1559 - val_loss: 1414.9514\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1085.0338 - val_loss: 1168.9806\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 958.3541 - val_loss: 1049.1921\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 872.9843 - val_loss: 941.7901\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 791.5211 - val_loss: 838.2421\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 713.8567 - val_loss: 745.0310\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 648.5001 - val_loss: 666.6533\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 593.0039 - val_loss: 606.3164\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 551.2823 - val_loss: 551.4713\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 512.8904 - val_loss: 507.6520\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 477.9183 - val_loss: 466.9901\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 447.6695 - val_loss: 430.0874\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 419.9146 - val_loss: 397.4534\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 395.8298 - val_loss: 375.5994\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 378.7094 - val_loss: 358.9854\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 363.5381 - val_loss: 341.4232\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 351.5929 - val_loss: 329.3059\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 341.9555 - val_loss: 320.0645\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 332.9674 - val_loss: 306.8519\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 323.2025 - val_loss: 301.4580\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 316.7348 - val_loss: 292.0552\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 309.0034 - val_loss: 283.9778\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 303.1139 - val_loss: 277.8836\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 297.4871 - val_loss: 272.0059\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 292.8251 - val_loss: 265.3457\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 288.2388 - val_loss: 262.7533\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 283.5057 - val_loss: 255.5279\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 279.7781 - val_loss: 252.7005\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 276.5914 - val_loss: 248.8994\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 273.3781 - val_loss: 244.4099\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 269.1754 - val_loss: 239.0169\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 266.1732 - val_loss: 237.3314\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 263.2987 - val_loss: 237.1853\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 260.5844 - val_loss: 232.8047\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 258.9177 - val_loss: 227.9942\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 254.8710 - val_loss: 227.8041\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 252.7516 - val_loss: 225.9239\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 250.6414 - val_loss: 223.0959\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 248.2995 - val_loss: 219.8938\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.8632 - val_loss: 218.6332\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.9538 - val_loss: 216.0014\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 242.0199 - val_loss: 215.6512\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 240.0625 - val_loss: 211.3287\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.7188 - val_loss: 209.0656\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.6236 - val_loss: 209.3476\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 233.9660 - val_loss: 205.1805\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 232.9141 - val_loss: 203.9885\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 230.4587 - val_loss: 206.3454\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 229.2560 - val_loss: 200.1450\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 227.2100 - val_loss: 200.0846\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 224.3552 - val_loss: 196.0269\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 221.8308 - val_loss: 195.6469\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 220.3896 - val_loss: 196.3460\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 216.9930 - val_loss: 189.7895\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 215.3195 - val_loss: 189.5236\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 213.4843 - val_loss: 187.3727\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 210.8752 - val_loss: 186.4605\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 210.8838 - val_loss: 182.2303\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 206.3524 - val_loss: 183.0428\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.8483 - val_loss: 180.0833\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 201.3125 - val_loss: 177.7384\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.8907 - val_loss: 175.8314\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.2792 - val_loss: 174.2638\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.9418 - val_loss: 172.8663\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 191.6008 - val_loss: 169.6964\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 188.5227 - val_loss: 168.5164\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 186.4005 - val_loss: 165.1822\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 183.6055 - val_loss: 164.3250\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.7383 - val_loss: 161.7688\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 178.8018 - val_loss: 159.0657\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.3785 - val_loss: 157.7603\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.6775 - val_loss: 155.8103\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 171.3699 - val_loss: 153.8710\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.9564 - val_loss: 151.1768\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 166.6814 - val_loss: 149.1553\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.8346 - val_loss: 148.6003\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.5541 - val_loss: 144.4208\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.7867 - val_loss: 144.2721\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.2916 - val_loss: 141.1704\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 155.0854 - val_loss: 141.1817\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.4125 - val_loss: 137.6196\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.5895 - val_loss: 139.1008\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.7688 - val_loss: 134.0862\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.5013 - val_loss: 133.7131\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.8885 - val_loss: 131.8295\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.5479 - val_loss: 130.4117\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.6846 - val_loss: 128.3840\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.8883 - val_loss: 127.0202\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.3736 - val_loss: 125.1583\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.3210 - val_loss: 123.4524\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.6246 - val_loss: 122.1805\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.0220 - val_loss: 120.8623\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.1726 - val_loss: 119.0711\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.0688 - val_loss: 118.0352\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.2136 - val_loss: 117.7178\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.5636 - val_loss: 114.8267\n",
      "\n",
      "\n",
      " 1/10 [==>...........................] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "# Create the for loop to split the data, create, compile & fit model, evaluate & nake predictions, caluclate mse and store\n",
    "# in list_of_mse\n",
    "\n",
    "start_time = datetime.now() # Starting time of the for loop execution\n",
    "\n",
    "for i in range(50) :\n",
    "    # Split the data into train and test set\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\n",
    "    \n",
    "    # Create and compile the regression model using the function regression_model as defined in TASK 1\n",
    "    model = regression_model()\n",
    "\n",
    "    # Fit the model on the train set\n",
    "    print('\\n\\n\\nTraining Model # ' , i+1 , '\\n\\n') # Print the Model Number that is being trained\n",
    "    model.fit(X_train, Y_train, validation_split=0.3, epochs=100)\n",
    "    print('\\n')\n",
    "    \n",
    "    # Make prediction on the test set\n",
    "    Y_predicted = model.predict(X_test)\n",
    "    \n",
    "    # Calculate the mean square error\n",
    "    mse = mean_squared_error(Y_test, Y_predicted)\n",
    "    \n",
    "    # Add the mse to the list_of_mse list\n",
    "    list_of_mse.append(mse)\n",
    "\n",
    "end_time = datetime.now() # Ending time of the for loop execution\n",
    "\n",
    "# Print time taken for fitting 50 models and calucating the Mean and Standard Deviation of MSE of 50 models\n",
    "print('\\n\\nTotal Execution Time : ' , format(end_time - start_time))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Mean of the MSE of 50 Models :  148.0049474659527\n",
      "Standard Deviation of MSE of 50 Models :  213.9519003781904\n"
     ]
    }
   ],
   "source": [
    "# Calculate the Mean of the MSE of 50 models\n",
    "mean_of_mse = stats.mean(list_of_mse)\n",
    "\n",
    "# Calculate the Standard Deviation of the MSE of 50 models\n",
    "std_of_mse = stats.stdev(list_of_mse)\n",
    "\n",
    "# Print the Mean and Standard Deviation of MSE of 50 models\n",
    "print('\\n\\nMean of the MSE of 50 Models : ' , str(mean_of_mse))\n",
    "print('Standard Deviation of MSE of 50 Models : ' , str(std_of_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = green> Comparision of Mean of MSE with Mean of MSE from PART B </font>\n",
    "<table style=\"width:20%\">\n",
    "  <tr>\n",
    "    <th>Mean of MSE of PART A</th>\n",
    "    <th>Mean of MSE of PART B</th>\n",
    "    <th>Mean of MSE of PART C</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>244.77</td>\n",
    "    <td>126.13</td>\n",
    "    <td>149.31</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "The table above compares the Mean of **MSE for PART A**, **Mean of MSE for PART B** and **Mean of MSE for PART C**. As can be seen, the value of Mean of MSE of PART C is marginally larger than that of PART B. This shows that the effect of **normalizing the features** as well as **increasing the number of epochs by 2** did not improve the performance of the regression model and helps it in finding the line of best fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = blue> END OF PART C </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = blue> PART D : BASELINE MODEL WITH INCREASED HIDDEN LAYERS </font>\n",
    "\n",
    "\n",
    "In this part, all the tasks from <b>PART B</b> are performed, but this time the number of hidden layers are increased to 3\n",
    "\n",
    "<b>The new model will have : </b>\n",
    "<ul>\n",
    "        <li> Input layer with 10 nodes </li>\n",
    "        <li> 3 hidden layers, each with 10 nodes and ReLU activation function </li>\n",
    "        <li> Adam optimizer and mean squared error loss function </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = darkorange>Task 1 : Train and Test the Baseline Model with 100 epochs and Increased Hidden Layers</font>\n",
    "\n",
    "In order to train and test the the baseline model with normalized features, 100 epochs and increased hidden layers, the following steps are performed :\n",
    "<ol>\n",
    "    <li>Normalize the features (X)</li>\n",
    "    <li>Randomly split the data into <i>X_train, X_test, Y_train, Y_test</i> sets</li>\n",
    "    <li>Create a new model with 100 epochs</li>\n",
    "    <li>Train the model using <i>X_train, Y_train</i> using <b>50</b> epochs</li>\n",
    "    <li>Evaluate the model using <i>X_test, Y_test</i></li>\n",
    "    <li>Get the predictions on the <i>X_test</i> set </li>\n",
    "    <li>Compute the <b>mean squared error</b> on the test set using sklearn</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #2980B9> Step 1 : Normalize the features (X) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note</b> : As the features (X) have already been normalized the features (X), hence this part is skipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #2980B9> Step 2 : Randomly split the data into <i>X_train, X_test, Y_train, Y_test </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X_train, X_test, Y_train and Y_test sets\n",
    "X_train, X_test, Y_train, Y_test = data_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #2980B9> Step 3 : Create a new regression model with 3 hidden layers, each with 10 nodes and ReLU activation  </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_layer_regression_model () :\n",
    "    \n",
    "    # Create the model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu', input_shape=(X.shape[1],)))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #2980B9> Step 4 : Train the model using <i>X_train, Y_train</i> using 50 epochs </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note </b> : Since the regression model has already been created using `def regression_model` in <b>TASK 1</b> and there are no changes being made to it, hence there is no need to write the code for the model again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 739.3081 - val_loss: 531.6807\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 356.2697 - val_loss: 315.1024\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 298.0998 - val_loss: 294.3824\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 273.7672 - val_loss: 284.2554\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 268.1984 - val_loss: 274.6451\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 264.1694 - val_loss: 272.1266\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 260.2380 - val_loss: 271.6527\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 257.1516 - val_loss: 269.2653\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 253.8722 - val_loss: 264.5289\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 250.9303 - val_loss: 261.8325\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 247.4613 - val_loss: 258.8852\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.4897 - val_loss: 258.9001\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.1171 - val_loss: 252.2506\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.6837 - val_loss: 254.3569\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.8130 - val_loss: 252.7958\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 233.7033 - val_loss: 244.7933\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 231.2562 - val_loss: 245.9434\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 228.3214 - val_loss: 238.2009\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 225.4963 - val_loss: 238.1073\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.6133 - val_loss: 226.8092\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.4443 - val_loss: 218.0430\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 209.1448 - val_loss: 213.7398\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.0542 - val_loss: 202.9538\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.8745 - val_loss: 199.3914\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 188.8596 - val_loss: 195.9150\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 183.6554 - val_loss: 188.9131\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 178.3055 - val_loss: 182.0439\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 172.2313 - val_loss: 179.1601\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.6690 - val_loss: 175.7469\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.5948 - val_loss: 170.2707\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.6270 - val_loss: 168.2709\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.2302 - val_loss: 163.1810\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.0851 - val_loss: 162.2902\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.7511 - val_loss: 158.7514\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.0322 - val_loss: 154.2554\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.0909 - val_loss: 156.4174\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.4256 - val_loss: 147.9788\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.6314 - val_loss: 149.8876\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.8793 - val_loss: 143.9652\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.0501 - val_loss: 137.8678\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.4034 - val_loss: 135.7301\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.0215 - val_loss: 135.5213\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.3564 - val_loss: 128.7152\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.5685 - val_loss: 128.2838\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.1280 - val_loss: 123.0428\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.4881 - val_loss: 121.2737\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.1047 - val_loss: 117.5264\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.2748 - val_loss: 117.8377\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.2182 - val_loss: 113.7778\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.2383 - val_loss: 122.2823\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.9669 - val_loss: 111.1493\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.7765 - val_loss: 112.6022\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.2272 - val_loss: 111.1906\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.5971 - val_loss: 112.6094\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.4193 - val_loss: 108.6820\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.8263 - val_loss: 105.6454\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.4154 - val_loss: 110.4552\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.2666 - val_loss: 106.0420\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.1136 - val_loss: 103.5001\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.8684 - val_loss: 102.4339\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.1447 - val_loss: 104.3548\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.9462 - val_loss: 105.7261\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.6787 - val_loss: 99.8635\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.6971 - val_loss: 104.5228\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.0616 - val_loss: 100.9521\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.0292 - val_loss: 99.6249\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.2324 - val_loss: 98.1937\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.8039 - val_loss: 98.9906\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.7627 - val_loss: 98.2315\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.9042 - val_loss: 101.1477\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.6967 - val_loss: 94.7312\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.4507 - val_loss: 94.1672\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.4019 - val_loss: 96.5761\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.6107 - val_loss: 94.7064\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.0992 - val_loss: 92.3398\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 86.7504 - val_loss: 93.9624\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.7473 - val_loss: 94.5787\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.4572 - val_loss: 90.7396\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.9053 - val_loss: 98.2766\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.3323 - val_loss: 91.3332\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.4329 - val_loss: 89.8313\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.2249 - val_loss: 92.1775\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.4292 - val_loss: 89.1046\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.9638 - val_loss: 88.5417\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.5093 - val_loss: 90.0149\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.4630 - val_loss: 91.0120\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.8144 - val_loss: 87.0465\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.9074 - val_loss: 89.1448\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.3987 - val_loss: 88.4326\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.6125 - val_loss: 87.0694\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.4604 - val_loss: 86.9086\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.1869 - val_loss: 88.1003\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.3931 - val_loss: 85.6581\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.4818 - val_loss: 84.7926\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.5025 - val_loss: 88.1400\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.9909 - val_loss: 90.8836\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.0820 - val_loss: 85.1539\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.9031 - val_loss: 84.4268\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.1131 - val_loss: 83.1804\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.4108 - val_loss: 85.1966\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f8b3f2f700>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = regression_model()\n",
    "\n",
    "# Fit the model on the train set\n",
    "model.fit(X_train, Y_train, validation_split=0.3, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #2980B9> Step 5 : Get the predictions on the X_test </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 668us/step\n"
     ]
    }
   ],
   "source": [
    "# Store the predictions in a variable Y_Predicted\n",
    "Y_predicted = predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #2980B9> Step 6 : Compute the <i>mean squared error</i> on the test set using sklearn </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error (MSE) of the Baseline Model with Normalized Features is :  77.07785350144503\n"
     ]
    }
   ],
   "source": [
    "# Calculate the mean square error\n",
    "\n",
    "mse = calculate_mse()\n",
    "print('Mean Square Error (MSE) of the Baseline Model with Normalized Features is : ' , str(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = darkorange>Task 2 : Create 50 Models and Calculate the <i>µ</i> & <i>σ</i> of their MSE Errors with new Features</font>\n",
    "\n",
    "In order to train 50 models with new features (normalized features) and calulate the mean (µ) and standard deviation (σ) of their mean square errors (MSE) the following steps are performed :\n",
    "<ol>\n",
    "    <li>Create an empty list <code>list_of_mse</code> to store the mean square error of each of the models</li>\n",
    "    <li>Define a for loop and perform each of the following steps in the loop</li>\n",
    "        <ol>\n",
    "        <li>Randomly split the data into <i>X_train, X_test, Y_train, Y_test</i> sets</li>\n",
    "        <li>Train the model using <i>X_train, Y_train</i> using <b>50</b> epochs</li>\n",
    "        <li>Evaluate the model using <i>X_test, Y_test</i></li>\n",
    "        <li>Get the predictions on the <i>X_test</i> set </li>\n",
    "        <li>Compute the <b>mean squared error</b> on the test set using sklearn</li>\n",
    "    </ol>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #2980B9> Step 1 : Creating the <i>list_of_mse</i> list</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the empty lists\n",
    "list_of_mse = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #2980B9> Step 2 : Creating 50 Models, Calculating The MSE for Each and <i>µ</i> & <i>σ</i> of the 50 MSE Values in <i>list_of_mse</i> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Training Model #  1 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 113153.5938 - val_loss: 59866.4141\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 35118.3672 - val_loss: 13645.3457\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 6410.4316 - val_loss: 2335.2993\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2492.2041 - val_loss: 2245.2185\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1901.4836 - val_loss: 1545.0839\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1502.3938 - val_loss: 1309.0674\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1302.4243 - val_loss: 1137.8712\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1160.2209 - val_loss: 1037.5649\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1074.4115 - val_loss: 953.3148\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1006.2141 - val_loss: 893.6822\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 952.3450 - val_loss: 849.5766\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 913.0290 - val_loss: 810.9116\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 873.5685 - val_loss: 773.1067\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 840.1495 - val_loss: 745.9739\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 812.4035 - val_loss: 719.4403\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 790.0089 - val_loss: 699.0107\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 763.3953 - val_loss: 677.0353\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 742.2905 - val_loss: 659.8373\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 721.7366 - val_loss: 643.4195\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 702.0588 - val_loss: 626.8793\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 686.5067 - val_loss: 612.3100\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 667.2951 - val_loss: 599.0441\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 652.0414 - val_loss: 585.8904\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 635.9749 - val_loss: 570.1946\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 621.9527 - val_loss: 557.1367\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 608.4130 - val_loss: 544.3337\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 595.9969 - val_loss: 532.0801\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 580.1034 - val_loss: 522.6867\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 568.6342 - val_loss: 506.8647\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 552.9251 - val_loss: 496.0014\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 540.2748 - val_loss: 485.7293\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 527.1321 - val_loss: 472.1161\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 512.8781 - val_loss: 460.8279\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 500.8041 - val_loss: 450.4339\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 486.9663 - val_loss: 436.7671\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 473.2918 - val_loss: 425.8892\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 460.5022 - val_loss: 415.4984\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 446.2711 - val_loss: 402.4028\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 432.7144 - val_loss: 390.2288\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 417.3010 - val_loss: 378.4037\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 401.9570 - val_loss: 363.1422\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 382.9220 - val_loss: 348.7086\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 364.5636 - val_loss: 332.3766\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 346.3656 - val_loss: 318.6080\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 331.0133 - val_loss: 305.1921\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 313.6321 - val_loss: 293.5588\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 298.1382 - val_loss: 281.8691\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 281.9087 - val_loss: 269.3100\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 268.3385 - val_loss: 258.5996\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.7195 - val_loss: 247.7187\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 242.3361 - val_loss: 237.8713\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 231.1071 - val_loss: 229.0005\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 221.6576 - val_loss: 220.8881\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 211.2364 - val_loss: 212.6094\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 201.9805 - val_loss: 205.0300\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.4328 - val_loss: 196.8365\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.2362 - val_loss: 191.0563\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.8061 - val_loss: 183.8626\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.3653 - val_loss: 177.3198\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.6238 - val_loss: 171.5678\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.3910 - val_loss: 167.6967\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.0535 - val_loss: 161.4609\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.5573 - val_loss: 158.2173\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.7884 - val_loss: 153.2514\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.7885 - val_loss: 149.1105\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.1703 - val_loss: 146.5354\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.6489 - val_loss: 142.1170\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.3572 - val_loss: 141.1048\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.6346 - val_loss: 139.1198\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.9821 - val_loss: 134.1789\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.1269 - val_loss: 132.0961\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.2432 - val_loss: 129.0076\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.9650 - val_loss: 127.1101\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.0342 - val_loss: 125.4196\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.3817 - val_loss: 127.5475\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 110.4963 - val_loss: 121.2336\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.9731 - val_loss: 119.9815\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.0123 - val_loss: 118.0708\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.5892 - val_loss: 116.8254\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.6103 - val_loss: 115.0788\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.3465 - val_loss: 113.9002\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.0418 - val_loss: 114.2172\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.1297 - val_loss: 111.4426\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.0199 - val_loss: 110.2512\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.2816 - val_loss: 109.3841\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.9345 - val_loss: 108.5219\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.8499 - val_loss: 107.5253\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.8748 - val_loss: 109.7833\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.8317 - val_loss: 105.1441\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.3167 - val_loss: 104.8252\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.2134 - val_loss: 104.7078\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.8561 - val_loss: 102.4716\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.9631 - val_loss: 102.7661\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.5045 - val_loss: 106.1064\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.1782 - val_loss: 101.0757\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.3375 - val_loss: 100.5074\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.7532 - val_loss: 99.6018\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.2635 - val_loss: 99.0480\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.1374 - val_loss: 98.6439\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.2186 - val_loss: 98.5984\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 555us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  2 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 695.5012 - val_loss: 397.0418\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 397.1376 - val_loss: 345.8521\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 364.4004 - val_loss: 326.8968\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 345.5685 - val_loss: 312.6162\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 330.7766 - val_loss: 301.4528\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 319.4736 - val_loss: 292.2097\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 308.3989 - val_loss: 283.0456\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 298.5491 - val_loss: 276.1115\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 291.8805 - val_loss: 268.1885\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 284.8784 - val_loss: 262.3510\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 279.6585 - val_loss: 258.1718\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 274.7004 - val_loss: 253.0517\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 273.0328 - val_loss: 248.6915\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 266.6452 - val_loss: 246.7608\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 263.1435 - val_loss: 241.1948\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 260.1141 - val_loss: 238.3108\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.6278 - val_loss: 234.8057\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 252.6085 - val_loss: 231.8060\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 249.1912 - val_loss: 228.9905\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 247.1888 - val_loss: 225.9407\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.7113 - val_loss: 223.1172\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.0329 - val_loss: 221.1642\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.4762 - val_loss: 217.3903\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 233.8075 - val_loss: 215.0797\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 231.9679 - val_loss: 212.7682\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 228.1174 - val_loss: 209.0606\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 224.2296 - val_loss: 207.0215\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.2194 - val_loss: 203.6269\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 218.6837 - val_loss: 199.8940\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.3015 - val_loss: 196.3059\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 212.7619 - val_loss: 194.4827\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.3832 - val_loss: 189.4384\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 206.1498 - val_loss: 185.6393\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.8467 - val_loss: 181.4692\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.6185 - val_loss: 179.0868\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.1234 - val_loss: 173.7930\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 188.4066 - val_loss: 170.4579\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 183.5028 - val_loss: 165.9430\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 178.6216 - val_loss: 162.2309\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 175.5844 - val_loss: 158.0524\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.9397 - val_loss: 154.0607\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.8271 - val_loss: 149.9543\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.4984 - val_loss: 146.5829\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.3090 - val_loss: 142.1290\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.8542 - val_loss: 138.7115\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.1129 - val_loss: 135.9783\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.7367 - val_loss: 132.3651\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.1609 - val_loss: 129.3665\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.2232 - val_loss: 130.2496\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.3875 - val_loss: 124.1285\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.4178 - val_loss: 123.8959\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.4927 - val_loss: 119.6633\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.1817 - val_loss: 117.9450\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.1393 - val_loss: 116.7291\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 123.4335 - val_loss: 116.2258\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.9667 - val_loss: 113.8568\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.2192 - val_loss: 113.4309\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.9605 - val_loss: 111.7203\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.8018 - val_loss: 111.2709\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.5633 - val_loss: 110.8185\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.2983 - val_loss: 109.7747\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.3226 - val_loss: 109.3101\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.4509 - val_loss: 109.8778\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.3714 - val_loss: 109.1429\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.2283 - val_loss: 108.4335\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.4524 - val_loss: 108.1276\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.4407 - val_loss: 108.3637\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.4878 - val_loss: 110.0724\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.9565 - val_loss: 112.7891\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.6337 - val_loss: 107.3185\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.8472 - val_loss: 110.2373\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.8381 - val_loss: 111.2226\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.3911 - val_loss: 107.4325\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.5704 - val_loss: 106.7124\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.6415 - val_loss: 106.7657\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.0768 - val_loss: 106.5055\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.3539 - val_loss: 106.1919\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.7394 - val_loss: 108.7492\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.4827 - val_loss: 106.0542\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.4792 - val_loss: 105.7059\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.4431 - val_loss: 106.2602\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.6834 - val_loss: 108.3391\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.9258 - val_loss: 105.9753\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.6154 - val_loss: 105.2565\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.8080 - val_loss: 104.7613\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.7132 - val_loss: 105.5702\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.8874 - val_loss: 107.2385\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.1496 - val_loss: 107.1662\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.2388 - val_loss: 106.5166\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.8193 - val_loss: 105.8934\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.2240 - val_loss: 103.9648\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.6242 - val_loss: 105.5679\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.9100 - val_loss: 105.4776\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.4182 - val_loss: 103.8270\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.4424 - val_loss: 104.3213\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.3828 - val_loss: 105.4095\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.1737 - val_loss: 103.7046\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.8062 - val_loss: 103.3509\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.3082 - val_loss: 104.9237\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.1686 - val_loss: 103.3478\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  3 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4973.7661 - val_loss: 4438.2300\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3463.3745 - val_loss: 3467.2625\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2609.8994 - val_loss: 2651.1946\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2101.7031 - val_loss: 2071.1204\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1693.3018 - val_loss: 1712.8351\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1422.9752 - val_loss: 1434.5127\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1201.9224 - val_loss: 1200.9194\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1035.1136 - val_loss: 1032.3828\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 901.1937 - val_loss: 881.8083\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 784.4394 - val_loss: 806.2307\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 714.8729 - val_loss: 662.6212\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 621.4989 - val_loss: 657.9006\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 557.0215 - val_loss: 513.5609\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 492.7731 - val_loss: 462.8524\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 440.0135 - val_loss: 401.1777\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 402.8464 - val_loss: 356.2387\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 355.2796 - val_loss: 327.4739\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 322.1057 - val_loss: 286.1566\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 289.9300 - val_loss: 266.1408\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 274.2820 - val_loss: 230.6942\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 258.8545 - val_loss: 210.2097\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.4057 - val_loss: 193.0890\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.0604 - val_loss: 185.3657\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 195.7988 - val_loss: 165.3714\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.2977 - val_loss: 157.1316\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.5609 - val_loss: 148.9783\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.1768 - val_loss: 145.2481\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.8779 - val_loss: 143.6535\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.0149 - val_loss: 142.9954\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.5585 - val_loss: 130.0649\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.6188 - val_loss: 128.3566\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.0980 - val_loss: 127.9489\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.8556 - val_loss: 121.2997\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.9820 - val_loss: 121.2442\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.7806 - val_loss: 119.0750\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.2497 - val_loss: 174.9496\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.0397 - val_loss: 129.6913\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.3878 - val_loss: 111.9489\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.3869 - val_loss: 113.2692\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.1127 - val_loss: 109.0268\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.1269 - val_loss: 108.4838\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.8809 - val_loss: 107.1007\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.2159 - val_loss: 105.0593\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.3195 - val_loss: 106.0382\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.6823 - val_loss: 102.7032\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.3374 - val_loss: 102.2896\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.9406 - val_loss: 101.0309\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.5222 - val_loss: 122.7853\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.8814 - val_loss: 99.9729\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.1615 - val_loss: 108.5294\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.3029 - val_loss: 123.5553\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.3380 - val_loss: 116.1188\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.7770 - val_loss: 96.6681\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.3692 - val_loss: 97.8465\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.7580 - val_loss: 95.2718\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.1139 - val_loss: 93.1361\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.0656 - val_loss: 99.6137\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.8612 - val_loss: 93.9877\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.4113 - val_loss: 93.6902\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.8241 - val_loss: 89.8035\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.2761 - val_loss: 92.2688\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.5917 - val_loss: 91.5202\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.1837 - val_loss: 86.7442\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.7670 - val_loss: 93.3987\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.6772 - val_loss: 85.4517\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.1763 - val_loss: 86.6758\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.2714 - val_loss: 83.7792\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.9553 - val_loss: 84.6129\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.1657 - val_loss: 82.4208\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.1585 - val_loss: 83.0729\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.1360 - val_loss: 81.3548\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.6140 - val_loss: 81.0677\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.5896 - val_loss: 80.3777\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.8632 - val_loss: 79.1758\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.1033 - val_loss: 88.3635\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.9962 - val_loss: 78.6347\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.4730 - val_loss: 76.8949\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.2489 - val_loss: 78.3183\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.6809 - val_loss: 76.3095\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.8582 - val_loss: 78.9110\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.1579 - val_loss: 74.9861\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.6555 - val_loss: 78.6080\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.9253 - val_loss: 94.2936\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.1806 - val_loss: 75.8036\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.8902 - val_loss: 72.3371\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.5714 - val_loss: 72.5862\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.4904 - val_loss: 89.3659\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.0752 - val_loss: 71.3808\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.5138 - val_loss: 72.5936\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.8688 - val_loss: 71.3420\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.9086 - val_loss: 71.0488\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 75.6164 - val_loss: 79.9455\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.0391 - val_loss: 67.3297\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.9844 - val_loss: 67.0735\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 73.5271 - val_loss: 67.0531\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 71.7526 - val_loss: 66.7922\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.5404 - val_loss: 68.1128\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 74.9336 - val_loss: 66.2259\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.5805 - val_loss: 68.8323\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 71.5576 - val_loss: 71.1639\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 557us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  4 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 327272.0312 - val_loss: 278450.9062\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 242105.2188 - val_loss: 207905.0312\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 182881.6562 - val_loss: 159857.1094\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142242.9219 - val_loss: 126173.0234\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113367.9922 - val_loss: 101817.9922\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92290.5703 - val_loss: 83675.3359\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76416.7891 - val_loss: 69813.1953\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 64162.7344 - val_loss: 59025.6758\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 54537.4609 - val_loss: 50456.9570\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 46846.7227 - val_loss: 43518.4180\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 40586.6914 - val_loss: 37805.2266\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 35334.1406 - val_loss: 32908.1992\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 30481.6777 - val_loss: 27807.2910\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 24553.8633 - val_loss: 21070.2441\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 18175.1367 - val_loss: 15281.2539\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 13317.8535 - val_loss: 11114.4883\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 9824.9160 - val_loss: 8151.4673\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 7311.6655 - val_loss: 6063.6768\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5506.0435 - val_loss: 4543.7607\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4185.7402 - val_loss: 3417.5508\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3196.6306 - val_loss: 2580.6382\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2450.4600 - val_loss: 1967.0947\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1895.7490 - val_loss: 1511.8616\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1484.5829 - val_loss: 1174.7202\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1178.1768 - val_loss: 935.1420\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 957.6495 - val_loss: 769.2177\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 801.7432 - val_loss: 655.8404\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 693.1107 - val_loss: 581.5107\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 619.8832 - val_loss: 533.9753\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 569.8057 - val_loss: 506.5193\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 538.7386 - val_loss: 490.2332\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 518.7867 - val_loss: 481.5443\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 504.3678 - val_loss: 477.8765\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 496.6128 - val_loss: 476.1288\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 490.9070 - val_loss: 475.6894\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 487.1857 - val_loss: 475.8541\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 484.7127 - val_loss: 476.1956\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 482.7636 - val_loss: 476.4562\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 481.1470 - val_loss: 476.5968\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 479.6580 - val_loss: 476.0197\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 478.3856 - val_loss: 475.6442\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 477.1668 - val_loss: 475.4243\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 476.1051 - val_loss: 474.8417\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 474.8857 - val_loss: 474.4183\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 473.7159 - val_loss: 473.3921\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 472.6975 - val_loss: 472.8680\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 471.5705 - val_loss: 471.9886\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 470.5275 - val_loss: 470.9498\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 469.5089 - val_loss: 470.4186\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 468.5601 - val_loss: 469.0573\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 467.3810 - val_loss: 468.7787\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 466.2821 - val_loss: 468.1814\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 465.3543 - val_loss: 467.9824\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 464.3581 - val_loss: 467.2330\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 463.3711 - val_loss: 466.1767\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 462.4583 - val_loss: 465.6456\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 461.4588 - val_loss: 465.0524\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 460.6062 - val_loss: 463.3466\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 459.5903 - val_loss: 462.9234\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 458.8012 - val_loss: 462.2865\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 458.0391 - val_loss: 462.3902\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 456.9359 - val_loss: 460.8357\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 456.0265 - val_loss: 460.1063\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 455.1905 - val_loss: 459.1844\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 454.5537 - val_loss: 459.6202\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 453.3669 - val_loss: 458.2737\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 452.8704 - val_loss: 458.2097\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 451.6212 - val_loss: 456.9032\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 450.9723 - val_loss: 455.7943\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 450.2540 - val_loss: 454.5403\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 449.3474 - val_loss: 454.9904\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 448.4156 - val_loss: 454.9823\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 447.6888 - val_loss: 454.0474\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 447.1896 - val_loss: 453.0035\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 446.1717 - val_loss: 452.9375\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 445.2229 - val_loss: 452.7681\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 444.5684 - val_loss: 452.8451\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 443.8524 - val_loss: 451.5047\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 442.9749 - val_loss: 450.0224\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 442.3497 - val_loss: 449.1299\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 441.7302 - val_loss: 448.7796\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 440.8388 - val_loss: 448.8173\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 440.1105 - val_loss: 449.2748\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 439.4272 - val_loss: 448.3272\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 438.7965 - val_loss: 447.6064\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 437.9805 - val_loss: 446.8587\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 437.4081 - val_loss: 445.7083\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 436.7826 - val_loss: 446.2316\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 435.9448 - val_loss: 444.8604\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 435.2618 - val_loss: 444.4123\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 434.5903 - val_loss: 443.4750\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 434.2072 - val_loss: 442.3768\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 433.1938 - val_loss: 442.4366\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 432.5490 - val_loss: 442.6934\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 432.0386 - val_loss: 442.7953\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 431.9725 - val_loss: 440.8163\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 430.5941 - val_loss: 440.9410\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 430.3526 - val_loss: 441.8458\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 429.5346 - val_loss: 439.2884\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 428.6937 - val_loss: 438.5178\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 557us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  5 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 20365.0039 - val_loss: 10766.4375\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 6981.7998 - val_loss: 3618.1970\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2266.2847 - val_loss: 1312.9752\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 898.2687 - val_loss: 690.5452\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 541.9887 - val_loss: 550.5598\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 454.6509 - val_loss: 497.0291\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 412.2337 - val_loss: 457.4064\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 383.7746 - val_loss: 426.7001\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 363.9715 - val_loss: 407.8196\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 349.2088 - val_loss: 393.9499\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 335.5113 - val_loss: 379.6164\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 322.6880 - val_loss: 364.1859\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 311.8064 - val_loss: 353.4793\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 301.0123 - val_loss: 341.9110\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 291.8501 - val_loss: 332.1504\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 285.1587 - val_loss: 324.4582\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 277.4777 - val_loss: 317.3169\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 271.1503 - val_loss: 308.8860\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 265.7541 - val_loss: 300.1190\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 258.8252 - val_loss: 295.3011\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 253.8911 - val_loss: 289.8335\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 248.5050 - val_loss: 284.6286\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.5503 - val_loss: 277.9185\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.3065 - val_loss: 272.5526\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.9093 - val_loss: 269.3204\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 230.9814 - val_loss: 262.3153\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 227.9737 - val_loss: 257.0308\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 223.5327 - val_loss: 255.4804\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 221.0608 - val_loss: 248.9931\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 216.6604 - val_loss: 246.2490\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 214.2023 - val_loss: 242.1555\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 210.3959 - val_loss: 237.1162\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.2704 - val_loss: 234.7319\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 204.0680 - val_loss: 229.2359\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.1735 - val_loss: 226.6519\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.0003 - val_loss: 223.2179\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 197.5927 - val_loss: 219.4384\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.6361 - val_loss: 216.2233\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.1988 - val_loss: 215.1329\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 190.5943 - val_loss: 210.5376\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 188.3434 - val_loss: 209.2780\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.5265 - val_loss: 205.7377\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 183.6709 - val_loss: 202.9565\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.4975 - val_loss: 201.1257\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.3724 - val_loss: 197.8950\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 178.2852 - val_loss: 195.3251\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.0392 - val_loss: 192.9669\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.3359 - val_loss: 190.9846\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 172.5385 - val_loss: 188.8350\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.7825 - val_loss: 187.0961\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 169.8819 - val_loss: 184.8295\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 169.4631 - val_loss: 183.6681\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 167.6322 - val_loss: 181.3174\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.1047 - val_loss: 180.0391\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.6944 - val_loss: 178.1610\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.1360 - val_loss: 177.3038\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.6236 - val_loss: 176.0276\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.1977 - val_loss: 174.3509\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.8703 - val_loss: 172.8404\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.2176 - val_loss: 171.6587\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.3464 - val_loss: 170.1233\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.2371 - val_loss: 168.1356\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.4027 - val_loss: 165.9826\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.5121 - val_loss: 164.0565\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.9032 - val_loss: 162.3011\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.1441 - val_loss: 159.7650\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.0197 - val_loss: 156.7411\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.3407 - val_loss: 153.8365\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.8743 - val_loss: 150.5429\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.9787 - val_loss: 146.8100\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.9746 - val_loss: 144.1766\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.6593 - val_loss: 141.3512\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.5013 - val_loss: 138.9595\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.3180 - val_loss: 137.0802\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 131.5891 - val_loss: 135.9860\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.0079 - val_loss: 135.8258\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.4195 - val_loss: 133.5698\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.3441 - val_loss: 133.4265\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.1172 - val_loss: 132.5524\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.2326 - val_loss: 132.0234\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.0004 - val_loss: 132.3929\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.6254 - val_loss: 131.4599\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.5539 - val_loss: 130.8361\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.9689 - val_loss: 130.5318\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.8612 - val_loss: 130.1883\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.2986 - val_loss: 130.2132\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.3660 - val_loss: 130.0558\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.9966 - val_loss: 129.3885\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.2935 - val_loss: 129.7457\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.0628 - val_loss: 129.5773\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.7409 - val_loss: 129.3388\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.4258 - val_loss: 128.6963\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.8828 - val_loss: 128.2434\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.9848 - val_loss: 135.3616\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.0043 - val_loss: 131.0900\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.8402 - val_loss: 128.3882\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.5769 - val_loss: 127.5583\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.8807 - val_loss: 128.4367\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.8470 - val_loss: 127.1087\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.0562 - val_loss: 126.7242\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  6 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 36128.2578 - val_loss: 12929.3545\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5030.8691 - val_loss: 927.6431\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 845.7408 - val_loss: 1107.7407\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 851.4971 - val_loss: 774.3458\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 619.8656 - val_loss: 738.0480\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 602.9965 - val_loss: 702.1618\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 576.0376 - val_loss: 672.7825\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 559.6884 - val_loss: 645.7330\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 533.6392 - val_loss: 622.5837\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 515.5628 - val_loss: 595.9048\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 495.6696 - val_loss: 570.1854\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 476.7014 - val_loss: 547.3749\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 457.8071 - val_loss: 523.1060\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 440.7456 - val_loss: 501.1495\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 423.1730 - val_loss: 479.3806\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 407.5227 - val_loss: 459.1536\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 391.1529 - val_loss: 439.8844\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 376.8539 - val_loss: 420.9372\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 362.0640 - val_loss: 403.2732\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 350.1380 - val_loss: 386.3458\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 338.0271 - val_loss: 371.1557\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 323.7649 - val_loss: 355.3463\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 311.3165 - val_loss: 341.6809\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 300.8066 - val_loss: 326.8666\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 289.1939 - val_loss: 313.9688\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 278.7901 - val_loss: 302.2127\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 269.0456 - val_loss: 290.5093\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 260.0029 - val_loss: 279.5048\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 251.3582 - val_loss: 269.0793\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.9920 - val_loss: 259.5056\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 234.6352 - val_loss: 250.8985\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 226.2530 - val_loss: 241.6456\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 218.3758 - val_loss: 233.3778\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 211.1740 - val_loss: 225.8049\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 205.1584 - val_loss: 218.5989\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.5772 - val_loss: 211.2857\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 191.4798 - val_loss: 205.1616\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.5602 - val_loss: 199.0098\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.2598 - val_loss: 193.1543\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 175.7095 - val_loss: 187.8613\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.6225 - val_loss: 183.4312\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.5265 - val_loss: 178.3463\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.8907 - val_loss: 175.0599\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.8944 - val_loss: 170.9112\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.5373 - val_loss: 166.6403\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.9702 - val_loss: 163.2023\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.6876 - val_loss: 159.3944\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.2712 - val_loss: 157.8807\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.1047 - val_loss: 153.6362\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.6075 - val_loss: 151.1882\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.2161 - val_loss: 149.2681\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.3794 - val_loss: 146.5657\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.9874 - val_loss: 144.4464\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.6779 - val_loss: 142.4001\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.4469 - val_loss: 140.6952\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.6218 - val_loss: 139.6260\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.2887 - val_loss: 138.0852\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.1046 - val_loss: 136.3252\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.1584 - val_loss: 136.4999\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.0337 - val_loss: 133.5870\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.6127 - val_loss: 132.5400\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.3215 - val_loss: 132.0503\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.5570 - val_loss: 133.2130\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.8672 - val_loss: 131.6269\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.7684 - val_loss: 131.9327\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.0701 - val_loss: 128.4438\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.8816 - val_loss: 129.8698\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.6848 - val_loss: 127.4851\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.0437 - val_loss: 128.8413\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.2666 - val_loss: 126.1779\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.2118 - val_loss: 125.8827\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.2436 - val_loss: 125.8518\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.5409 - val_loss: 128.1922\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.3225 - val_loss: 124.9828\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.9256 - val_loss: 126.3730\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.4022 - val_loss: 124.4527\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.7914 - val_loss: 124.8272\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.1530 - val_loss: 123.9196\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.0022 - val_loss: 125.6330\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.1344 - val_loss: 123.9050\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.6998 - val_loss: 122.9599\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.4584 - val_loss: 122.5900\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.7875 - val_loss: 122.4127\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 98.4913 - val_loss: 122.5470\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.8231 - val_loss: 122.4990\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.5502 - val_loss: 122.1692\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.1176 - val_loss: 122.1875\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.9522 - val_loss: 122.3981\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.5337 - val_loss: 121.8316\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.2736 - val_loss: 121.5599\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.2620 - val_loss: 121.5145\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.3103 - val_loss: 123.6180\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.4430 - val_loss: 121.9352\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.8787 - val_loss: 122.0567\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.9787 - val_loss: 120.9534\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.9007 - val_loss: 123.9588\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.8406 - val_loss: 122.6398\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.9427 - val_loss: 120.8211\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.9244 - val_loss: 123.7331\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.8301 - val_loss: 128.1168\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  7 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2045.3536 - val_loss: 1768.5065\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1584.5736 - val_loss: 1471.0398\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1333.0374 - val_loss: 1232.7516\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1108.3352 - val_loss: 1056.7687\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 945.4892 - val_loss: 909.6199\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 795.9822 - val_loss: 752.7182\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 668.3085 - val_loss: 662.7917\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 580.0846 - val_loss: 563.8381\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 504.6593 - val_loss: 499.3363\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 445.2823 - val_loss: 439.2981\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 401.7209 - val_loss: 392.5651\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 354.4535 - val_loss: 362.2013\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 328.8491 - val_loss: 322.0984\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 300.2714 - val_loss: 305.9175\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 277.3531 - val_loss: 277.2561\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 263.7343 - val_loss: 258.0963\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.3679 - val_loss: 247.1453\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 232.4297 - val_loss: 232.4249\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 223.9605 - val_loss: 222.1880\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 211.6545 - val_loss: 208.1118\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.0995 - val_loss: 200.5854\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 195.2568 - val_loss: 190.2305\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 186.8331 - val_loss: 182.6385\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.4776 - val_loss: 183.1642\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 182.2708 - val_loss: 176.7744\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.2060 - val_loss: 170.4261\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 169.6644 - val_loss: 159.7754\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.9584 - val_loss: 151.7946\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.9590 - val_loss: 158.3131\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.5650 - val_loss: 143.7027\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 148.8022 - val_loss: 140.5890\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.6868 - val_loss: 144.8828\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.0665 - val_loss: 134.8224\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.8833 - val_loss: 135.5110\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.9407 - val_loss: 128.8954\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.3800 - val_loss: 126.0059\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.6625 - val_loss: 123.8908\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.3553 - val_loss: 127.7353\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.1071 - val_loss: 121.4534\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.6021 - val_loss: 123.8125\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.6122 - val_loss: 118.2426\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.7335 - val_loss: 114.8220\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.4035 - val_loss: 113.5735\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.7422 - val_loss: 111.7858\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.6821 - val_loss: 110.9738\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.5591 - val_loss: 109.7045\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.6279 - val_loss: 107.3778\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.5532 - val_loss: 106.4117\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.8390 - val_loss: 107.8770\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.7447 - val_loss: 104.2463\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.0998 - val_loss: 101.9128\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.1682 - val_loss: 102.8818\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.7837 - val_loss: 99.6778\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.4204 - val_loss: 98.8851\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.7564 - val_loss: 98.0401\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.2610 - val_loss: 96.9717\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.1116 - val_loss: 99.5195\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.9002 - val_loss: 93.9406\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.0394 - val_loss: 93.6048\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.4789 - val_loss: 92.0143\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.2485 - val_loss: 92.4134\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.3229 - val_loss: 89.3288\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.0956 - val_loss: 88.8750\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.2536 - val_loss: 92.0732\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.7086 - val_loss: 88.2383\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.8499 - val_loss: 88.7437\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 90.7590 - val_loss: 87.7249\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.2724 - val_loss: 95.6365\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.4186 - val_loss: 103.4799\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.0536 - val_loss: 86.7779\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.0443 - val_loss: 90.3932\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.4429 - val_loss: 87.1036\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.7803 - val_loss: 86.1048\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.3986 - val_loss: 85.9990\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.9031 - val_loss: 85.7909\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.3056 - val_loss: 86.5957\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.7541 - val_loss: 85.9081\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.1676 - val_loss: 85.6542\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.9309 - val_loss: 89.6912\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.9376 - val_loss: 85.4733\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.2692 - val_loss: 85.8477\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.0944 - val_loss: 86.4876\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.4712 - val_loss: 84.5424\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.7075 - val_loss: 86.3847\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.2285 - val_loss: 84.7601\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.2155 - val_loss: 90.5134\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.4393 - val_loss: 84.3257\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.5206 - val_loss: 84.4350\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.3768 - val_loss: 83.8384\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.9273 - val_loss: 83.5896\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.2292 - val_loss: 83.6106\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.9492 - val_loss: 83.6307\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.4012 - val_loss: 84.9604\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.1077 - val_loss: 84.0177\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.9220 - val_loss: 85.8518\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.7251 - val_loss: 83.0888\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.6092 - val_loss: 84.0948\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.0235 - val_loss: 84.6936\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.2562 - val_loss: 85.0915\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.4872 - val_loss: 82.9521\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  8 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 160161.0156 - val_loss: 124498.2188\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99414.3203 - val_loss: 71593.4766\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 52644.6562 - val_loss: 36500.5234\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 26265.2129 - val_loss: 17973.8691\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 12031.1348 - val_loss: 7251.8179\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3656.0522 - val_loss: 1675.8230\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 942.2258 - val_loss: 1130.8015\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 886.9667 - val_loss: 1048.4769\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 789.2292 - val_loss: 1000.5051\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 741.5448 - val_loss: 953.4296\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 701.7638 - val_loss: 898.1464\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 664.7208 - val_loss: 845.5274\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 631.2082 - val_loss: 798.6614\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 598.7210 - val_loss: 751.8201\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 569.0347 - val_loss: 711.2999\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 540.6724 - val_loss: 677.1955\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 513.9139 - val_loss: 641.7820\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 490.3069 - val_loss: 607.1750\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 467.7910 - val_loss: 581.2517\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 447.5814 - val_loss: 547.8934\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 429.6396 - val_loss: 528.5128\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 411.0733 - val_loss: 498.7502\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 395.8495 - val_loss: 474.8545\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 381.5539 - val_loss: 460.5423\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 368.5748 - val_loss: 436.8631\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 356.5576 - val_loss: 417.9720\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 346.3735 - val_loss: 401.9089\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 337.0350 - val_loss: 380.5359\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 327.9988 - val_loss: 369.3818\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 318.2420 - val_loss: 355.3481\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 311.3415 - val_loss: 339.2392\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 303.7841 - val_loss: 330.3279\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 297.3492 - val_loss: 321.0096\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 291.7752 - val_loss: 309.5654\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 287.0432 - val_loss: 298.6766\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 282.2791 - val_loss: 293.3405\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 278.5368 - val_loss: 282.1149\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 273.7385 - val_loss: 279.6107\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 270.4580 - val_loss: 274.5116\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 267.8666 - val_loss: 265.9482\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 264.1171 - val_loss: 259.6100\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 261.8903 - val_loss: 254.5907\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 258.5484 - val_loss: 252.1899\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 257.2310 - val_loss: 248.2943\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.0739 - val_loss: 243.2353\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 251.8059 - val_loss: 242.1710\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 250.8073 - val_loss: 237.8255\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 250.1901 - val_loss: 231.9145\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 247.5116 - val_loss: 235.7061\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.2691 - val_loss: 229.0144\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.3222 - val_loss: 225.7049\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 242.4602 - val_loss: 223.6098\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.4139 - val_loss: 222.4170\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 240.7415 - val_loss: 221.0835\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.6321 - val_loss: 216.0381\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.8027 - val_loss: 216.0285\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.1623 - val_loss: 213.5751\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.1495 - val_loss: 214.7750\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.6997 - val_loss: 209.9802\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.5973 - val_loss: 209.6336\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 234.8799 - val_loss: 210.6321\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 234.7453 - val_loss: 205.4828\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 232.5356 - val_loss: 207.3818\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 231.8638 - val_loss: 203.2207\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 230.9058 - val_loss: 204.2198\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 230.3452 - val_loss: 203.0991\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 229.3681 - val_loss: 201.8235\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 228.9804 - val_loss: 199.5263\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 228.3895 - val_loss: 200.3952\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 227.8888 - val_loss: 198.3445\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 227.2894 - val_loss: 197.9917\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 226.7108 - val_loss: 197.3197\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 226.3119 - val_loss: 195.8224\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 225.9121 - val_loss: 195.9678\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 225.1330 - val_loss: 195.5352\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 224.8869 - val_loss: 193.6379\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 225.0195 - val_loss: 193.9640\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 224.2542 - val_loss: 193.8072\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 224.5457 - val_loss: 193.0311\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.8146 - val_loss: 189.4308\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.3948 - val_loss: 194.5321\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.6323 - val_loss: 191.0571\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.5759 - val_loss: 189.5273\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 223.8202 - val_loss: 190.9003\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 220.9877 - val_loss: 186.9113\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 219.9242 - val_loss: 190.7225\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 220.9514 - val_loss: 188.8321\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 219.5734 - val_loss: 186.4581\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 219.5510 - val_loss: 186.4399\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 218.9716 - val_loss: 188.3796\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 218.0536 - val_loss: 186.5024\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.7706 - val_loss: 185.5167\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.3518 - val_loss: 185.4521\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 218.1051 - val_loss: 185.5966\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.4168 - val_loss: 185.6949\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 215.7186 - val_loss: 183.2738\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 216.2382 - val_loss: 185.6895\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 216.1249 - val_loss: 183.1823\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 214.5379 - val_loss: 182.6802\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 214.6403 - val_loss: 182.4621\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 668us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  9 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 150053.4062 - val_loss: 110921.2500\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89625.0234 - val_loss: 67069.8516\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 55994.8633 - val_loss: 43574.1016\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 37269.9883 - val_loss: 29601.3926\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 25462.2930 - val_loss: 20436.9648\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 16945.6660 - val_loss: 11905.6016\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 8449.3760 - val_loss: 4222.2100\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2782.7576 - val_loss: 1296.0887\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1415.5402 - val_loss: 1177.8591\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1136.5670 - val_loss: 878.4985\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 892.1951 - val_loss: 719.1159\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 717.6406 - val_loss: 600.3098\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 585.2121 - val_loss: 508.2418\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 494.8205 - val_loss: 441.6885\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 419.5734 - val_loss: 389.2371\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 363.6686 - val_loss: 344.8588\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 318.9451 - val_loss: 310.7092\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 283.3000 - val_loss: 280.8490\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 250.3825 - val_loss: 258.0775\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 225.1076 - val_loss: 234.3626\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.6620 - val_loss: 216.4638\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.2507 - val_loss: 202.3385\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 165.5439 - val_loss: 190.9420\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.6831 - val_loss: 182.6412\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.9056 - val_loss: 176.7276\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.6322 - val_loss: 171.9219\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.6986 - val_loss: 166.7944\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.1179 - val_loss: 162.2262\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.9997 - val_loss: 160.1896\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.9966 - val_loss: 155.9908\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.1190 - val_loss: 153.2676\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.7392 - val_loss: 150.1988\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.7674 - val_loss: 148.4951\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.3327 - val_loss: 146.3347\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.5836 - val_loss: 144.1504\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.3247 - val_loss: 142.3773\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.1898 - val_loss: 141.0261\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.0699 - val_loss: 140.1833\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.5596 - val_loss: 138.3086\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.1005 - val_loss: 136.7609\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.8362 - val_loss: 135.8254\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.8425 - val_loss: 134.3891\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.4934 - val_loss: 133.4449\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.8346 - val_loss: 132.3430\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.1730 - val_loss: 131.5613\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.8415 - val_loss: 131.4293\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.9081 - val_loss: 130.1397\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.8869 - val_loss: 128.9600\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.7010 - val_loss: 128.6652\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.2255 - val_loss: 127.7568\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.7320 - val_loss: 127.1793\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.8659 - val_loss: 126.7561\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 101.5476 - val_loss: 126.2529\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.9092 - val_loss: 125.2729\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.8675 - val_loss: 124.7986\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.4366 - val_loss: 125.1591\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.5666 - val_loss: 125.5591\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.2318 - val_loss: 123.5346\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.8902 - val_loss: 123.0284\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.1945 - val_loss: 123.0399\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.1125 - val_loss: 122.8847\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.4910 - val_loss: 121.8363\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.4268 - val_loss: 121.3350\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.0054 - val_loss: 121.0677\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.6667 - val_loss: 120.7282\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.3964 - val_loss: 120.4190\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.5226 - val_loss: 120.0490\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.8826 - val_loss: 119.7160\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.2584 - val_loss: 121.2851\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.3735 - val_loss: 121.0957\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.9006 - val_loss: 118.9403\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.1694 - val_loss: 119.2215\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.1934 - val_loss: 118.3429\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.8905 - val_loss: 120.6799\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.6078 - val_loss: 119.0865\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.3421 - val_loss: 120.6362\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.2466 - val_loss: 117.8769\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.9009 - val_loss: 116.8109\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.3363 - val_loss: 118.4896\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.7431 - val_loss: 116.4555\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.9920 - val_loss: 116.9161\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.1430 - val_loss: 116.6850\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.4126 - val_loss: 117.0096\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.9358 - val_loss: 117.3315\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.7411 - val_loss: 116.3318\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.4829 - val_loss: 117.5559\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.8831 - val_loss: 115.2831\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.9752 - val_loss: 114.7852\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.5112 - val_loss: 116.3578\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.0021 - val_loss: 114.5708\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.3691 - val_loss: 114.8872\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.6925 - val_loss: 116.5479\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.5185 - val_loss: 114.0058\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.9438 - val_loss: 115.1018\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.2343 - val_loss: 116.2422\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.5767 - val_loss: 113.4372\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.5651 - val_loss: 113.4346\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.6699 - val_loss: 113.0822\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.5977 - val_loss: 113.0560\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.4687 - val_loss: 112.8339\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 557us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  10 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 12107.8398 - val_loss: 4362.6030\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2292.5159 - val_loss: 584.8705\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 530.5847 - val_loss: 528.4249\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 524.6029 - val_loss: 471.8809\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 458.1918 - val_loss: 432.0207\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 432.3810 - val_loss: 417.6787\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 413.1389 - val_loss: 403.2899\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 390.6860 - val_loss: 390.7686\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 374.2592 - val_loss: 375.8163\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 358.0499 - val_loss: 361.5660\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 341.7272 - val_loss: 349.3586\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 327.4734 - val_loss: 338.2764\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 314.1210 - val_loss: 327.8593\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 302.2707 - val_loss: 317.3597\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 288.5053 - val_loss: 306.4897\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 276.2993 - val_loss: 297.2283\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 265.2185 - val_loss: 288.9274\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 254.0968 - val_loss: 279.8314\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.3389 - val_loss: 272.4820\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.9658 - val_loss: 264.4763\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 228.9875 - val_loss: 257.1781\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 221.6315 - val_loss: 250.8580\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 214.9766 - val_loss: 244.8220\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 209.6417 - val_loss: 239.2302\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 204.7762 - val_loss: 233.6645\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.4638 - val_loss: 230.7562\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.6145 - val_loss: 224.5457\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 189.8699 - val_loss: 220.8055\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 186.6705 - val_loss: 215.8798\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 182.8472 - val_loss: 212.5883\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.5115 - val_loss: 208.7044\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.8063 - val_loss: 205.6895\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.9193 - val_loss: 201.4169\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 167.2943 - val_loss: 198.4233\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.3571 - val_loss: 195.6107\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.8266 - val_loss: 192.3592\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.5022 - val_loss: 190.6639\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.5682 - val_loss: 188.0298\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.2421 - val_loss: 184.3888\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.6772 - val_loss: 183.8368\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.7544 - val_loss: 179.3786\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.9449 - val_loss: 177.0111\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.2508 - val_loss: 175.1518\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.7834 - val_loss: 173.5026\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.1384 - val_loss: 170.4369\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.9769 - val_loss: 168.3553\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.2667 - val_loss: 166.5499\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.8342 - val_loss: 164.7787\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.3946 - val_loss: 163.2903\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.7148 - val_loss: 160.9656\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.4255 - val_loss: 159.1180\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.4933 - val_loss: 159.4969\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.8569 - val_loss: 155.7261\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.8777 - val_loss: 154.8349\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.0232 - val_loss: 153.7328\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.6264 - val_loss: 151.3484\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 124.4574 - val_loss: 150.5999\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.9957 - val_loss: 148.4436\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.1933 - val_loss: 147.5218\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.4955 - val_loss: 145.8539\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.5796 - val_loss: 144.8691\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.3572 - val_loss: 143.2985\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.1878 - val_loss: 142.7354\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.6339 - val_loss: 140.5242\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.8278 - val_loss: 139.7356\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.1763 - val_loss: 138.1369\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.0702 - val_loss: 138.3180\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.4615 - val_loss: 136.0591\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.2501 - val_loss: 137.0345\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.9145 - val_loss: 134.2244\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.9222 - val_loss: 133.8603\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.0426 - val_loss: 132.7921\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.9340 - val_loss: 130.6156\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.2179 - val_loss: 130.6603\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.0496 - val_loss: 128.2021\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.2479 - val_loss: 128.9403\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.1785 - val_loss: 128.3488\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.0649 - val_loss: 127.3945\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.2332 - val_loss: 127.2766\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.6993 - val_loss: 123.5072\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.4299 - val_loss: 123.0136\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.8596 - val_loss: 121.4939\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.4444 - val_loss: 120.9866\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.4696 - val_loss: 120.3204\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.4772 - val_loss: 119.3645\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.1263 - val_loss: 118.2325\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.2591 - val_loss: 117.6216\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.3534 - val_loss: 117.2908\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.9149 - val_loss: 116.5439\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.4269 - val_loss: 115.8044\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.2133 - val_loss: 115.5453\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.1765 - val_loss: 114.8642\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 99.0269 - val_loss: 116.9816\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.3759 - val_loss: 113.6484\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.6639 - val_loss: 113.0238\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.9620 - val_loss: 114.8440\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.9283 - val_loss: 112.1038\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.4637 - val_loss: 113.3755\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.0640 - val_loss: 111.7046\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.8906 - val_loss: 110.5716\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 554us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  11 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1606.7273 - val_loss: 403.6492\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 290.4916 - val_loss: 342.8440\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 314.1279 - val_loss: 305.9878\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 281.4543 - val_loss: 291.5599\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 277.6834 - val_loss: 290.7755\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 274.0338 - val_loss: 288.7838\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 271.7914 - val_loss: 285.8123\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 269.8547 - val_loss: 282.8377\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 265.9540 - val_loss: 278.5906\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 263.8836 - val_loss: 274.8277\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 260.6052 - val_loss: 271.0397\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 258.2816 - val_loss: 268.8145\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 258.5575 - val_loss: 265.3264\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 253.3058 - val_loss: 261.9476\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 250.9154 - val_loss: 260.3814\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 246.2709 - val_loss: 256.2352\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.1124 - val_loss: 252.9379\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 242.0676 - val_loss: 249.6478\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.0969 - val_loss: 247.2191\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.8092 - val_loss: 242.4619\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 232.9653 - val_loss: 237.3811\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 230.7442 - val_loss: 236.2520\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 224.1676 - val_loss: 230.7422\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 221.1448 - val_loss: 225.9119\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.6143 - val_loss: 221.8536\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 214.0588 - val_loss: 218.5516\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 214.1305 - val_loss: 216.6574\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.2992 - val_loss: 210.6596\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 205.7804 - val_loss: 210.2195\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 202.6250 - val_loss: 203.3178\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 200.8860 - val_loss: 202.4393\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 195.1326 - val_loss: 202.5287\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.1149 - val_loss: 197.7346\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 188.5521 - val_loss: 194.4858\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.4375 - val_loss: 189.9848\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 182.0956 - val_loss: 188.1732\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.9212 - val_loss: 185.9980\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.8087 - val_loss: 182.8838\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 172.7132 - val_loss: 178.7317\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.2816 - val_loss: 177.4731\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 167.2917 - val_loss: 174.4428\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.3330 - val_loss: 171.5042\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.9583 - val_loss: 168.7825\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.7716 - val_loss: 166.8358\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.6331 - val_loss: 164.7973\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.9539 - val_loss: 161.7428\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.6046 - val_loss: 159.0502\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.8425 - val_loss: 157.4046\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.7232 - val_loss: 153.4669\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.2263 - val_loss: 151.1214\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.6833 - val_loss: 148.2843\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.9336 - val_loss: 146.8492\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.0273 - val_loss: 144.7717\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.5721 - val_loss: 142.2303\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.7041 - val_loss: 138.6215\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.4362 - val_loss: 136.4616\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.1657 - val_loss: 134.9397\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.4193 - val_loss: 131.9485\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.9646 - val_loss: 130.3989\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.2704 - val_loss: 128.4620\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.8334 - val_loss: 126.1212\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.0146 - val_loss: 124.9481\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.6285 - val_loss: 122.8894\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.6785 - val_loss: 123.3809\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.4944 - val_loss: 120.4140\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.6854 - val_loss: 117.7264\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 108.9170 - val_loss: 116.0946\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.7517 - val_loss: 115.2304\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.0371 - val_loss: 114.6511\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.1616 - val_loss: 112.5249\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.3449 - val_loss: 110.6679\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.6011 - val_loss: 110.2167\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.2755 - val_loss: 108.8183\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.4107 - val_loss: 106.9720\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.4332 - val_loss: 108.0003\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.8464 - val_loss: 105.2708\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.4200 - val_loss: 104.5375\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.7912 - val_loss: 103.0600\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.3687 - val_loss: 102.9328\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.5257 - val_loss: 101.9771\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.0121 - val_loss: 100.8008\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.2363 - val_loss: 98.9977\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.2794 - val_loss: 99.4294\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.2862 - val_loss: 100.0558\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.1213 - val_loss: 96.1660\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.6168 - val_loss: 95.1307\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.8452 - val_loss: 94.9540\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.5960 - val_loss: 93.0315\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.9416 - val_loss: 93.4014\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.9539 - val_loss: 92.4958\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.9732 - val_loss: 90.9247\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.1888 - val_loss: 91.6242\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.7928 - val_loss: 92.4914\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.7273 - val_loss: 91.8614\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.9735 - val_loss: 89.1098\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.0380 - val_loss: 89.0077\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.6391 - val_loss: 89.0363\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.8858 - val_loss: 89.2098\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.5076 - val_loss: 88.5205\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.0869 - val_loss: 87.4423\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 660us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  12 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 376.2369 - val_loss: 323.6737\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 298.9911 - val_loss: 273.2026\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 261.9833 - val_loss: 241.3997\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 240.5065 - val_loss: 221.0635\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 224.6373 - val_loss: 207.3077\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 213.0149 - val_loss: 196.8361\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.9319 - val_loss: 190.2845\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.4751 - val_loss: 182.9886\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.2165 - val_loss: 177.1592\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.2372 - val_loss: 173.0186\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.4870 - val_loss: 166.2797\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.1712 - val_loss: 165.0712\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 167.8170 - val_loss: 159.9605\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.9670 - val_loss: 158.7260\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.4567 - val_loss: 152.1433\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.0488 - val_loss: 153.1668\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.6320 - val_loss: 148.5926\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.7410 - val_loss: 148.4287\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.8983 - val_loss: 142.5207\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.8444 - val_loss: 141.6526\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.9099 - val_loss: 141.3027\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.7857 - val_loss: 138.6742\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.0041 - val_loss: 135.5883\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.2059 - val_loss: 135.0669\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.7974 - val_loss: 132.5507\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.0313 - val_loss: 131.8434\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.3097 - val_loss: 133.8405\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.6865 - val_loss: 128.4290\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.8073 - val_loss: 127.4889\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.8358 - val_loss: 128.0588\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.3345 - val_loss: 124.0045\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.9002 - val_loss: 134.1602\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.3238 - val_loss: 126.1010\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.7864 - val_loss: 122.9565\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.2873 - val_loss: 127.3480\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.9947 - val_loss: 120.4736\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.4921 - val_loss: 129.1956\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.7988 - val_loss: 119.6354\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.5744 - val_loss: 128.7497\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.3868 - val_loss: 119.9999\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.6815 - val_loss: 119.2725\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.0021 - val_loss: 119.7840\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.8758 - val_loss: 119.3316\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.2452 - val_loss: 119.3071\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.8983 - val_loss: 117.7896\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.9713 - val_loss: 118.2107\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.1956 - val_loss: 116.4558\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.0690 - val_loss: 117.3758\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.5430 - val_loss: 118.6910\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.7598 - val_loss: 123.4395\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.6154 - val_loss: 115.0374\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.6056 - val_loss: 116.3546\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.9952 - val_loss: 117.6636\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.3092 - val_loss: 117.1665\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.6059 - val_loss: 113.5519\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.3620 - val_loss: 121.4681\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.5094 - val_loss: 115.7585\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.8333 - val_loss: 116.6971\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.5047 - val_loss: 113.8035\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.0610 - val_loss: 112.9144\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.2199 - val_loss: 114.5506\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.4402 - val_loss: 123.0497\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.3180 - val_loss: 113.3272\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.4258 - val_loss: 111.5402\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.8145 - val_loss: 113.7283\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.4768 - val_loss: 113.9446\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 111.4195 - val_loss: 111.3145\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.5710 - val_loss: 125.2676\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.8396 - val_loss: 111.6735\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.7383 - val_loss: 119.1717\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.6072 - val_loss: 110.5421\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.6106 - val_loss: 118.7622\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.2767 - val_loss: 110.5767\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.0297 - val_loss: 115.2059\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.8457 - val_loss: 112.8648\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.3023 - val_loss: 113.4172\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.8599 - val_loss: 118.6863\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.1164 - val_loss: 109.7355\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.4640 - val_loss: 116.0228\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.8372 - val_loss: 109.3592\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.1932 - val_loss: 108.8792\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.8469 - val_loss: 122.0421\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.6382 - val_loss: 108.4210\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.6516 - val_loss: 111.0659\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.7432 - val_loss: 112.3363\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.6807 - val_loss: 108.9451\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.8550 - val_loss: 110.3507\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.3379 - val_loss: 108.8941\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.6719 - val_loss: 108.1545\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 108.1072 - val_loss: 112.9558\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.1558 - val_loss: 108.3229\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.5662 - val_loss: 114.3852\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.7514 - val_loss: 109.7195\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.2630 - val_loss: 110.7974\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.9948 - val_loss: 107.4011\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.3368 - val_loss: 110.3934\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.8570 - val_loss: 110.2138\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.6401 - val_loss: 109.3060\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.7528 - val_loss: 107.9079\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.5959 - val_loss: 111.4269\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  13 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 214853.4531 - val_loss: 142316.8750\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101795.9141 - val_loss: 61341.9414\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 40183.5781 - val_loss: 18913.7148\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 10733.6133 - val_loss: 3365.7488\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2473.2346 - val_loss: 1885.6122\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1910.0568 - val_loss: 1973.3514\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1839.8546 - val_loss: 1782.2120\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1723.6499 - val_loss: 1659.7333\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1657.8542 - val_loss: 1602.1309\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1592.5795 - val_loss: 1566.9385\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1529.3694 - val_loss: 1504.3905\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1470.5217 - val_loss: 1450.6545\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1416.2887 - val_loss: 1408.4156\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1363.9550 - val_loss: 1364.5162\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1312.9313 - val_loss: 1322.2534\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1265.7693 - val_loss: 1280.2667\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1219.7878 - val_loss: 1241.9103\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1177.0245 - val_loss: 1211.8330\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1137.7480 - val_loss: 1176.2460\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1098.4828 - val_loss: 1133.2338\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1063.3950 - val_loss: 1106.8673\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1030.0282 - val_loss: 1076.9458\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 995.4414 - val_loss: 1057.6910\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 964.3119 - val_loss: 1025.6453\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 937.5917 - val_loss: 995.3032\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 906.3927 - val_loss: 978.2198\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 883.2534 - val_loss: 956.5546\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 855.2867 - val_loss: 929.1194\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 833.2055 - val_loss: 906.6748\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 807.8855 - val_loss: 893.0513\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 784.9471 - val_loss: 867.4618\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 771.3097 - val_loss: 846.6552\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 741.9694 - val_loss: 840.1870\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 724.7328 - val_loss: 814.9562\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 704.6938 - val_loss: 798.1680\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 686.7516 - val_loss: 780.5134\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 669.4361 - val_loss: 768.9608\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 652.3719 - val_loss: 750.2697\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 636.4429 - val_loss: 733.1475\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 620.3176 - val_loss: 721.1777\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 608.4755 - val_loss: 710.2119\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 591.1700 - val_loss: 692.0458\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 582.2338 - val_loss: 682.5649\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 565.0645 - val_loss: 665.8702\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 550.9395 - val_loss: 655.0034\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 540.8492 - val_loss: 643.8221\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 526.6490 - val_loss: 633.0629\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 513.9294 - val_loss: 618.8078\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 503.0117 - val_loss: 611.8713\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 497.3188 - val_loss: 596.3304\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 481.3466 - val_loss: 591.2156\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 469.0587 - val_loss: 576.2601\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 460.3513 - val_loss: 566.1652\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 448.2494 - val_loss: 553.8989\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 439.7970 - val_loss: 547.4734\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 430.4679 - val_loss: 537.4483\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 420.2184 - val_loss: 525.0104\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 411.8358 - val_loss: 516.0864\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 402.5558 - val_loss: 507.3636\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 395.4948 - val_loss: 496.6989\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 385.7675 - val_loss: 492.4481\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 378.0532 - val_loss: 480.3701\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 371.3841 - val_loss: 470.7348\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 363.3179 - val_loss: 463.6482\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 354.5330 - val_loss: 452.8317\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 347.4022 - val_loss: 445.2218\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 341.1420 - val_loss: 438.0113\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 333.9641 - val_loss: 428.7191\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 329.3038 - val_loss: 418.9660\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 320.1695 - val_loss: 412.8536\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 313.4766 - val_loss: 402.1500\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 306.4765 - val_loss: 395.8040\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 302.6198 - val_loss: 385.7255\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 294.4684 - val_loss: 378.0827\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 286.3206 - val_loss: 369.5261\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 281.4823 - val_loss: 361.9965\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 276.9089 - val_loss: 354.3796\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 266.9951 - val_loss: 344.5529\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 260.4132 - val_loss: 335.6259\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.1340 - val_loss: 326.9771\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 246.4905 - val_loss: 317.3807\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.0170 - val_loss: 308.6260\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 233.1163 - val_loss: 299.4045\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 227.0775 - val_loss: 292.3568\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.1475 - val_loss: 283.2969\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 219.2843 - val_loss: 275.6106\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 213.2659 - val_loss: 268.5789\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.3736 - val_loss: 261.7969\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 205.1519 - val_loss: 255.3727\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 201.6070 - val_loss: 250.0488\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 197.9746 - val_loss: 243.3814\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.1636 - val_loss: 236.7197\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 190.6505 - val_loss: 230.8299\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.5873 - val_loss: 225.2701\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 184.6555 - val_loss: 220.5610\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.2569 - val_loss: 215.8286\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 178.8997 - val_loss: 211.2755\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 175.3464 - val_loss: 206.7496\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.2564 - val_loss: 202.5412\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.9668 - val_loss: 199.3706\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  14 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 38975.8867 - val_loss: 17701.7812\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 10882.0176 - val_loss: 5304.4360\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3486.1694 - val_loss: 1863.9685\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1386.4615 - val_loss: 902.6028\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 807.2621 - val_loss: 693.7942\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 698.9521 - val_loss: 657.1862\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 671.6219 - val_loss: 646.2994\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 658.5594 - val_loss: 635.8461\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 647.9117 - val_loss: 625.5524\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 639.0714 - val_loss: 617.9639\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 630.8630 - val_loss: 610.9003\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 622.9509 - val_loss: 603.6915\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 614.9930 - val_loss: 596.5075\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 607.8104 - val_loss: 589.7035\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 600.1634 - val_loss: 582.7823\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 592.8123 - val_loss: 576.5817\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 586.9714 - val_loss: 569.6741\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 579.5868 - val_loss: 563.4375\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 573.3839 - val_loss: 557.4553\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 566.9221 - val_loss: 552.3990\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 560.3287 - val_loss: 545.6599\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 554.6477 - val_loss: 539.7722\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 548.4507 - val_loss: 534.5051\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 542.4213 - val_loss: 529.4993\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 536.3743 - val_loss: 523.3926\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 530.2688 - val_loss: 517.6129\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 525.3924 - val_loss: 511.8842\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 518.4840 - val_loss: 506.4839\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 514.0790 - val_loss: 502.1963\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 507.2116 - val_loss: 496.6040\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 502.8895 - val_loss: 491.1089\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 496.0385 - val_loss: 487.0652\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 491.4860 - val_loss: 481.7833\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 486.8051 - val_loss: 477.3072\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 479.6037 - val_loss: 472.6983\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 474.2678 - val_loss: 467.3175\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 469.2991 - val_loss: 462.1261\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 464.7879 - val_loss: 457.8020\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 459.1560 - val_loss: 452.1910\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 453.5435 - val_loss: 447.3088\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 448.2751 - val_loss: 442.3009\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 443.3047 - val_loss: 437.1324\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 439.5233 - val_loss: 432.6689\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 432.7601 - val_loss: 428.7079\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 429.3164 - val_loss: 423.7717\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 423.8726 - val_loss: 418.1212\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 419.5338 - val_loss: 414.1536\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 414.7361 - val_loss: 410.4818\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 410.3336 - val_loss: 406.6212\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 405.6487 - val_loss: 402.4234\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 401.4467 - val_loss: 398.4389\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 397.5562 - val_loss: 394.1084\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 393.7870 - val_loss: 390.9717\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 388.6843 - val_loss: 387.5793\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 385.0672 - val_loss: 383.7244\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 381.8464 - val_loss: 380.2592\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 376.9500 - val_loss: 376.7173\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 373.6219 - val_loss: 373.1467\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 369.3943 - val_loss: 370.4966\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 368.1897 - val_loss: 367.3742\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 363.9073 - val_loss: 364.3011\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 359.1768 - val_loss: 361.5766\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 355.6116 - val_loss: 358.8146\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 354.0246 - val_loss: 356.0363\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 349.8840 - val_loss: 353.5074\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 347.2810 - val_loss: 351.0206\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 344.7703 - val_loss: 348.1525\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 342.0006 - val_loss: 346.1768\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 339.1132 - val_loss: 343.7925\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 336.6655 - val_loss: 342.2459\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 334.4048 - val_loss: 338.9341\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 333.8521 - val_loss: 337.8498\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 330.3709 - val_loss: 336.0601\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 326.5677 - val_loss: 333.6476\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 324.5779 - val_loss: 330.9980\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 323.1062 - val_loss: 329.3510\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 321.4626 - val_loss: 327.6060\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 317.9660 - val_loss: 325.9276\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 316.6342 - val_loss: 323.9406\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 313.8320 - val_loss: 322.0938\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 312.2849 - val_loss: 320.7582\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 310.3776 - val_loss: 318.9541\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 308.4580 - val_loss: 317.4267\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 306.8286 - val_loss: 315.5668\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 304.8401 - val_loss: 314.5746\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 303.7315 - val_loss: 313.1433\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 302.7491 - val_loss: 310.8184\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 300.3284 - val_loss: 309.8163\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 297.9807 - val_loss: 308.1744\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 297.5556 - val_loss: 306.8804\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 295.0803 - val_loss: 305.5447\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 293.0352 - val_loss: 304.0895\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 292.4215 - val_loss: 302.8414\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 291.4371 - val_loss: 301.3434\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 289.7872 - val_loss: 300.4536\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 288.7341 - val_loss: 298.8212\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 287.1715 - val_loss: 297.9683\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 285.8819 - val_loss: 297.2267\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 283.8036 - val_loss: 295.4153\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 281.9279 - val_loss: 294.3229\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  15 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1132.8560 - val_loss: 942.8467\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 857.0817 - val_loss: 739.5756\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 666.6898 - val_loss: 594.0288\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 533.4819 - val_loss: 531.1144\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 444.7689 - val_loss: 415.1928\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 382.8395 - val_loss: 361.0228\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 344.2113 - val_loss: 325.0825\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 307.5135 - val_loss: 298.1305\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 290.2549 - val_loss: 291.8494\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 268.5237 - val_loss: 280.0794\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 251.5316 - val_loss: 250.2975\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 240.2149 - val_loss: 251.2919\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 232.4637 - val_loss: 248.0768\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 228.4718 - val_loss: 232.1309\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 224.0570 - val_loss: 235.8983\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.7703 - val_loss: 204.0248\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 200.8935 - val_loss: 211.6303\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.3753 - val_loss: 196.9696\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 182.7852 - val_loss: 188.3212\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.7845 - val_loss: 179.5094\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.2473 - val_loss: 180.9520\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.6078 - val_loss: 166.8561\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.5976 - val_loss: 163.5931\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.6290 - val_loss: 170.9669\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.0566 - val_loss: 182.5846\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.7325 - val_loss: 150.0566\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.4638 - val_loss: 145.6221\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 142.5772 - val_loss: 143.5046\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.1124 - val_loss: 164.4178\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.0707 - val_loss: 138.2496\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.1674 - val_loss: 136.5908\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.0607 - val_loss: 137.4241\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.2771 - val_loss: 132.7034\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.0163 - val_loss: 131.7528\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.2477 - val_loss: 128.2319\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.3037 - val_loss: 133.9962\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.9691 - val_loss: 127.6482\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.3714 - val_loss: 129.5912\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.6948 - val_loss: 125.2735\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.7336 - val_loss: 126.7933\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.1021 - val_loss: 121.1793\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.7688 - val_loss: 117.4399\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.1296 - val_loss: 126.5417\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.6032 - val_loss: 114.3039\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.6670 - val_loss: 119.4311\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.7662 - val_loss: 111.8298\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.5395 - val_loss: 110.4524\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.7312 - val_loss: 114.6884\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.2364 - val_loss: 108.1860\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.6019 - val_loss: 107.3675\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.2503 - val_loss: 107.0633\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.2222 - val_loss: 105.8148\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.2704 - val_loss: 105.7181\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.0519 - val_loss: 108.5457\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.7226 - val_loss: 101.1811\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.7783 - val_loss: 106.9391\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.7233 - val_loss: 104.3709\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.8137 - val_loss: 100.6910\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.4589 - val_loss: 111.8426\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.4811 - val_loss: 100.1072\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.8466 - val_loss: 110.7341\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.8115 - val_loss: 95.0504\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.0933 - val_loss: 94.4602\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.6159 - val_loss: 92.4470\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 90.6269 - val_loss: 93.0531\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.3338 - val_loss: 90.7088\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.2066 - val_loss: 91.1187\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.4294 - val_loss: 89.1494\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.4925 - val_loss: 87.9319\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.7493 - val_loss: 94.2330\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.0620 - val_loss: 93.1945\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.9746 - val_loss: 86.1483\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.2785 - val_loss: 86.8527\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.8084 - val_loss: 85.2956\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.6817 - val_loss: 96.3253\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.5791 - val_loss: 83.3030\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.2586 - val_loss: 94.5309\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.2142 - val_loss: 82.4625\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.3574 - val_loss: 82.0074\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.9931 - val_loss: 87.9385\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.1574 - val_loss: 84.8381\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.9288 - val_loss: 80.0290\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.0387 - val_loss: 83.1968\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.5603 - val_loss: 79.7086\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.7618 - val_loss: 84.5298\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.7133 - val_loss: 78.1473\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.4848 - val_loss: 77.9407\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.7481 - val_loss: 81.9625\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.6412 - val_loss: 78.9717\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.0783 - val_loss: 90.1106\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.5327 - val_loss: 76.5912\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 73.0265 - val_loss: 76.5408\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.4217 - val_loss: 79.4003\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.2845 - val_loss: 84.2241\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.6900 - val_loss: 78.7459\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 74.9480 - val_loss: 75.8002\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 71.8503 - val_loss: 75.8434\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 72.3848 - val_loss: 76.0732\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 71.4033 - val_loss: 76.0066\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 72.5271 - val_loss: 74.3994\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 773us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  16 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 200345.8438 - val_loss: 112600.7734\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 70021.7656 - val_loss: 31247.2793\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 16435.7480 - val_loss: 4988.2358\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2201.5725 - val_loss: 573.3915\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 439.2957 - val_loss: 566.9063\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 483.4763 - val_loss: 544.8722\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 437.3756 - val_loss: 490.0385\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 415.3086 - val_loss: 476.7018\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 409.4863 - val_loss: 471.3425\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 404.8593 - val_loss: 466.6849\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 399.4235 - val_loss: 463.8591\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 395.5721 - val_loss: 458.0214\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 391.2871 - val_loss: 453.1631\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 386.3055 - val_loss: 447.2993\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 381.9511 - val_loss: 443.0045\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 377.9980 - val_loss: 437.7970\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 373.4938 - val_loss: 434.2290\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 369.3628 - val_loss: 429.6102\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 367.0405 - val_loss: 424.4633\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 362.5837 - val_loss: 422.7958\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 358.5257 - val_loss: 417.2746\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 355.4691 - val_loss: 413.7719\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 351.5886 - val_loss: 409.8045\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 348.4786 - val_loss: 406.4599\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 346.5418 - val_loss: 405.0390\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 342.6392 - val_loss: 400.1099\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 340.3511 - val_loss: 397.5848\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 338.2307 - val_loss: 394.6863\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 334.7352 - val_loss: 392.0356\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 332.1247 - val_loss: 389.6930\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 330.3987 - val_loss: 387.4043\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 327.8814 - val_loss: 384.1717\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 325.6719 - val_loss: 382.2239\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 323.9141 - val_loss: 379.7468\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 321.4168 - val_loss: 377.5209\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 319.7532 - val_loss: 375.4786\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 318.7898 - val_loss: 375.1897\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 316.0643 - val_loss: 370.6819\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 314.3303 - val_loss: 369.7898\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 312.6796 - val_loss: 368.4025\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 310.8032 - val_loss: 365.3759\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 309.2581 - val_loss: 364.4347\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 307.9534 - val_loss: 364.0741\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 306.9698 - val_loss: 360.3109\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 304.7282 - val_loss: 359.7453\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 302.9026 - val_loss: 356.4279\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 302.3789 - val_loss: 356.0362\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 300.7029 - val_loss: 353.5319\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 298.4659 - val_loss: 351.9509\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 296.8171 - val_loss: 349.8819\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 295.7224 - val_loss: 348.8433\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 294.2120 - val_loss: 348.2253\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 294.5953 - val_loss: 345.1115\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 292.5569 - val_loss: 343.8516\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 290.0719 - val_loss: 342.1327\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 288.1130 - val_loss: 340.3340\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 286.7458 - val_loss: 338.9329\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 284.9982 - val_loss: 338.4696\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 283.5487 - val_loss: 335.5385\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 282.3516 - val_loss: 333.7019\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 280.9792 - val_loss: 331.8441\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 279.7776 - val_loss: 331.6100\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 279.6022 - val_loss: 328.3579\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 276.5543 - val_loss: 329.4918\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 276.2717 - val_loss: 326.4291\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 275.2328 - val_loss: 323.7466\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 272.6923 - val_loss: 323.8416\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 271.1945 - val_loss: 319.6770\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 269.7699 - val_loss: 320.9588\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 268.7310 - val_loss: 316.9884\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 267.3747 - val_loss: 315.9496\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 265.5320 - val_loss: 313.5675\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 265.7188 - val_loss: 313.0258\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 264.7917 - val_loss: 311.4417\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 261.3586 - val_loss: 307.9806\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 259.5882 - val_loss: 307.3671\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 258.7972 - val_loss: 307.0049\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 256.4745 - val_loss: 303.2514\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.4969 - val_loss: 302.2757\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 254.1046 - val_loss: 300.3520\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 252.3351 - val_loss: 298.3775\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 251.3912 - val_loss: 299.6885\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 249.9195 - val_loss: 295.0200\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 247.4021 - val_loss: 294.6837\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 247.0255 - val_loss: 293.4919\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.9875 - val_loss: 290.0007\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.7276 - val_loss: 290.2790\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 242.5423 - val_loss: 286.9390\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.8107 - val_loss: 286.5022\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.8612 - val_loss: 284.0321\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.8082 - val_loss: 282.3241\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.6146 - val_loss: 279.5494\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.1085 - val_loss: 278.1319\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 234.2521 - val_loss: 278.0060\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 232.2781 - val_loss: 275.0506\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 230.8439 - val_loss: 272.8536\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 229.1502 - val_loss: 273.1219\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 226.9524 - val_loss: 269.3924\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 225.8668 - val_loss: 268.8479\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 224.3148 - val_loss: 266.0623\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 667us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  17 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2875.7249 - val_loss: 1376.1115\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1114.9781 - val_loss: 1015.5204\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1031.3901 - val_loss: 906.5549\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 883.9279 - val_loss: 864.9386\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 801.4169 - val_loss: 761.0400\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 734.7599 - val_loss: 703.6671\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 673.8893 - val_loss: 653.4029\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 624.7963 - val_loss: 609.8232\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 581.6791 - val_loss: 569.1884\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 547.3253 - val_loss: 533.0756\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 516.7095 - val_loss: 496.0248\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 487.2049 - val_loss: 482.7553\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 459.3972 - val_loss: 448.6212\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 436.6561 - val_loss: 428.5140\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 413.1907 - val_loss: 410.8528\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 391.9409 - val_loss: 382.7499\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 372.9116 - val_loss: 367.6934\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 353.9094 - val_loss: 351.3875\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 335.7137 - val_loss: 329.7864\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 320.3199 - val_loss: 315.9532\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 306.6741 - val_loss: 302.3478\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 287.6076 - val_loss: 293.3226\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 271.9895 - val_loss: 275.3525\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 258.0435 - val_loss: 257.3647\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.6156 - val_loss: 251.0988\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 231.6279 - val_loss: 233.6309\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 221.1415 - val_loss: 221.3786\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 213.6223 - val_loss: 221.6176\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 200.0405 - val_loss: 197.6959\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 188.1682 - val_loss: 202.4318\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.6843 - val_loss: 180.3639\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 171.0837 - val_loss: 176.7162\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.7044 - val_loss: 170.1444\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.0400 - val_loss: 168.5731\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.5785 - val_loss: 159.2537\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.4372 - val_loss: 154.8571\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.1978 - val_loss: 157.2432\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.8182 - val_loss: 146.6796\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.8739 - val_loss: 156.0514\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.3131 - val_loss: 141.1936\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.8714 - val_loss: 138.7678\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.7391 - val_loss: 134.7758\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.8075 - val_loss: 133.5009\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.4952 - val_loss: 137.8663\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.5102 - val_loss: 127.6953\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.0676 - val_loss: 125.5604\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.2742 - val_loss: 124.4381\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 113.7802 - val_loss: 126.4440\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.7693 - val_loss: 119.3042\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.9977 - val_loss: 121.9141\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.0999 - val_loss: 119.2462\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.8683 - val_loss: 114.9963\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.0780 - val_loss: 114.5127\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.8130 - val_loss: 115.0063\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.6207 - val_loss: 108.7473\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.8311 - val_loss: 108.6601\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.0365 - val_loss: 107.2827\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.8997 - val_loss: 121.8439\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.4522 - val_loss: 109.6059\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.1823 - val_loss: 104.3826\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.1597 - val_loss: 101.9671\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.9809 - val_loss: 101.2598\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.6227 - val_loss: 99.5726\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.8854 - val_loss: 97.2744\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.6058 - val_loss: 98.0864\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.3289 - val_loss: 94.6196\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.7853 - val_loss: 96.9438\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.7439 - val_loss: 92.0784\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.4089 - val_loss: 90.7655\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.5439 - val_loss: 87.7004\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.1757 - val_loss: 86.8981\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.3163 - val_loss: 85.6320\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.9787 - val_loss: 89.4118\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.1446 - val_loss: 86.0283\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.3564 - val_loss: 82.8590\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.4278 - val_loss: 83.8530\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.1942 - val_loss: 81.1484\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.0471 - val_loss: 79.8853\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.9062 - val_loss: 79.9689\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.3488 - val_loss: 82.0607\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.5702 - val_loss: 83.9215\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.8129 - val_loss: 79.2937\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 74.5467 - val_loss: 77.9455\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 74.5734 - val_loss: 81.6332\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 73.1972 - val_loss: 77.9857\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 71.7247 - val_loss: 77.1524\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 70.6191 - val_loss: 79.9076\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 71.9506 - val_loss: 82.6994\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 72.6838 - val_loss: 76.3898\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 73.0430 - val_loss: 76.2324\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 69.9764 - val_loss: 75.6139\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 68.5469 - val_loss: 81.9323\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 70.3113 - val_loss: 74.9173\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 68.8675 - val_loss: 74.0484\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 67.6747 - val_loss: 74.5340\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 69.0032 - val_loss: 73.4735\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 66.3623 - val_loss: 77.3215\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 67.4242 - val_loss: 80.4159\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 71.3778 - val_loss: 72.6825\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 67.8962 - val_loss: 70.8078\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 557us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  18 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 51551.8906 - val_loss: 16662.3242\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 6717.3091 - val_loss: 864.2552\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 711.3494 - val_loss: 633.7659\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 721.5916 - val_loss: 488.9968\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 541.0215 - val_loss: 384.0454\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 469.5413 - val_loss: 370.3911\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 442.8532 - val_loss: 356.2246\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 418.9227 - val_loss: 343.9904\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 398.1363 - val_loss: 336.6448\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 379.8567 - val_loss: 328.8945\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 362.7834 - val_loss: 326.9857\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 353.7574 - val_loss: 325.9333\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 345.8860 - val_loss: 322.8158\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 339.3906 - val_loss: 320.4043\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 333.8774 - val_loss: 317.8229\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 327.5823 - val_loss: 311.7512\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 322.4568 - val_loss: 307.7800\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 316.7386 - val_loss: 306.3022\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 311.3299 - val_loss: 300.5676\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 306.2065 - val_loss: 296.3804\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 301.5508 - val_loss: 293.4261\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 296.8445 - val_loss: 289.1634\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 291.0389 - val_loss: 287.1477\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 287.8613 - val_loss: 281.6053\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 283.7724 - val_loss: 279.5129\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 278.9141 - val_loss: 276.5439\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 274.8293 - val_loss: 271.8383\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 270.4945 - val_loss: 268.5279\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 269.9733 - val_loss: 267.7140\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 263.6033 - val_loss: 262.2089\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 259.8441 - val_loss: 259.6342\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.5556 - val_loss: 256.3389\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 251.6804 - val_loss: 253.0768\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 247.8499 - val_loss: 250.1988\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.4233 - val_loss: 245.9040\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.7705 - val_loss: 242.3698\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.2761 - val_loss: 238.0116\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 233.3549 - val_loss: 232.5578\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 226.0022 - val_loss: 227.7739\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 220.3203 - val_loss: 221.8491\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 214.4635 - val_loss: 217.7612\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 209.2170 - val_loss: 213.2803\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 204.9722 - val_loss: 208.4141\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 201.8956 - val_loss: 204.9784\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 195.9160 - val_loss: 202.1182\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 191.9518 - val_loss: 199.1876\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.9159 - val_loss: 196.4890\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.3003 - val_loss: 194.0620\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 182.3903 - val_loss: 191.0277\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 178.8368 - val_loss: 188.2395\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.7014 - val_loss: 185.5055\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.4639 - val_loss: 181.9174\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.3282 - val_loss: 178.9374\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 167.3193 - val_loss: 175.6418\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.4695 - val_loss: 173.3977\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.9257 - val_loss: 172.6119\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.4470 - val_loss: 169.3298\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.6897 - val_loss: 167.2434\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.9139 - val_loss: 165.2479\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.3236 - val_loss: 163.2574\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.7398 - val_loss: 162.0378\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.3260 - val_loss: 160.0490\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.8985 - val_loss: 158.4515\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.0140 - val_loss: 157.0577\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.0921 - val_loss: 155.5711\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.0355 - val_loss: 154.2961\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.0698 - val_loss: 153.7460\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.9597 - val_loss: 151.9681\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.4522 - val_loss: 150.9886\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.3103 - val_loss: 150.0314\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.1523 - val_loss: 149.1728\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.4492 - val_loss: 148.8782\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.7892 - val_loss: 145.7417\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.1616 - val_loss: 144.7446\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.0182 - val_loss: 142.2647\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.6938 - val_loss: 141.3647\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.6810 - val_loss: 138.1862\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.6261 - val_loss: 135.7865\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.1397 - val_loss: 134.5732\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.6115 - val_loss: 130.9676\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.3137 - val_loss: 129.7705\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.1341 - val_loss: 127.7888\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.4032 - val_loss: 127.4269\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.4935 - val_loss: 126.1791\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.5370 - val_loss: 125.4935\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.3210 - val_loss: 123.1555\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.8842 - val_loss: 122.6897\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.3602 - val_loss: 122.7475\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.2755 - val_loss: 120.7529\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.9648 - val_loss: 120.9411\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.6693 - val_loss: 119.8463\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.5635 - val_loss: 119.0056\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.1932 - val_loss: 118.3887\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.0027 - val_loss: 119.0162\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.1204 - val_loss: 119.8865\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.7087 - val_loss: 117.8444\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.6580 - val_loss: 116.9387\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.8995 - val_loss: 116.3020\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.1703 - val_loss: 117.3948\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.2066 - val_loss: 116.3729\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 668us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  19 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 34934.2070 - val_loss: 21491.8184\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 14022.4639 - val_loss: 8434.7314\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5555.0474 - val_loss: 3536.8843\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2488.8330 - val_loss: 1852.5919\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1466.0654 - val_loss: 1286.3147\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1122.6207 - val_loss: 1080.4258\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 967.6337 - val_loss: 971.3619\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 894.0062 - val_loss: 900.1806\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 828.5936 - val_loss: 839.5701\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 754.1119 - val_loss: 746.4686\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 630.5322 - val_loss: 608.7260\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 493.0029 - val_loss: 479.7927\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 409.5983 - val_loss: 402.6374\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 350.9221 - val_loss: 354.6753\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 312.5228 - val_loss: 312.0189\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 280.9430 - val_loss: 273.2807\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 249.5172 - val_loss: 241.5464\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 228.1391 - val_loss: 215.0269\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 207.3341 - val_loss: 195.2489\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 190.3122 - val_loss: 179.7905\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 178.1550 - val_loss: 165.2508\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 167.4744 - val_loss: 154.4602\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.1724 - val_loss: 146.0113\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.0105 - val_loss: 139.2396\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.6860 - val_loss: 133.6211\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.8288 - val_loss: 130.3541\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.3418 - val_loss: 127.3117\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.9315 - val_loss: 123.0339\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.4607 - val_loss: 121.0261\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.7269 - val_loss: 119.6850\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.5406 - val_loss: 117.4190\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.8355 - val_loss: 116.4673\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.4481 - val_loss: 115.2644\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.1902 - val_loss: 113.8111\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.1202 - val_loss: 114.6677\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.3645 - val_loss: 115.3045\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.7905 - val_loss: 111.5843\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.2274 - val_loss: 113.3004\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.5210 - val_loss: 111.7817\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.6183 - val_loss: 111.6780\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.1101 - val_loss: 109.8505\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.7665 - val_loss: 109.5272\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.6460 - val_loss: 108.6382\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.9022 - val_loss: 108.7949\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.1036 - val_loss: 108.1738\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.2377 - val_loss: 107.7105\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.6755 - val_loss: 107.1044\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.4928 - val_loss: 106.7555\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.0292 - val_loss: 109.2619\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.0124 - val_loss: 106.8161\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.0523 - val_loss: 107.7338\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.3880 - val_loss: 105.4652\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.9086 - val_loss: 106.0257\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.3164 - val_loss: 104.3553\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 112.1714 - val_loss: 104.7516\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.0734 - val_loss: 105.3696\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.1571 - val_loss: 103.9270\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.5641 - val_loss: 104.3869\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.7802 - val_loss: 103.3208\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.9395 - val_loss: 102.3197\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.8563 - val_loss: 101.4990\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.7938 - val_loss: 101.4762\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.2936 - val_loss: 101.4670\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.0074 - val_loss: 99.8285\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.6068 - val_loss: 100.6539\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.5611 - val_loss: 99.2857\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.9982 - val_loss: 100.5567\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.7472 - val_loss: 98.1409\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.0289 - val_loss: 97.3016\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.7287 - val_loss: 97.2018\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.5321 - val_loss: 96.7132\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.2714 - val_loss: 96.2626\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.1117 - val_loss: 95.7823\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.2105 - val_loss: 95.9883\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.2641 - val_loss: 98.1610\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.8215 - val_loss: 94.4530\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.0276 - val_loss: 94.2450\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.0746 - val_loss: 94.0142\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.5477 - val_loss: 93.3705\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.7501 - val_loss: 93.3598\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.3937 - val_loss: 92.9375\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.2827 - val_loss: 92.7616\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.0925 - val_loss: 93.1505\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.4731 - val_loss: 91.9210\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.2257 - val_loss: 92.7466\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.4715 - val_loss: 91.7171\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.0483 - val_loss: 91.5346\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.8734 - val_loss: 91.1988\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.9974 - val_loss: 91.1910\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.5911 - val_loss: 96.6073\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.1482 - val_loss: 90.6296\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.4076 - val_loss: 92.6249\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.8258 - val_loss: 91.2078\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.2521 - val_loss: 90.9268\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.4348 - val_loss: 89.9744\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.4043 - val_loss: 89.7111\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.5170 - val_loss: 95.0972\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.4203 - val_loss: 89.6656\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.8451 - val_loss: 89.3298\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.8228 - val_loss: 90.1451\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  20 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4729.5293 - val_loss: 1763.2469\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1435.7710 - val_loss: 1546.2786\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1255.1974 - val_loss: 1224.7766\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1008.1246 - val_loss: 1039.6235\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 839.4887 - val_loss: 873.0799\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 702.5090 - val_loss: 727.8934\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 584.5302 - val_loss: 609.3026\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 477.1955 - val_loss: 490.9157\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 389.4051 - val_loss: 428.7747\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 349.0762 - val_loss: 385.7125\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 320.5496 - val_loss: 355.7755\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 295.8176 - val_loss: 329.2970\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 273.0559 - val_loss: 305.1712\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 253.0231 - val_loss: 290.7810\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.8577 - val_loss: 266.5951\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.4529 - val_loss: 249.6281\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.6088 - val_loss: 232.3589\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 190.3056 - val_loss: 221.3905\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.8258 - val_loss: 209.7988\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.6096 - val_loss: 201.4574\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.3065 - val_loss: 182.4979\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.0695 - val_loss: 178.1540\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.9476 - val_loss: 163.4955\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.4733 - val_loss: 154.8022\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.4749 - val_loss: 147.6123\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.0336 - val_loss: 141.6872\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.6355 - val_loss: 136.8700\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.2368 - val_loss: 128.5410\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.6551 - val_loss: 122.8030\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.2767 - val_loss: 119.1051\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.1148 - val_loss: 115.3292\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.3414 - val_loss: 111.6153\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.2248 - val_loss: 106.5137\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.0876 - val_loss: 103.4118\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.0872 - val_loss: 100.6308\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.4988 - val_loss: 98.0579\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.3255 - val_loss: 97.1926\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.8026 - val_loss: 95.2147\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.5532 - val_loss: 91.9991\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.3405 - val_loss: 90.6772\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.0289 - val_loss: 89.6966\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.0830 - val_loss: 96.4951\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.3354 - val_loss: 94.0922\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.6275 - val_loss: 87.9308\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.2120 - val_loss: 85.1664\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.6694 - val_loss: 84.0144\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.3247 - val_loss: 84.1831\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.7720 - val_loss: 83.3815\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.3095 - val_loss: 82.2100\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 74.1567 - val_loss: 84.2527\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.5711 - val_loss: 91.3282\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.9674 - val_loss: 81.2414\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.5501 - val_loss: 82.2496\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 74.9516 - val_loss: 82.1612\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 74.1942 - val_loss: 80.4396\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 73.1142 - val_loss: 79.6482\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 74.1648 - val_loss: 78.7861\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 72.8284 - val_loss: 78.8096\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 72.2789 - val_loss: 79.4311\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 72.2452 - val_loss: 77.9775\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 72.2932 - val_loss: 79.6948\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 72.9614 - val_loss: 86.0071\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 72.9635 - val_loss: 83.7713\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 72.6648 - val_loss: 77.1912\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 70.3428 - val_loss: 76.3978\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 70.3200 - val_loss: 76.0252\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 70.2648 - val_loss: 76.2471\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 71.1242 - val_loss: 76.0183\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 69.9137 - val_loss: 77.4511\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 69.3995 - val_loss: 79.6117\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 71.1906 - val_loss: 75.7988\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 70.4452 - val_loss: 75.5120\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 70.0843 - val_loss: 75.3665\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 69.3613 - val_loss: 74.3849\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 69.1235 - val_loss: 74.3892\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 69.2497 - val_loss: 74.0707\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 69.4269 - val_loss: 74.4528\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 69.7611 - val_loss: 76.3277\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 72.1408 - val_loss: 75.4012\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 69.3310 - val_loss: 73.8649\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 68.4389 - val_loss: 75.2462\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 68.0884 - val_loss: 73.4842\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 68.3918 - val_loss: 73.4529\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 66.9766 - val_loss: 75.4832\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 69.3536 - val_loss: 73.4802\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 67.3881 - val_loss: 73.1802\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 67.1292 - val_loss: 81.9569\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 67.4600 - val_loss: 74.2873\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 66.6271 - val_loss: 72.6008\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 66.8886 - val_loss: 73.3783\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 66.7677 - val_loss: 72.2636\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 69.3887 - val_loss: 74.2538\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 66.3912 - val_loss: 72.0627\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 65.4660 - val_loss: 74.5428\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 66.2371 - val_loss: 74.2432\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 65.9035 - val_loss: 71.6423\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 64.8086 - val_loss: 71.5499\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 64.6799 - val_loss: 71.9188\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 65.4049 - val_loss: 73.1163\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 66.7940 - val_loss: 71.3490\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 666us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  21 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 54405.2500 - val_loss: 23716.2969\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12318.1367 - val_loss: 3565.9004\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1867.4449 - val_loss: 1100.6761\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1104.8817 - val_loss: 1203.0841\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1083.1062 - val_loss: 1063.7595\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 991.0726 - val_loss: 983.1467\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 950.0688 - val_loss: 944.2355\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 913.2005 - val_loss: 890.2722\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 861.0330 - val_loss: 829.7235\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 825.4683 - val_loss: 789.6278\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 793.4703 - val_loss: 758.6922\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 764.8228 - val_loss: 739.2693\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 740.9811 - val_loss: 706.5430\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 713.9083 - val_loss: 685.9304\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 690.5043 - val_loss: 659.2336\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 663.4880 - val_loss: 634.4691\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 634.6268 - val_loss: 607.2811\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 606.3725 - val_loss: 575.6755\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 580.8598 - val_loss: 556.0062\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 562.6447 - val_loss: 533.8593\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 544.4257 - val_loss: 524.6547\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 523.5267 - val_loss: 497.4846\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 507.2849 - val_loss: 482.2246\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 490.5236 - val_loss: 462.7757\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 476.6508 - val_loss: 454.6564\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 457.0612 - val_loss: 432.8049\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 443.9824 - val_loss: 420.3412\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 431.4763 - val_loss: 409.1193\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 416.6533 - val_loss: 395.4505\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 405.0348 - val_loss: 381.6818\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 394.2129 - val_loss: 375.9407\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 383.9511 - val_loss: 360.0118\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 373.0411 - val_loss: 348.9050\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 361.9251 - val_loss: 337.2027\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 352.0891 - val_loss: 329.1150\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 344.3412 - val_loss: 317.3446\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 333.7745 - val_loss: 314.1352\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 325.3585 - val_loss: 301.6891\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 316.0473 - val_loss: 291.1662\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 308.7248 - val_loss: 286.3077\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 296.4824 - val_loss: 271.9667\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 285.5556 - val_loss: 261.6922\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 272.5519 - val_loss: 251.3197\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 259.8459 - val_loss: 242.2409\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.4695 - val_loss: 225.8161\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 229.4625 - val_loss: 215.6014\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.6348 - val_loss: 203.0667\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 206.3852 - val_loss: 192.4934\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.5367 - val_loss: 183.3713\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 186.6526 - val_loss: 175.2178\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.9473 - val_loss: 164.5219\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 171.9777 - val_loss: 172.5592\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.8174 - val_loss: 153.2574\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.2204 - val_loss: 145.7375\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.3790 - val_loss: 142.9978\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.3878 - val_loss: 139.9887\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.3325 - val_loss: 130.3636\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.8684 - val_loss: 126.8013\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.3577 - val_loss: 128.1862\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.5138 - val_loss: 121.4570\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.5558 - val_loss: 115.6958\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.8077 - val_loss: 113.6647\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.2734 - val_loss: 112.7594\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.8327 - val_loss: 111.4900\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.6736 - val_loss: 113.0465\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.0633 - val_loss: 107.7203\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.1166 - val_loss: 106.4235\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.5456 - val_loss: 105.4134\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.3594 - val_loss: 105.7675\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.8609 - val_loss: 103.2645\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.4394 - val_loss: 102.3831\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.2076 - val_loss: 101.5910\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.9453 - val_loss: 101.7256\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.3116 - val_loss: 100.8100\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.2696 - val_loss: 99.8187\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.4469 - val_loss: 99.7283\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.7866 - val_loss: 99.8240\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.1983 - val_loss: 99.1917\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.3621 - val_loss: 98.6153\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.6992 - val_loss: 97.4988\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.8800 - val_loss: 99.0255\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.4194 - val_loss: 99.6521\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.6059 - val_loss: 97.0293\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.2077 - val_loss: 96.6560\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.3987 - val_loss: 96.3433\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.7567 - val_loss: 96.2618\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.6797 - val_loss: 95.8952\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.9492 - val_loss: 95.3055\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.4297 - val_loss: 103.3462\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.7876 - val_loss: 97.2612\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.2624 - val_loss: 99.1211\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.7499 - val_loss: 94.6871\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.9107 - val_loss: 94.0354\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.0386 - val_loss: 94.9151\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.0531 - val_loss: 94.4836\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.6628 - val_loss: 100.9895\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.1028 - val_loss: 96.7084\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.1812 - val_loss: 93.0248\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.4306 - val_loss: 92.2061\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.7850 - val_loss: 94.0386\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 662us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  22 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 133892.3750 - val_loss: 106954.4141\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91927.9922 - val_loss: 70360.9766\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 58736.0430 - val_loss: 42849.6680\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 34685.9805 - val_loss: 23815.4902\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 18636.8008 - val_loss: 11947.8652\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 9032.7412 - val_loss: 5292.5117\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3875.1423 - val_loss: 2181.5898\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1611.2299 - val_loss: 973.4969\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 782.5303 - val_loss: 599.7244\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 557.9587 - val_loss: 497.2578\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 485.2985 - val_loss: 467.6116\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 461.4406 - val_loss: 446.7831\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 441.6066 - val_loss: 426.2414\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 423.1294 - val_loss: 405.3407\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 405.2404 - val_loss: 386.1032\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 388.0756 - val_loss: 367.3624\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 371.0549 - val_loss: 350.2290\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 355.3602 - val_loss: 333.1043\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 339.9766 - val_loss: 317.7564\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 325.9381 - val_loss: 302.8383\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 312.1847 - val_loss: 290.2045\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 299.8269 - val_loss: 279.2367\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 289.2641 - val_loss: 268.5403\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 279.3549 - val_loss: 259.3595\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 270.4316 - val_loss: 251.5957\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.7228 - val_loss: 244.9981\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.8432 - val_loss: 238.7181\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 249.7302 - val_loss: 233.3043\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.0997 - val_loss: 228.7085\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.5202 - val_loss: 224.0439\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 234.6797 - val_loss: 220.9149\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 231.2147 - val_loss: 217.6170\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 227.6185 - val_loss: 215.7317\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 224.8122 - val_loss: 213.0524\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.3570 - val_loss: 211.3582\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 220.5532 - val_loss: 208.9254\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 218.0623 - val_loss: 208.3175\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 216.4386 - val_loss: 207.4072\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 215.0117 - val_loss: 206.6628\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 214.0195 - val_loss: 205.4971\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 212.7554 - val_loss: 205.0040\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 211.8917 - val_loss: 204.4602\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 211.2805 - val_loss: 204.4735\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 210.2962 - val_loss: 203.5687\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 209.9560 - val_loss: 202.9375\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 209.0396 - val_loss: 202.9999\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.8524 - val_loss: 203.4668\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.2019 - val_loss: 202.5736\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 207.9259 - val_loss: 201.6313\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 207.3091 - val_loss: 201.8966\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 207.5262 - val_loss: 202.6063\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 207.0688 - val_loss: 201.3849\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 206.2986 - val_loss: 202.0637\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 206.1969 - val_loss: 202.1398\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 205.7040 - val_loss: 201.0026\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 205.5247 - val_loss: 201.1906\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 205.1552 - val_loss: 201.2692\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 204.9601 - val_loss: 201.6338\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 204.7778 - val_loss: 200.7593\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 204.4602 - val_loss: 200.9627\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 204.2365 - val_loss: 200.5989\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.8847 - val_loss: 201.2984\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.9207 - val_loss: 201.5996\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.5856 - val_loss: 200.6821\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.5668 - val_loss: 200.4338\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.2408 - val_loss: 199.6582\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.5899 - val_loss: 201.1673\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.0372 - val_loss: 199.5663\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.9322 - val_loss: 200.9794\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.3506 - val_loss: 200.1878\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.1805 - val_loss: 199.4230\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.1452 - val_loss: 200.1210\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 201.8925 - val_loss: 199.9799\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 201.9347 - val_loss: 199.2455\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 201.5855 - val_loss: 199.6758\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 201.6073 - val_loss: 199.2118\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 201.3136 - val_loss: 199.9543\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 201.3031 - val_loss: 198.9734\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 201.2319 - val_loss: 199.0444\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 201.0797 - val_loss: 198.7739\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 200.6026 - val_loss: 200.1283\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 200.5883 - val_loss: 198.7653\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 200.6599 - val_loss: 198.1403\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 200.6256 - val_loss: 199.5148\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 200.0843 - val_loss: 197.9997\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.9280 - val_loss: 197.8934\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.7469 - val_loss: 197.9276\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.6390 - val_loss: 197.8792\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.5871 - val_loss: 198.4678\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.6010 - val_loss: 197.4064\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.1253 - val_loss: 197.7180\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.2124 - val_loss: 197.5901\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.1584 - val_loss: 197.1851\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.7448 - val_loss: 198.5541\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.6370 - val_loss: 197.2950\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.4097 - val_loss: 196.4176\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.6133 - val_loss: 196.1527\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.0444 - val_loss: 197.3340\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.0546 - val_loss: 196.9935\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 197.9873 - val_loss: 195.5806\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  23 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 22783.2969 - val_loss: 4340.2544\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1839.6765 - val_loss: 1697.3536\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1587.3656 - val_loss: 1131.6974\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 988.7035 - val_loss: 920.6548\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 935.9124 - val_loss: 855.3679\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 860.1261 - val_loss: 808.3832\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 807.7143 - val_loss: 754.8270\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 753.0448 - val_loss: 702.2852\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 701.5780 - val_loss: 641.6189\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 644.5790 - val_loss: 580.8184\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 583.8468 - val_loss: 509.3474\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 513.8920 - val_loss: 438.6152\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 449.7649 - val_loss: 389.0471\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 399.2302 - val_loss: 353.5122\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 365.0275 - val_loss: 325.1436\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 337.8628 - val_loss: 306.8512\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 318.1841 - val_loss: 287.7705\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 298.7470 - val_loss: 269.3370\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 284.8350 - val_loss: 259.8487\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 272.2318 - val_loss: 246.8029\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 263.9896 - val_loss: 238.7908\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 254.7960 - val_loss: 229.7519\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 247.6357 - val_loss: 225.5439\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 242.1655 - val_loss: 216.9574\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 234.9090 - val_loss: 212.3812\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 232.3842 - val_loss: 210.1001\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 226.2560 - val_loss: 201.1821\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 218.2965 - val_loss: 198.4039\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 216.5038 - val_loss: 196.3096\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 213.7099 - val_loss: 189.6678\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.6052 - val_loss: 185.4766\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.5518 - val_loss: 182.7848\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.0730 - val_loss: 180.0029\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.6131 - val_loss: 176.1037\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.8703 - val_loss: 171.9077\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.8060 - val_loss: 175.0983\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 189.6084 - val_loss: 167.2079\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.4376 - val_loss: 163.7353\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 182.2466 - val_loss: 162.6320\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.6774 - val_loss: 159.5465\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.3417 - val_loss: 157.0587\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.8702 - val_loss: 156.8315\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.1611 - val_loss: 151.8249\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.0868 - val_loss: 160.1588\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.5940 - val_loss: 149.9179\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.1470 - val_loss: 147.9194\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.3852 - val_loss: 153.6786\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 166.3173 - val_loss: 143.7345\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.8268 - val_loss: 142.2280\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.6728 - val_loss: 140.0413\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.8165 - val_loss: 138.1803\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.9615 - val_loss: 140.1319\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.2812 - val_loss: 137.0668\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.5520 - val_loss: 134.2299\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.1844 - val_loss: 133.9268\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.8020 - val_loss: 132.0499\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.2406 - val_loss: 133.0850\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.6734 - val_loss: 131.6918\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.2559 - val_loss: 128.0049\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.1432 - val_loss: 126.9300\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.0375 - val_loss: 126.7646\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.5499 - val_loss: 124.3342\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.6221 - val_loss: 126.9950\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.8893 - val_loss: 122.1157\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.2079 - val_loss: 121.2367\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.9008 - val_loss: 137.6198\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.1803 - val_loss: 119.6326\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.1375 - val_loss: 119.2986\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.1165 - val_loss: 117.1546\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.8674 - val_loss: 116.5902\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.3669 - val_loss: 116.2001\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.2120 - val_loss: 119.1257\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.0870 - val_loss: 113.9345\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.0918 - val_loss: 116.6827\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.1551 - val_loss: 114.6744\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.1675 - val_loss: 111.8259\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.9433 - val_loss: 111.0162\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.8873 - val_loss: 112.0503\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.6265 - val_loss: 110.4368\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.9991 - val_loss: 109.3935\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.9815 - val_loss: 108.9149\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.0103 - val_loss: 111.3728\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.0727 - val_loss: 111.1916\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.9733 - val_loss: 107.7709\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.9435 - val_loss: 107.2419\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.9426 - val_loss: 106.0489\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.3232 - val_loss: 105.6734\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.0946 - val_loss: 108.7701\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.4431 - val_loss: 108.4540\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.4619 - val_loss: 105.0572\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.5931 - val_loss: 104.2994\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.0482 - val_loss: 104.2636\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.1639 - val_loss: 103.3955\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.6392 - val_loss: 105.3579\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.3929 - val_loss: 102.5751\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.3968 - val_loss: 103.9137\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.2567 - val_loss: 101.8102\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.3472 - val_loss: 101.8319\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.9369 - val_loss: 101.2240\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.2292 - val_loss: 102.8592\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  24 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1314.0947 - val_loss: 663.0994\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 597.9752 - val_loss: 567.8459\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 461.4604 - val_loss: 470.3107\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 415.7326 - val_loss: 404.4627\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 369.7758 - val_loss: 362.4196\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 334.6871 - val_loss: 320.5287\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 299.9821 - val_loss: 288.3254\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 271.7162 - val_loss: 261.8589\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 248.3390 - val_loss: 238.9223\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 227.8890 - val_loss: 217.1445\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 210.7274 - val_loss: 202.2274\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.5455 - val_loss: 187.8341\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 183.7402 - val_loss: 173.4232\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.3655 - val_loss: 165.6087\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.8378 - val_loss: 160.0984\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.3113 - val_loss: 149.5645\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.2250 - val_loss: 142.8068\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.8601 - val_loss: 138.3702\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.0654 - val_loss: 134.4948\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.0666 - val_loss: 130.8690\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.4578 - val_loss: 127.6316\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.8955 - val_loss: 131.3318\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.0307 - val_loss: 123.6253\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.3568 - val_loss: 120.2261\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.1262 - val_loss: 118.9526\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.1932 - val_loss: 116.4839\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.1966 - val_loss: 114.8739\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.8034 - val_loss: 118.0659\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.3540 - val_loss: 112.4124\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.6239 - val_loss: 110.2534\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.3365 - val_loss: 112.5933\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.2625 - val_loss: 109.3756\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.0091 - val_loss: 119.8711\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.8597 - val_loss: 109.0138\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.5316 - val_loss: 103.8255\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.2602 - val_loss: 102.7514\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.1189 - val_loss: 101.9116\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.6099 - val_loss: 101.2942\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.8806 - val_loss: 100.5390\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.6596 - val_loss: 100.1817\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.8294 - val_loss: 98.1826\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.1187 - val_loss: 97.0989\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.2575 - val_loss: 96.6194\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.8652 - val_loss: 101.0138\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.4245 - val_loss: 96.9668\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.7118 - val_loss: 96.4058\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.9427 - val_loss: 93.6203\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.7246 - val_loss: 93.0790\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.2188 - val_loss: 96.0543\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.2360 - val_loss: 91.5449\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 85.9763 - val_loss: 91.1772\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.7203 - val_loss: 90.9304\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.6701 - val_loss: 92.2245\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.1330 - val_loss: 90.3535\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.1150 - val_loss: 95.6469\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.4008 - val_loss: 89.6486\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.4760 - val_loss: 89.9868\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.4057 - val_loss: 88.4884\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.1202 - val_loss: 91.5623\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.6583 - val_loss: 87.3943\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.7239 - val_loss: 89.5895\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.0417 - val_loss: 87.4556\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.0872 - val_loss: 90.4619\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.1120 - val_loss: 88.8103\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.9813 - val_loss: 91.4379\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.9501 - val_loss: 85.2289\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.8815 - val_loss: 86.0459\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.5240 - val_loss: 87.3605\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.5092 - val_loss: 84.4601\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.5827 - val_loss: 84.3785\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.8244 - val_loss: 83.4673\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.6227 - val_loss: 83.2866\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.6059 - val_loss: 86.1304\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.5399 - val_loss: 86.9969\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.0350 - val_loss: 82.8974\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.2051 - val_loss: 82.3626\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.0493 - val_loss: 82.5092\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.4073 - val_loss: 86.9336\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.4924 - val_loss: 87.5090\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.8677 - val_loss: 82.3055\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.0220 - val_loss: 83.6230\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.7957 - val_loss: 81.8692\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.1408 - val_loss: 81.8853\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.8702 - val_loss: 84.8569\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 74.4444 - val_loss: 81.5843\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.5543 - val_loss: 80.8185\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.9854 - val_loss: 80.3735\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.2243 - val_loss: 80.6602\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.9296 - val_loss: 80.1683\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.3600 - val_loss: 90.6839\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.7703 - val_loss: 79.8067\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.4236 - val_loss: 81.5473\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.1102 - val_loss: 86.9153\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.9718 - val_loss: 86.2346\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.6079 - val_loss: 83.6170\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.4440 - val_loss: 83.2310\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 74.4792 - val_loss: 80.2945\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 74.5654 - val_loss: 79.7004\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.1275 - val_loss: 81.7673\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 74.7943 - val_loss: 79.6199\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 664us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  25 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2486.1523 - val_loss: 1832.6624\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1420.4659 - val_loss: 1209.5623\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 972.8098 - val_loss: 865.4768\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 743.3101 - val_loss: 636.5283\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 584.1982 - val_loss: 503.0544\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 495.4692 - val_loss: 433.4049\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 443.9375 - val_loss: 389.5166\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 411.5971 - val_loss: 362.7360\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 386.0552 - val_loss: 339.7942\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 372.1747 - val_loss: 323.9438\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 352.4301 - val_loss: 315.9784\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 339.6808 - val_loss: 304.5471\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 328.7103 - val_loss: 293.5377\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 317.0647 - val_loss: 288.4184\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 307.8992 - val_loss: 282.1283\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 301.5685 - val_loss: 276.5913\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 292.7932 - val_loss: 273.4319\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 285.9966 - val_loss: 268.3422\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 278.0767 - val_loss: 264.0491\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 272.2465 - val_loss: 254.5804\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 264.0380 - val_loss: 251.2348\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 257.5963 - val_loss: 241.6026\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 252.2595 - val_loss: 234.6243\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.9556 - val_loss: 228.4823\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.1635 - val_loss: 222.1456\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 232.0408 - val_loss: 210.2533\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 224.8054 - val_loss: 204.0932\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 215.7048 - val_loss: 192.0918\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 212.1933 - val_loss: 185.1271\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.6724 - val_loss: 180.1664\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.3281 - val_loss: 175.2827\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 191.6104 - val_loss: 169.0306\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.1398 - val_loss: 164.6987\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 182.5888 - val_loss: 159.0417\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.8657 - val_loss: 155.4971\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 169.6630 - val_loss: 151.8866\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 166.3660 - val_loss: 149.3413\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.9255 - val_loss: 144.5630\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.4637 - val_loss: 147.2986\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.1293 - val_loss: 139.9534\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.6232 - val_loss: 141.3934\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.6268 - val_loss: 136.5345\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.8526 - val_loss: 144.1639\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.2123 - val_loss: 129.0369\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.7619 - val_loss: 127.1785\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.3424 - val_loss: 131.9588\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.9240 - val_loss: 129.8351\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.9890 - val_loss: 123.7386\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.8864 - val_loss: 122.5201\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.9584 - val_loss: 120.1894\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.2229 - val_loss: 119.6481\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.9709 - val_loss: 121.0652\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.8469 - val_loss: 121.6673\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.7520 - val_loss: 116.7803\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.4602 - val_loss: 115.2610\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.6453 - val_loss: 115.5569\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.3949 - val_loss: 114.5209\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.9620 - val_loss: 113.6575\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.6591 - val_loss: 112.7150\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.1910 - val_loss: 113.0678\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.4830 - val_loss: 110.3875\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.5723 - val_loss: 109.4845\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.6593 - val_loss: 107.2238\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.0912 - val_loss: 107.3308\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.0932 - val_loss: 124.2883\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.7369 - val_loss: 104.0720\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.4843 - val_loss: 103.9859\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.8753 - val_loss: 101.8998\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.3872 - val_loss: 99.6381\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.5302 - val_loss: 102.8929\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.9062 - val_loss: 98.9459\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.9694 - val_loss: 98.2265\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.6429 - val_loss: 96.1514\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.9470 - val_loss: 96.2912\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.2952 - val_loss: 101.4086\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.0837 - val_loss: 94.1875\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.1314 - val_loss: 93.3851\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.4097 - val_loss: 101.0776\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.4744 - val_loss: 90.5583\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.9952 - val_loss: 88.7194\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.6316 - val_loss: 88.0400\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.8355 - val_loss: 90.3948\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.1040 - val_loss: 89.0734\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.6316 - val_loss: 86.5338\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.1735 - val_loss: 85.7856\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.5144 - val_loss: 86.1264\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.3759 - val_loss: 84.6746\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.8188 - val_loss: 83.0052\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.7071 - val_loss: 82.8955\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.9721 - val_loss: 82.9085\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.1159 - val_loss: 80.9007\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.0397 - val_loss: 79.9621\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.0341 - val_loss: 79.6307\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.0723 - val_loss: 79.9604\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.2040 - val_loss: 79.5499\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.4987 - val_loss: 80.2654\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.1594 - val_loss: 81.3730\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.5808 - val_loss: 77.4290\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.1257 - val_loss: 77.5575\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.8022 - val_loss: 77.3049\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  26 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 359369.7812 - val_loss: 268945.4062\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 204559.6719 - val_loss: 136348.6406\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87975.8125 - val_loss: 42252.5898\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 20191.9121 - val_loss: 6137.8462\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4328.6812 - val_loss: 3891.4917\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4146.8364 - val_loss: 3273.8335\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3276.6011 - val_loss: 2717.2593\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2850.2454 - val_loss: 2407.9963\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2514.9023 - val_loss: 2092.8696\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2215.3635 - val_loss: 1813.9248\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1937.2131 - val_loss: 1588.5538\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1698.8521 - val_loss: 1391.5614\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1499.7213 - val_loss: 1211.9520\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1312.9386 - val_loss: 1066.9545\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1152.5659 - val_loss: 938.8376\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1022.7061 - val_loss: 824.9042\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 913.4884 - val_loss: 733.7141\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 804.4591 - val_loss: 651.7036\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 716.0324 - val_loss: 586.6060\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 640.2101 - val_loss: 529.5856\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 575.8305 - val_loss: 481.7090\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 522.0001 - val_loss: 440.0889\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 476.5506 - val_loss: 405.8382\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 433.3591 - val_loss: 376.3159\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 399.0332 - val_loss: 350.4831\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 368.5968 - val_loss: 328.5384\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 342.4203 - val_loss: 310.2017\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 318.7664 - val_loss: 293.9352\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 299.3080 - val_loss: 280.2071\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 281.8683 - val_loss: 267.7141\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 267.7936 - val_loss: 256.8001\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 253.3337 - val_loss: 247.3892\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 242.2837 - val_loss: 238.7403\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 231.2790 - val_loss: 230.9921\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 221.9883 - val_loss: 224.2305\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 213.5347 - val_loss: 217.8947\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 206.0178 - val_loss: 211.7050\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.5829 - val_loss: 206.3045\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.5088 - val_loss: 201.0049\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.2835 - val_loss: 195.8945\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.8091 - val_loss: 191.4442\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.0300 - val_loss: 187.4267\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 172.7180 - val_loss: 183.6323\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.6924 - val_loss: 180.1249\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.5943 - val_loss: 177.2240\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.8423 - val_loss: 174.5592\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.8469 - val_loss: 172.1208\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.5904 - val_loss: 169.9916\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.8181 - val_loss: 168.1760\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.3542 - val_loss: 165.8505\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.3863 - val_loss: 164.0417\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.6772 - val_loss: 162.1910\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.8576 - val_loss: 160.7310\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.7930 - val_loss: 159.2865\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.8411 - val_loss: 157.8022\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.2443 - val_loss: 156.4320\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.2191 - val_loss: 155.1472\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.8529 - val_loss: 153.7771\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.4984 - val_loss: 152.7166\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.5609 - val_loss: 151.6345\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.5690 - val_loss: 150.8210\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.5118 - val_loss: 150.0035\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.5637 - val_loss: 149.0032\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.2901 - val_loss: 148.1879\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.4707 - val_loss: 147.2710\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.7714 - val_loss: 146.3330\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.1601 - val_loss: 145.6730\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.7155 - val_loss: 145.1305\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.4654 - val_loss: 144.4339\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.1109 - val_loss: 143.8993\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.0033 - val_loss: 143.4491\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.8199 - val_loss: 142.9490\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.9049 - val_loss: 142.3886\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.4413 - val_loss: 142.1444\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.5695 - val_loss: 141.7981\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.5305 - val_loss: 141.4625\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.3258 - val_loss: 140.7605\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.4463 - val_loss: 140.3813\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.9998 - val_loss: 140.1655\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.8369 - val_loss: 139.7197\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.5336 - val_loss: 139.8442\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.1866 - val_loss: 139.2163\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 122.8178 - val_loss: 139.4268\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.7426 - val_loss: 138.7467\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.1336 - val_loss: 139.0721\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.0538 - val_loss: 138.3827\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.1156 - val_loss: 137.8203\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.7605 - val_loss: 137.6015\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.8271 - val_loss: 137.5159\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.4385 - val_loss: 137.4141\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.6634 - val_loss: 137.0205\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.8111 - val_loss: 137.0669\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.3987 - val_loss: 136.8344\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.1685 - val_loss: 136.6978\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.8460 - val_loss: 136.8545\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.2993 - val_loss: 136.3910\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.0294 - val_loss: 136.2993\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.3354 - val_loss: 136.4063\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.8805 - val_loss: 136.3732\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.9937 - val_loss: 135.8675\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 557us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  27 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 298740.6250 - val_loss: 207055.5781\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153671.3594 - val_loss: 100298.5156\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 69241.5078 - val_loss: 39545.3438\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 23697.0664 - val_loss: 9809.1738\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4605.8047 - val_loss: 1388.1055\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1460.9996 - val_loss: 1406.2212\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1242.7161 - val_loss: 1031.4945\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 981.5683 - val_loss: 884.7137\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 845.9346 - val_loss: 750.4489\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 733.9897 - val_loss: 657.0546\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 653.3153 - val_loss: 578.8257\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 589.1328 - val_loss: 525.5164\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 528.7647 - val_loss: 472.6121\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 482.6273 - val_loss: 433.9736\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 440.5917 - val_loss: 398.9110\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 410.4616 - val_loss: 367.3463\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 381.1805 - val_loss: 342.4219\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 354.9987 - val_loss: 323.7518\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 333.6293 - val_loss: 304.2241\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 316.1467 - val_loss: 289.1473\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 299.1367 - val_loss: 275.9266\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 285.9167 - val_loss: 263.9128\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 273.1284 - val_loss: 254.1432\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 263.5980 - val_loss: 246.1992\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 253.4464 - val_loss: 238.7365\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 246.4109 - val_loss: 232.3762\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.6156 - val_loss: 226.3837\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 231.8207 - val_loss: 221.6133\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 225.9361 - val_loss: 217.5976\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 223.0077 - val_loss: 214.2483\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.1094 - val_loss: 210.5742\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 212.6620 - val_loss: 206.7216\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 209.7850 - val_loss: 204.6583\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 205.9177 - val_loss: 201.1229\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.9087 - val_loss: 200.7451\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.9038 - val_loss: 198.9045\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.8470 - val_loss: 196.5525\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 195.7765 - val_loss: 191.8545\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.6067 - val_loss: 189.7289\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.6399 - val_loss: 190.8698\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.6278 - val_loss: 187.8762\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 189.2867 - val_loss: 187.2120\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 184.7946 - val_loss: 183.2484\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 183.1175 - val_loss: 180.1702\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.1750 - val_loss: 179.2468\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.7207 - val_loss: 178.2460\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.9588 - val_loss: 175.9643\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 175.2149 - val_loss: 174.3732\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.0413 - val_loss: 173.7722\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 171.9871 - val_loss: 171.3973\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.0488 - val_loss: 170.3236\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.7232 - val_loss: 170.6775\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 166.7441 - val_loss: 167.8368\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 166.9609 - val_loss: 167.7829\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.9126 - val_loss: 164.7515\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.1853 - val_loss: 163.4018\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.5728 - val_loss: 162.9760\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.7683 - val_loss: 161.1470\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.9957 - val_loss: 161.4821\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.9037 - val_loss: 158.7603\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.1373 - val_loss: 157.5292\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.9198 - val_loss: 156.4784\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.7756 - val_loss: 155.4309\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.5432 - val_loss: 154.0954\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.0502 - val_loss: 153.9184\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.6874 - val_loss: 152.0012\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.9080 - val_loss: 152.9832\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.3430 - val_loss: 150.8601\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.4935 - val_loss: 151.3158\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.1635 - val_loss: 148.9342\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.0536 - val_loss: 149.1986\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.5129 - val_loss: 150.7246\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.0365 - val_loss: 145.7865\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.2320 - val_loss: 150.8500\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.4759 - val_loss: 152.3135\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.2919 - val_loss: 143.2170\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.0685 - val_loss: 142.9168\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.7287 - val_loss: 141.8310\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.7446 - val_loss: 141.7730\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.4147 - val_loss: 145.5573\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.0009 - val_loss: 140.1954\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.2426 - val_loss: 143.7834\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.8988 - val_loss: 146.8971\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.7653 - val_loss: 140.9100\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.5993 - val_loss: 138.8742\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.9677 - val_loss: 140.1178\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.3922 - val_loss: 141.6385\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.4439 - val_loss: 138.0349\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.6485 - val_loss: 137.0243\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.1600 - val_loss: 137.4023\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.0428 - val_loss: 135.8215\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.5620 - val_loss: 135.2842\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.0979 - val_loss: 136.4774\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.1564 - val_loss: 136.3509\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.7356 - val_loss: 134.3690\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.3160 - val_loss: 134.6275\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.1714 - val_loss: 133.8681\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.0048 - val_loss: 137.8775\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.8221 - val_loss: 134.3901\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.6824 - val_loss: 133.1991\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  28 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 93912.0547 - val_loss: 56426.1289\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 39547.8828 - val_loss: 27184.7168\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 20642.5762 - val_loss: 15282.1455\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 11646.8008 - val_loss: 8570.1084\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 6214.1987 - val_loss: 4395.5698\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2977.9629 - val_loss: 2176.9053\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1788.9027 - val_loss: 1722.5005\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1459.7224 - val_loss: 1453.4143\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1215.7041 - val_loss: 1257.1816\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1044.9911 - val_loss: 1109.7285\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 911.4648 - val_loss: 986.3504\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 806.6019 - val_loss: 887.8353\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 721.4881 - val_loss: 809.3919\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 656.5500 - val_loss: 743.3998\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 595.7090 - val_loss: 696.9001\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 553.5131 - val_loss: 648.9963\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 515.9052 - val_loss: 614.0958\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 483.5055 - val_loss: 580.3112\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 453.8788 - val_loss: 556.3104\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 425.8488 - val_loss: 519.3901\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 398.8130 - val_loss: 495.7734\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 372.4713 - val_loss: 470.6637\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 353.7781 - val_loss: 436.9628\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 332.3190 - val_loss: 420.1194\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 313.3228 - val_loss: 397.7285\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 295.5923 - val_loss: 371.4363\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 278.5986 - val_loss: 360.2734\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 266.4918 - val_loss: 336.5583\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 251.5121 - val_loss: 328.2121\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.4144 - val_loss: 305.5527\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 233.8766 - val_loss: 316.7601\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.0073 - val_loss: 282.2438\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 219.4994 - val_loss: 275.9196\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 212.7927 - val_loss: 281.7693\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 205.7652 - val_loss: 266.3939\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.0416 - val_loss: 251.3520\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.9962 - val_loss: 250.8800\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.0897 - val_loss: 255.8024\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.7924 - val_loss: 232.4421\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 183.2862 - val_loss: 243.5622\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 183.6280 - val_loss: 238.0209\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.7899 - val_loss: 217.1063\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.5141 - val_loss: 231.9295\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 170.6756 - val_loss: 206.8889\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.7686 - val_loss: 215.8244\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.0243 - val_loss: 199.0163\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.2852 - val_loss: 193.5300\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.5571 - val_loss: 190.2524\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.6651 - val_loss: 199.1723\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.6692 - val_loss: 183.0189\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.3397 - val_loss: 177.1673\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.6435 - val_loss: 186.2632\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.6609 - val_loss: 172.8490\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.5050 - val_loss: 166.0217\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.8327 - val_loss: 165.2321\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.2057 - val_loss: 167.3955\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.9890 - val_loss: 162.1573\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.0022 - val_loss: 160.4464\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.8080 - val_loss: 152.8770\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.5884 - val_loss: 148.7432\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.8297 - val_loss: 146.7344\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.1669 - val_loss: 144.2840\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.1840 - val_loss: 148.7816\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.5749 - val_loss: 139.9002\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.1414 - val_loss: 137.4828\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.5625 - val_loss: 135.1996\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.6472 - val_loss: 138.4673\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.0555 - val_loss: 133.9189\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.6387 - val_loss: 134.9431\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.9270 - val_loss: 126.7049\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.0238 - val_loss: 127.6168\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.0816 - val_loss: 128.1612\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.0962 - val_loss: 125.0161\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.3460 - val_loss: 119.1811\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.9793 - val_loss: 117.8347\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 104.9712 - val_loss: 117.9045\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.6458 - val_loss: 118.0929\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.0750 - val_loss: 115.2134\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.1031 - val_loss: 112.4548\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.1118 - val_loss: 140.0660\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.7542 - val_loss: 113.5476\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.0713 - val_loss: 108.5443\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.0332 - val_loss: 114.5938\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.5002 - val_loss: 125.2468\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.0702 - val_loss: 106.7826\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.6827 - val_loss: 104.6049\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.6959 - val_loss: 105.3118\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.8080 - val_loss: 104.0565\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.4043 - val_loss: 102.0670\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.1431 - val_loss: 100.8779\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.7856 - val_loss: 100.8858\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.9085 - val_loss: 107.1779\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.5624 - val_loss: 104.5390\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.9874 - val_loss: 99.3143\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.8036 - val_loss: 98.1200\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.5760 - val_loss: 97.5035\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.6274 - val_loss: 109.0534\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.8481 - val_loss: 109.8137\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.4332 - val_loss: 103.6595\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.5769 - val_loss: 97.5335\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 555us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  29 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 21221.9004 - val_loss: 11132.4434\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5256.5435 - val_loss: 3266.8047\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2031.4127 - val_loss: 2166.0630\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1455.8276 - val_loss: 1453.4797\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1132.3447 - val_loss: 1209.3972\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 989.8905 - val_loss: 1055.5137\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 907.2921 - val_loss: 966.8629\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 849.9625 - val_loss: 899.1424\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 798.7885 - val_loss: 827.2084\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 755.4480 - val_loss: 777.9382\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 720.5515 - val_loss: 726.7188\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 684.4472 - val_loss: 702.9368\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 657.9167 - val_loss: 646.0792\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 628.1409 - val_loss: 636.6712\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 595.2586 - val_loss: 588.2620\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 572.4836 - val_loss: 565.3030\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 548.4624 - val_loss: 544.9205\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 529.7153 - val_loss: 522.5721\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 514.8167 - val_loss: 494.6432\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 486.9717 - val_loss: 480.0609\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 469.2267 - val_loss: 452.8807\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 452.3332 - val_loss: 439.8273\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 436.4014 - val_loss: 421.9716\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 420.4164 - val_loss: 400.1728\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 405.6482 - val_loss: 387.4331\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 390.6020 - val_loss: 373.5059\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 377.3709 - val_loss: 354.4279\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 365.1024 - val_loss: 340.6926\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 352.7626 - val_loss: 327.0429\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 340.5084 - val_loss: 315.7856\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 331.5861 - val_loss: 304.2854\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 322.8939 - val_loss: 296.5974\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 307.7226 - val_loss: 281.6552\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 297.3082 - val_loss: 269.6801\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 291.3852 - val_loss: 264.2207\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 279.9616 - val_loss: 256.0722\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 271.6486 - val_loss: 242.0171\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 261.8136 - val_loss: 238.1329\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 258.5334 - val_loss: 237.2200\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 248.3394 - val_loss: 217.9905\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.9938 - val_loss: 217.3663\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 232.1033 - val_loss: 206.0468\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 228.3366 - val_loss: 197.5769\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 220.9485 - val_loss: 195.1782\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 212.2779 - val_loss: 186.5420\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.4496 - val_loss: 191.5559\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.5614 - val_loss: 176.4693\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 197.5403 - val_loss: 178.1050\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.0735 - val_loss: 168.0172\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 188.9045 - val_loss: 164.8297\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.1090 - val_loss: 163.9881\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 182.7127 - val_loss: 166.4688\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 184.0945 - val_loss: 154.4982\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.1171 - val_loss: 151.7424\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.2913 - val_loss: 149.4208\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 169.0864 - val_loss: 150.6022\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 167.8611 - val_loss: 144.5586\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.9565 - val_loss: 142.9121\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.2130 - val_loss: 141.7905\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.4102 - val_loss: 138.0680\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.2818 - val_loss: 141.6193\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.0602 - val_loss: 134.3454\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.7151 - val_loss: 132.9078\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.3044 - val_loss: 137.5615\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.9043 - val_loss: 136.1116\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.7036 - val_loss: 134.4711\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.2072 - val_loss: 129.3458\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.8495 - val_loss: 125.2687\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.0927 - val_loss: 124.2931\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.2268 - val_loss: 125.6783\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.5863 - val_loss: 123.6068\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.6271 - val_loss: 121.1182\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.7682 - val_loss: 120.6923\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.2739 - val_loss: 118.2987\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.4415 - val_loss: 120.2502\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.9815 - val_loss: 117.5704\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.0115 - val_loss: 115.0642\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.2175 - val_loss: 113.6930\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.6273 - val_loss: 113.4919\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.1820 - val_loss: 118.7450\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.3724 - val_loss: 112.6115\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.0578 - val_loss: 111.3311\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.8564 - val_loss: 109.8108\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.0928 - val_loss: 110.4276\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.9031 - val_loss: 107.9139\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.8365 - val_loss: 107.1210\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.5643 - val_loss: 106.6605\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.5302 - val_loss: 112.7362\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.9219 - val_loss: 104.1455\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.2245 - val_loss: 104.1773\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.6743 - val_loss: 102.8865\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.5221 - val_loss: 101.6363\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.7175 - val_loss: 105.8237\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.7069 - val_loss: 102.6425\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.0205 - val_loss: 102.7329\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.9193 - val_loss: 103.7910\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.6133 - val_loss: 99.7965\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.5686 - val_loss: 101.8048\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.2096 - val_loss: 104.8488\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.9626 - val_loss: 99.1068\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 557us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  30 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7923.8911 - val_loss: 5533.1577\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4443.3511 - val_loss: 3788.0063\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3457.5969 - val_loss: 3198.7859\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2939.9636 - val_loss: 2758.9204\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2512.7051 - val_loss: 2394.8777\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2161.0759 - val_loss: 2094.3145\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1884.6104 - val_loss: 1838.1440\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1671.2473 - val_loss: 1641.1351\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1445.6682 - val_loss: 1429.2908\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1261.7091 - val_loss: 1255.3833\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1087.1857 - val_loss: 1119.0354\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 966.8495 - val_loss: 990.6270\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 843.9305 - val_loss: 897.0928\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 760.8492 - val_loss: 822.3583\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 688.4477 - val_loss: 738.8177\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 624.7021 - val_loss: 671.6700\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 573.6392 - val_loss: 621.5306\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 534.7127 - val_loss: 561.9504\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 492.4532 - val_loss: 553.1904\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 466.3965 - val_loss: 477.9693\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 427.9083 - val_loss: 454.4166\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 404.0359 - val_loss: 411.5443\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 378.2319 - val_loss: 388.4888\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 352.4435 - val_loss: 378.4941\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 340.0584 - val_loss: 336.5147\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 321.2130 - val_loss: 318.8524\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 304.1260 - val_loss: 298.7717\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 297.8822 - val_loss: 281.7729\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 280.2795 - val_loss: 266.9865\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 268.3661 - val_loss: 253.2450\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.9322 - val_loss: 242.1116\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 254.2774 - val_loss: 230.0116\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.1171 - val_loss: 220.4725\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.2639 - val_loss: 213.3792\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 229.0206 - val_loss: 202.6036\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 225.0409 - val_loss: 198.6460\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 216.6670 - val_loss: 188.9801\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 214.6746 - val_loss: 182.2659\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 205.7344 - val_loss: 177.0348\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 204.1921 - val_loss: 175.8749\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.1024 - val_loss: 166.4619\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.3821 - val_loss: 161.9980\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.5304 - val_loss: 158.0076\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 191.0594 - val_loss: 154.8181\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 182.9890 - val_loss: 151.2467\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 182.4234 - val_loss: 150.4359\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.7654 - val_loss: 167.5192\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.2936 - val_loss: 142.0126\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.0432 - val_loss: 140.2839\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 171.5305 - val_loss: 137.0431\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.9971 - val_loss: 134.7562\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.3738 - val_loss: 132.1939\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 166.6992 - val_loss: 145.7520\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.1401 - val_loss: 132.4135\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 166.1775 - val_loss: 133.7110\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.9648 - val_loss: 124.7204\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.0125 - val_loss: 124.0875\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.5015 - val_loss: 124.4363\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.2467 - val_loss: 122.5596\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.1955 - val_loss: 119.6464\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.3881 - val_loss: 120.9323\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.4375 - val_loss: 120.7304\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.6296 - val_loss: 126.1798\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.4743 - val_loss: 131.5604\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.0499 - val_loss: 135.8294\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.9686 - val_loss: 121.9605\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.3401 - val_loss: 116.9619\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.8732 - val_loss: 114.4925\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.7732 - val_loss: 115.0057\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.5446 - val_loss: 118.6462\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.0367 - val_loss: 111.5848\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.7250 - val_loss: 114.9648\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.8013 - val_loss: 109.3814\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.4006 - val_loss: 107.8902\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.2363 - val_loss: 110.9639\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.2339 - val_loss: 107.2536\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.2671 - val_loss: 109.6914\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.8121 - val_loss: 106.8524\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.0181 - val_loss: 105.2339\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.1402 - val_loss: 106.1953\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.3985 - val_loss: 104.6533\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.3167 - val_loss: 107.6490\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.0143 - val_loss: 107.3585\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.6172 - val_loss: 103.1712\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.7355 - val_loss: 103.5534\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.0764 - val_loss: 101.2862\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.0634 - val_loss: 100.5796\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.4025 - val_loss: 102.4602\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.5057 - val_loss: 101.9217\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.1632 - val_loss: 100.5346\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.5609 - val_loss: 99.7362\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.1651 - val_loss: 100.0403\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.4719 - val_loss: 100.8212\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.4480 - val_loss: 99.5378\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.5652 - val_loss: 103.8575\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.2005 - val_loss: 98.4166\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.7278 - val_loss: 98.3111\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.8585 - val_loss: 100.2021\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.5627 - val_loss: 108.3651\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.7771 - val_loss: 99.4010\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 668us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  31 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 19656.4238 - val_loss: 7025.7554\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3324.7153 - val_loss: 1033.8252\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 979.4494 - val_loss: 1013.8190\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 925.4741 - val_loss: 853.5452\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 770.2263 - val_loss: 719.6400\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 690.1245 - val_loss: 665.0871\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 636.7979 - val_loss: 620.6542\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 591.6036 - val_loss: 582.5128\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 558.0296 - val_loss: 553.9720\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 535.5472 - val_loss: 532.3282\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 517.4404 - val_loss: 512.5939\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 499.9392 - val_loss: 496.1830\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 480.7536 - val_loss: 479.5920\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 461.0973 - val_loss: 463.3570\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 443.2692 - val_loss: 445.7406\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 424.4530 - val_loss: 430.0935\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 406.8626 - val_loss: 414.7386\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 390.8145 - val_loss: 399.4892\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 375.2991 - val_loss: 385.5865\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 361.2682 - val_loss: 373.1110\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 349.0965 - val_loss: 362.8654\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 337.2918 - val_loss: 348.2730\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 326.0552 - val_loss: 340.9765\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 319.9318 - val_loss: 329.2803\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 307.0802 - val_loss: 320.1041\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 299.0217 - val_loss: 310.7831\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 289.0702 - val_loss: 300.2914\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 282.2379 - val_loss: 292.8018\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 273.4763 - val_loss: 282.9327\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 266.0341 - val_loss: 275.6297\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 257.8121 - val_loss: 269.0624\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 250.0644 - val_loss: 259.5830\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.4854 - val_loss: 253.7159\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.1547 - val_loss: 246.6741\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 230.8154 - val_loss: 240.3564\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 225.9020 - val_loss: 233.6094\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 220.6202 - val_loss: 228.0191\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 215.2262 - val_loss: 222.4506\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.5324 - val_loss: 218.2473\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 204.0150 - val_loss: 212.4556\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 200.0096 - val_loss: 207.2009\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 195.2789 - val_loss: 201.6730\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 190.8250 - val_loss: 197.2402\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 188.0083 - val_loss: 195.0432\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 183.3381 - val_loss: 188.3823\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.5347 - val_loss: 184.8829\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.1037 - val_loss: 182.5770\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 172.8075 - val_loss: 178.7428\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 171.0501 - val_loss: 177.1219\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.5929 - val_loss: 171.0673\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.1537 - val_loss: 170.3101\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.9756 - val_loss: 166.3002\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.5134 - val_loss: 162.1367\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.0835 - val_loss: 159.4684\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.8018 - val_loss: 157.2768\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.3764 - val_loss: 154.5536\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.8472 - val_loss: 153.5663\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.7861 - val_loss: 149.6961\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.5956 - val_loss: 148.5856\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.8224 - val_loss: 145.7588\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.0372 - val_loss: 144.8066\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.4561 - val_loss: 143.6230\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.4891 - val_loss: 139.6423\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 140.1709 - val_loss: 137.8563\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.8681 - val_loss: 134.3763\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.8963 - val_loss: 132.4881\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.0736 - val_loss: 129.3362\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.8395 - val_loss: 128.1171\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.6309 - val_loss: 126.3424\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.5410 - val_loss: 125.7404\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.5589 - val_loss: 121.9567\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.5996 - val_loss: 123.2528\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.1985 - val_loss: 119.2578\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.5529 - val_loss: 117.9705\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.8802 - val_loss: 117.7305\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.8105 - val_loss: 115.9563\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.8461 - val_loss: 112.3013\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.5555 - val_loss: 111.4806\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.9548 - val_loss: 114.6147\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.3440 - val_loss: 110.3502\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.1280 - val_loss: 109.2756\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.8110 - val_loss: 108.9927\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.4592 - val_loss: 110.2816\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.8623 - val_loss: 107.8392\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.2782 - val_loss: 107.6541\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.0513 - val_loss: 107.1744\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.2522 - val_loss: 106.2099\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.1754 - val_loss: 107.7247\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.3961 - val_loss: 105.3612\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.4164 - val_loss: 109.6957\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.7382 - val_loss: 105.6328\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.1398 - val_loss: 104.5708\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.5228 - val_loss: 106.2999\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.5731 - val_loss: 107.2965\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.8919 - val_loss: 104.9042\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.9780 - val_loss: 108.3825\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.6563 - val_loss: 104.3582\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.4588 - val_loss: 102.6981\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.0801 - val_loss: 102.7757\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.9312 - val_loss: 102.8422\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 666us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  32 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 114855.3438 - val_loss: 79362.5547\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 51914.7070 - val_loss: 34716.8906\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 22209.3945 - val_loss: 14987.7461\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 9594.4570 - val_loss: 7043.0024\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4522.2505 - val_loss: 3669.3979\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2423.0107 - val_loss: 2259.2627\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1605.9075 - val_loss: 1628.3496\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1250.4312 - val_loss: 1349.1891\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1101.2290 - val_loss: 1201.2959\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1010.4282 - val_loss: 1114.4106\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 950.5421 - val_loss: 1040.2343\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 896.3135 - val_loss: 975.8391\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 843.7987 - val_loss: 917.4581\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 795.3306 - val_loss: 863.6547\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 747.2973 - val_loss: 810.8535\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 702.1998 - val_loss: 765.1134\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 661.4973 - val_loss: 721.8195\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 624.7769 - val_loss: 683.0013\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 591.4866 - val_loss: 646.3757\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 561.3002 - val_loss: 612.9987\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 533.6252 - val_loss: 584.7972\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 510.6134 - val_loss: 559.4689\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 488.4879 - val_loss: 534.4951\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 469.7814 - val_loss: 512.1804\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 452.3520 - val_loss: 492.7726\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 436.5638 - val_loss: 475.4505\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 422.0799 - val_loss: 458.8945\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 408.4494 - val_loss: 444.7899\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 396.3580 - val_loss: 431.2054\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 385.2981 - val_loss: 417.7886\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 375.6392 - val_loss: 406.5841\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 367.4642 - val_loss: 398.1219\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 359.5112 - val_loss: 388.7366\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 352.9977 - val_loss: 380.9237\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 346.8294 - val_loss: 373.8367\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 341.4670 - val_loss: 367.9212\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 336.1283 - val_loss: 362.4809\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 332.7941 - val_loss: 356.4555\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 328.2759 - val_loss: 352.5513\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 324.8639 - val_loss: 348.1932\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 321.6273 - val_loss: 345.1342\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 319.2555 - val_loss: 340.7382\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 316.8183 - val_loss: 338.3347\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 313.5708 - val_loss: 335.1050\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 311.2570 - val_loss: 332.3494\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 309.0659 - val_loss: 330.2179\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 306.7229 - val_loss: 327.9630\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 305.2102 - val_loss: 325.3824\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 303.1365 - val_loss: 323.7026\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 301.5608 - val_loss: 321.1712\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 300.2830 - val_loss: 320.1596\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 297.9949 - val_loss: 317.8504\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 296.2449 - val_loss: 315.8554\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 294.5282 - val_loss: 314.1436\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 292.9171 - val_loss: 312.4076\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 291.2268 - val_loss: 310.6999\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 289.6087 - val_loss: 308.5584\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 288.1277 - val_loss: 306.7821\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 287.0238 - val_loss: 304.7730\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 285.3300 - val_loss: 303.2994\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 284.4410 - val_loss: 302.1480\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 282.6660 - val_loss: 300.1499\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 282.3506 - val_loss: 298.3253\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 280.3034 - val_loss: 297.7949\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 279.2219 - val_loss: 295.8872\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 277.8001 - val_loss: 294.8085\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 276.7670 - val_loss: 293.4179\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 275.6206 - val_loss: 292.5000\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 274.4289 - val_loss: 290.7848\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 273.3139 - val_loss: 289.6558\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 272.1994 - val_loss: 288.4373\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 271.7986 - val_loss: 287.2940\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 269.6986 - val_loss: 286.6869\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 269.0690 - val_loss: 285.5110\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 267.3287 - val_loss: 284.1941\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 266.2136 - val_loss: 283.1944\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 264.8935 - val_loss: 282.5477\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 263.2901 - val_loss: 281.0889\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 263.0452 - val_loss: 279.9683\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 260.8666 - val_loss: 279.8293\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 259.9652 - val_loss: 278.7733\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 258.6848 - val_loss: 277.1517\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 257.7873 - val_loss: 276.5923\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 256.5467 - val_loss: 275.3409\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.0540 - val_loss: 274.4828\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 253.7812 - val_loss: 273.2759\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 252.3938 - val_loss: 272.1487\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 251.0042 - val_loss: 271.1417\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 250.4911 - val_loss: 270.1676\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 248.3049 - val_loss: 269.1687\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 247.2543 - val_loss: 268.7975\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.3473 - val_loss: 266.9601\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.3125 - val_loss: 265.7252\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.0389 - val_loss: 265.3387\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.1425 - val_loss: 263.6952\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.9114 - val_loss: 262.1851\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.1989 - val_loss: 261.1193\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 237.4307 - val_loss: 260.5011\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.5337 - val_loss: 258.8797\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 233.9801 - val_loss: 257.8329\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 669us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  33 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1360.7833 - val_loss: 860.1808\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 698.3318 - val_loss: 412.3672\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 365.2731 - val_loss: 278.3630\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 276.9371 - val_loss: 231.9066\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 247.3752 - val_loss: 216.1199\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 232.7909 - val_loss: 206.4699\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 220.9880 - val_loss: 196.8641\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 209.3125 - val_loss: 190.7060\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 201.0367 - val_loss: 183.5404\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.7788 - val_loss: 183.4465\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 189.0961 - val_loss: 172.3852\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.3097 - val_loss: 167.4414\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.1627 - val_loss: 166.8908\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.9496 - val_loss: 157.9324\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.9581 - val_loss: 154.1889\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.8389 - val_loss: 150.3734\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.8453 - val_loss: 146.2192\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.4763 - val_loss: 144.7044\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.3594 - val_loss: 140.4131\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.4221 - val_loss: 137.2363\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.3348 - val_loss: 135.3056\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.8843 - val_loss: 131.6557\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.1136 - val_loss: 131.6814\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.2855 - val_loss: 128.5995\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.1904 - val_loss: 125.8695\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.0742 - val_loss: 122.3973\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.0666 - val_loss: 120.5005\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.6344 - val_loss: 121.0086\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.1073 - val_loss: 117.6253\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.2770 - val_loss: 116.9549\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.6429 - val_loss: 115.0936\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.1265 - val_loss: 114.1659\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.1663 - val_loss: 113.3700\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.0742 - val_loss: 111.2578\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.7466 - val_loss: 112.0864\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.7368 - val_loss: 112.5470\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.3536 - val_loss: 112.9940\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.5420 - val_loss: 107.7164\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.4052 - val_loss: 106.9308\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.1005 - val_loss: 111.4952\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.6194 - val_loss: 106.4338\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.4493 - val_loss: 106.1897\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.3627 - val_loss: 110.9099\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.3866 - val_loss: 103.8881\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.4965 - val_loss: 103.3895\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.1427 - val_loss: 104.2697\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.0691 - val_loss: 106.4921\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.5929 - val_loss: 101.7207\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.9596 - val_loss: 102.2199\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.2857 - val_loss: 100.7414\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.6582 - val_loss: 100.3938\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.2921 - val_loss: 101.7437\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.4721 - val_loss: 101.3177\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.9882 - val_loss: 99.4152\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.2341 - val_loss: 106.1528\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.3789 - val_loss: 98.6380\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.8618 - val_loss: 97.7721\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.3308 - val_loss: 98.8102\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.4144 - val_loss: 98.4390\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.5115 - val_loss: 98.4025\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.4097 - val_loss: 95.9245\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.8647 - val_loss: 96.2058\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.2995 - val_loss: 97.5517\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.2988 - val_loss: 95.5310\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.4043 - val_loss: 96.8207\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.0378 - val_loss: 93.8653\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.7813 - val_loss: 94.5076\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.4102 - val_loss: 93.1982\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.2710 - val_loss: 93.2927\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.7609 - val_loss: 96.2460\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.2138 - val_loss: 92.0140\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.7797 - val_loss: 94.1954\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.0025 - val_loss: 93.9458\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.7226 - val_loss: 92.7442\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.9243 - val_loss: 92.3692\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 83.4971 - val_loss: 90.6652\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.2811 - val_loss: 90.4119\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.8056 - val_loss: 90.0601\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.9157 - val_loss: 89.7132\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.8128 - val_loss: 89.1261\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.8968 - val_loss: 88.9574\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.9816 - val_loss: 99.2034\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.5520 - val_loss: 90.0339\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.6760 - val_loss: 88.1187\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.5267 - val_loss: 87.8327\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.0828 - val_loss: 88.4439\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.3171 - val_loss: 90.0625\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.0366 - val_loss: 87.7081\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.7842 - val_loss: 86.8522\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.7991 - val_loss: 88.0857\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.9337 - val_loss: 86.6994\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.9017 - val_loss: 87.6627\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.0654 - val_loss: 86.9779\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.6510 - val_loss: 86.9142\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.8574 - val_loss: 86.9916\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.3936 - val_loss: 86.6948\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 74.9101 - val_loss: 84.9842\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 74.2432 - val_loss: 87.3149\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.5547 - val_loss: 89.2758\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.4471 - val_loss: 84.3528\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 554us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  34 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 13790.9111 - val_loss: 4475.2188\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1997.5961 - val_loss: 975.2874\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 661.4453 - val_loss: 660.5682\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 518.5369 - val_loss: 540.7588\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 443.1916 - val_loss: 461.2119\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 398.1766 - val_loss: 415.9183\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 372.4120 - val_loss: 384.2200\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 353.9996 - val_loss: 362.6513\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 340.3000 - val_loss: 345.4518\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 331.3877 - val_loss: 332.0626\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 323.9128 - val_loss: 324.2310\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 318.7647 - val_loss: 317.3258\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 313.4567 - val_loss: 310.4065\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 309.5883 - val_loss: 307.2013\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 305.4002 - val_loss: 297.7217\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 300.4314 - val_loss: 297.4368\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 296.3091 - val_loss: 293.6558\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 292.5425 - val_loss: 288.8668\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 288.9688 - val_loss: 287.6824\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 286.0920 - val_loss: 286.2248\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 283.3864 - val_loss: 283.2893\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 280.9935 - val_loss: 282.5966\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 279.0843 - val_loss: 280.5758\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 278.0376 - val_loss: 278.0550\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 274.0093 - val_loss: 279.5189\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 271.8584 - val_loss: 276.3814\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 270.2082 - val_loss: 275.4946\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 267.9390 - val_loss: 272.9164\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 265.6731 - val_loss: 274.0266\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 263.6661 - val_loss: 270.4246\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.0528 - val_loss: 269.3479\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 259.8712 - val_loss: 266.8957\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 258.0822 - val_loss: 266.2823\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 256.7973 - val_loss: 264.8814\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 254.3856 - val_loss: 264.4314\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 253.5812 - val_loss: 261.3960\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 251.1232 - val_loss: 260.7864\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 250.3423 - val_loss: 258.6797\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 247.6373 - val_loss: 258.6454\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 246.9561 - val_loss: 257.2534\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.9426 - val_loss: 252.4945\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 242.5531 - val_loss: 254.7128\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 242.1596 - val_loss: 251.3983\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.5095 - val_loss: 250.5072\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.7486 - val_loss: 248.2836\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.6195 - val_loss: 247.7039\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3812 - val_loss: 243.5724\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 233.5487 - val_loss: 246.6412\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 231.2747 - val_loss: 241.6711\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 230.1556 - val_loss: 240.1707\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 228.4030 - val_loss: 240.7151\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 226.8647 - val_loss: 236.5794\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 225.3369 - val_loss: 236.0149\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 223.4310 - val_loss: 236.0328\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.3745 - val_loss: 232.3171\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 220.5433 - val_loss: 230.7837\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 219.5242 - val_loss: 227.0849\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 216.7708 - val_loss: 229.0110\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 215.0966 - val_loss: 225.8160\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 213.4438 - val_loss: 222.2335\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 211.4794 - val_loss: 221.4338\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 209.8598 - val_loss: 220.5124\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.6672 - val_loss: 217.4256\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 206.0558 - val_loss: 218.5511\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 205.0924 - val_loss: 214.9664\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.1720 - val_loss: 212.9248\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 201.5119 - val_loss: 211.6038\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 200.8710 - val_loss: 209.0834\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 198.8658 - val_loss: 209.1222\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 197.4520 - val_loss: 205.8053\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.8115 - val_loss: 205.6025\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.3715 - val_loss: 203.9295\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 191.8620 - val_loss: 200.6331\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 190.1624 - val_loss: 201.0122\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 188.8284 - val_loss: 198.0158\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 186.9975 - val_loss: 196.7876\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 184.9687 - val_loss: 194.6132\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.0851 - val_loss: 193.7649\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.6573 - val_loss: 191.2243\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.2828 - val_loss: 189.5959\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 178.9429 - val_loss: 189.8554\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 178.2593 - val_loss: 189.5237\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.0482 - val_loss: 184.4915\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.1573 - val_loss: 183.7270\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 172.4143 - val_loss: 181.4850\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 171.1465 - val_loss: 182.3732\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.4234 - val_loss: 178.8031\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 167.6715 - val_loss: 177.1038\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.5285 - val_loss: 174.8932\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.4853 - val_loss: 175.4268\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.3707 - val_loss: 172.7154\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.8942 - val_loss: 171.0742\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.5526 - val_loss: 168.8137\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.1041 - val_loss: 167.4963\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.0562 - val_loss: 166.9246\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.6309 - val_loss: 162.6041\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.9386 - val_loss: 165.1005\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.4455 - val_loss: 160.3975\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.0979 - val_loss: 159.6955\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.8274 - val_loss: 157.4377\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 554us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  35 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 9671.7441 - val_loss: 3422.9202\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1395.9044 - val_loss: 727.5500\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 636.3064 - val_loss: 790.8542\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 505.8288 - val_loss: 589.9366\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 407.3381 - val_loss: 520.1075\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 348.8522 - val_loss: 466.9576\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 310.8891 - val_loss: 421.3054\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 282.9171 - val_loss: 390.5323\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.3188 - val_loss: 364.8781\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 248.5385 - val_loss: 346.5735\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.4642 - val_loss: 328.7842\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 227.3350 - val_loss: 316.4357\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.9656 - val_loss: 301.7527\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 210.4079 - val_loss: 290.9272\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 204.0175 - val_loss: 280.9368\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.0256 - val_loss: 270.2749\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.0879 - val_loss: 264.3270\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.6526 - val_loss: 252.7271\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 183.6274 - val_loss: 246.0546\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.4697 - val_loss: 237.5096\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.2054 - val_loss: 231.7175\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 169.6272 - val_loss: 224.7670\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.8093 - val_loss: 220.9818\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.4324 - val_loss: 213.2878\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.0090 - val_loss: 208.8449\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.0452 - val_loss: 203.6077\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.7780 - val_loss: 201.2982\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.1707 - val_loss: 194.6230\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.0963 - val_loss: 191.8088\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.5492 - val_loss: 186.5061\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.9469 - val_loss: 183.4142\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.9200 - val_loss: 180.0851\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.7902 - val_loss: 176.3258\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.4194 - val_loss: 173.2337\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.6096 - val_loss: 170.2401\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.0260 - val_loss: 167.6923\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.6240 - val_loss: 165.0932\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.5990 - val_loss: 164.9516\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.8604 - val_loss: 159.3343\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.3844 - val_loss: 158.7873\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.4438 - val_loss: 155.8036\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.0937 - val_loss: 154.8651\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.5000 - val_loss: 151.8394\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.7066 - val_loss: 149.3778\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.0807 - val_loss: 147.5090\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.9740 - val_loss: 149.4220\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.5616 - val_loss: 145.7082\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.6274 - val_loss: 142.8566\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.0293 - val_loss: 144.6778\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.3644 - val_loss: 140.0621\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.1698 - val_loss: 140.1702\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.3569 - val_loss: 137.1384\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.7640 - val_loss: 136.2506\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 118.5221 - val_loss: 136.0969\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.7396 - val_loss: 136.0475\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.6277 - val_loss: 134.9219\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.4494 - val_loss: 133.1584\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.7860 - val_loss: 131.2769\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.7251 - val_loss: 131.4570\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.3534 - val_loss: 129.3811\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.8836 - val_loss: 131.0299\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.5437 - val_loss: 127.5841\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.2624 - val_loss: 128.4589\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.6831 - val_loss: 126.5814\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.9776 - val_loss: 125.8701\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.1105 - val_loss: 124.4769\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.0389 - val_loss: 124.9901\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.2266 - val_loss: 122.8670\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.3210 - val_loss: 128.7692\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.6537 - val_loss: 120.9321\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.0858 - val_loss: 120.9497\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.6526 - val_loss: 119.2284\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.0943 - val_loss: 123.3222\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.7787 - val_loss: 117.8055\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.0102 - val_loss: 117.5413\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.2724 - val_loss: 121.6381\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.4343 - val_loss: 115.9549\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.7968 - val_loss: 116.4223\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.5732 - val_loss: 116.5216\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.4075 - val_loss: 114.2412\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.5034 - val_loss: 114.8197\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.8733 - val_loss: 113.2214\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.4421 - val_loss: 113.4156\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.1547 - val_loss: 113.5328\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.0963 - val_loss: 118.8433\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.2098 - val_loss: 111.2374\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.0511 - val_loss: 111.9149\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.2844 - val_loss: 112.5175\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.5545 - val_loss: 114.1879\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.6880 - val_loss: 109.8836\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.7718 - val_loss: 109.3086\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.7155 - val_loss: 109.7266\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.8785 - val_loss: 109.7826\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.8379 - val_loss: 112.5764\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.9666 - val_loss: 111.8654\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.9178 - val_loss: 110.0097\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.2372 - val_loss: 107.0592\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.7992 - val_loss: 106.3398\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.0225 - val_loss: 106.1739\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.6999 - val_loss: 106.8460\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  36 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 104782.3125 - val_loss: 69341.5703\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 49496.0391 - val_loss: 28758.6934\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 19254.3809 - val_loss: 10264.1982\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 8212.3291 - val_loss: 5625.5259\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5813.9077 - val_loss: 4540.2031\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4774.7046 - val_loss: 3788.7009\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4085.3889 - val_loss: 3323.9834\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3548.5178 - val_loss: 2871.7041\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3077.6948 - val_loss: 2480.5117\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2663.3533 - val_loss: 2138.1953\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2294.0762 - val_loss: 1840.9377\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1974.7280 - val_loss: 1578.8759\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1703.8962 - val_loss: 1349.2947\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1459.0071 - val_loss: 1161.1429\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1257.2894 - val_loss: 995.5139\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1090.5475 - val_loss: 855.0206\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 950.4631 - val_loss: 736.6796\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 829.5237 - val_loss: 645.9987\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 729.2247 - val_loss: 563.8091\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 647.8846 - val_loss: 499.2804\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 581.9048 - val_loss: 447.3636\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 525.4184 - val_loss: 405.8057\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 483.2073 - val_loss: 370.5442\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 444.2685 - val_loss: 347.4984\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 415.5861 - val_loss: 320.1324\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 393.1152 - val_loss: 302.6608\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 370.1873 - val_loss: 287.7978\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 354.6534 - val_loss: 278.2990\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 339.0249 - val_loss: 267.3057\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 324.1399 - val_loss: 257.0866\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 312.8696 - val_loss: 247.6859\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 301.2451 - val_loss: 241.4757\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 291.9301 - val_loss: 235.8471\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 283.9879 - val_loss: 228.2085\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 275.5608 - val_loss: 224.4444\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 268.0873 - val_loss: 217.7804\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.8234 - val_loss: 214.3078\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.9129 - val_loss: 209.2338\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 251.2203 - val_loss: 206.9657\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.5527 - val_loss: 201.4047\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.1195 - val_loss: 199.3704\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 233.9604 - val_loss: 194.9995\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 228.7938 - val_loss: 193.1806\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 224.5126 - val_loss: 190.4187\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.4314 - val_loss: 187.4656\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 224.8815 - val_loss: 188.4142\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 213.5934 - val_loss: 182.2097\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 210.7997 - val_loss: 180.6724\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.1665 - val_loss: 179.2162\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 204.5719 - val_loss: 176.2942\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.3109 - val_loss: 176.8701\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 200.2294 - val_loss: 172.4276\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.7663 - val_loss: 171.1246\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.6467 - val_loss: 169.5593\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.0297 - val_loss: 168.1360\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 190.6315 - val_loss: 167.6194\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 188.0049 - val_loss: 164.6398\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 186.2227 - val_loss: 164.1148\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 186.6232 - val_loss: 163.4565\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 183.9109 - val_loss: 162.0560\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.9161 - val_loss: 159.7469\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 183.9169 - val_loss: 161.0832\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 178.7586 - val_loss: 157.6353\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 178.6943 - val_loss: 157.1085\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 175.6094 - val_loss: 155.9967\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.8479 - val_loss: 154.5996\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 172.1260 - val_loss: 153.3842\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 171.3150 - val_loss: 154.1129\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 169.8561 - val_loss: 150.8713\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 169.4416 - val_loss: 149.9365\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 167.5138 - val_loss: 150.6586\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.4215 - val_loss: 149.8772\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.7697 - val_loss: 149.6258\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.3441 - val_loss: 146.0651\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.6436 - val_loss: 147.7208\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.5140 - val_loss: 144.3557\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.8618 - val_loss: 143.4086\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.3154 - val_loss: 143.1062\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.4068 - val_loss: 141.5904\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.7053 - val_loss: 140.7114\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.8598 - val_loss: 140.2050\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.4727 - val_loss: 145.3638\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.3469 - val_loss: 141.6622\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 156.1861 - val_loss: 137.4323\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.7795 - val_loss: 138.1850\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.2434 - val_loss: 135.7638\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.9399 - val_loss: 136.4415\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.9558 - val_loss: 134.7884\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.0370 - val_loss: 134.9984\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.2178 - val_loss: 133.3828\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.8648 - val_loss: 132.8613\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.8961 - val_loss: 131.7372\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.1357 - val_loss: 130.8047\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.8534 - val_loss: 132.5275\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.9934 - val_loss: 129.8122\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.9541 - val_loss: 133.2428\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.6987 - val_loss: 128.6705\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.1734 - val_loss: 127.5940\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.2104 - val_loss: 126.9662\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.4550 - val_loss: 128.1631\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  37 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 73461.4453 - val_loss: 13285.2441\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5357.1904 - val_loss: 5296.1792\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4137.5859 - val_loss: 3702.9983\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2342.0264 - val_loss: 2425.8318\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2017.2560 - val_loss: 2227.3411\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1803.4004 - val_loss: 2114.0818\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1664.8684 - val_loss: 1945.5226\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1545.7271 - val_loss: 1790.2627\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1444.3301 - val_loss: 1688.9102\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1347.8013 - val_loss: 1584.9768\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1260.1665 - val_loss: 1474.5002\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1182.4146 - val_loss: 1397.1581\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1113.1244 - val_loss: 1311.5737\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1048.2625 - val_loss: 1234.5692\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 987.2347 - val_loss: 1170.7703\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 938.4168 - val_loss: 1111.4313\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 890.8088 - val_loss: 1047.0909\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 841.7566 - val_loss: 1001.3194\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 802.8231 - val_loss: 950.5038\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 765.3626 - val_loss: 905.5793\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 733.1798 - val_loss: 867.4102\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 704.5016 - val_loss: 831.4799\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 673.6909 - val_loss: 791.3845\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 648.7856 - val_loss: 766.9476\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 625.7273 - val_loss: 740.4496\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 604.1523 - val_loss: 708.3309\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 586.7693 - val_loss: 688.7719\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 567.1277 - val_loss: 661.8107\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 552.0033 - val_loss: 645.2771\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 536.5073 - val_loss: 625.4985\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 521.5327 - val_loss: 602.7338\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 511.9308 - val_loss: 588.0215\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 496.5564 - val_loss: 570.8247\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 486.2425 - val_loss: 561.4921\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 476.9800 - val_loss: 543.2576\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 467.5402 - val_loss: 532.0560\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 460.5984 - val_loss: 524.5009\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 451.6146 - val_loss: 508.4684\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 443.1995 - val_loss: 498.3561\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 435.7641 - val_loss: 488.2330\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 429.7148 - val_loss: 483.1992\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 427.6421 - val_loss: 470.6399\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 425.1764 - val_loss: 461.8638\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 412.9816 - val_loss: 454.6793\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 405.4244 - val_loss: 448.1279\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 399.1395 - val_loss: 439.7355\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 394.2360 - val_loss: 435.4273\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 391.5767 - val_loss: 426.8344\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 387.6585 - val_loss: 419.9059\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 382.8245 - val_loss: 419.1161\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 377.1462 - val_loss: 408.5778\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 374.7413 - val_loss: 402.7594\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 369.6415 - val_loss: 397.9952\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 366.9433 - val_loss: 396.8087\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 360.9477 - val_loss: 387.7151\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 356.8478 - val_loss: 384.1617\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 352.6885 - val_loss: 379.5714\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 356.6335 - val_loss: 379.2538\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 346.1150 - val_loss: 369.4377\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 345.0887 - val_loss: 373.5861\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 344.8694 - val_loss: 361.7604\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 337.7556 - val_loss: 356.7483\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 333.3070 - val_loss: 352.8039\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 328.1320 - val_loss: 349.5404\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 324.6998 - val_loss: 345.6874\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 323.8294 - val_loss: 340.9219\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 322.0148 - val_loss: 337.9766\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 318.3200 - val_loss: 337.8624\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 314.3756 - val_loss: 331.3365\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 309.4471 - val_loss: 327.2348\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 305.7435 - val_loss: 323.8692\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 303.3594 - val_loss: 326.0067\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 299.7382 - val_loss: 316.9792\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 299.6360 - val_loss: 331.9508\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 306.2499 - val_loss: 317.9022\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 299.4711 - val_loss: 313.0374\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 291.9708 - val_loss: 305.5784\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 286.7398 - val_loss: 302.8711\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 285.6801 - val_loss: 298.6063\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 287.8845 - val_loss: 295.9474\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 281.6656 - val_loss: 294.0754\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 280.9382 - val_loss: 301.4295\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 278.6714 - val_loss: 287.3526\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 270.5364 - val_loss: 283.7360\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 266.6942 - val_loss: 282.0736\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 264.2816 - val_loss: 277.5764\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 265.6612 - val_loss: 277.0420\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 258.9899 - val_loss: 273.4016\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 257.4796 - val_loss: 272.9476\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 258.8359 - val_loss: 267.8724\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 250.3955 - val_loss: 265.5573\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 249.3185 - val_loss: 265.7419\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 247.0686 - val_loss: 259.2075\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 246.3967 - val_loss: 256.6973\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 242.8331 - val_loss: 256.8200\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.3186 - val_loss: 252.3367\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 237.5166 - val_loss: 249.6480\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.3906 - val_loss: 249.5872\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 232.7385 - val_loss: 245.3048\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 233.8080 - val_loss: 243.0047\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  38 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 176934.4844 - val_loss: 133520.2969\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101989.6328 - val_loss: 75126.4453\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 55073.6602 - val_loss: 38009.5586\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 26059.3398 - val_loss: 16421.6133\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 9192.8584 - val_loss: 3526.2847\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1356.9561 - val_loss: 267.7198\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 284.3758 - val_loss: 263.2185\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 290.6227 - val_loss: 214.5761\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.3466 - val_loss: 220.8576\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 233.9091 - val_loss: 216.1382\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 229.0501 - val_loss: 207.8811\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 224.6070 - val_loss: 205.4883\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 220.1751 - val_loss: 202.8335\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 215.8582 - val_loss: 196.6232\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 211.0745 - val_loss: 195.0675\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 206.8836 - val_loss: 192.0563\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.7816 - val_loss: 187.5283\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.1922 - val_loss: 186.0795\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.9991 - val_loss: 182.6182\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 191.1631 - val_loss: 178.6609\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.6032 - val_loss: 176.2546\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 184.1515 - val_loss: 174.3026\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.7481 - val_loss: 171.0877\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.5410 - val_loss: 168.0637\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.4357 - val_loss: 167.3351\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 172.0628 - val_loss: 162.7376\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.3169 - val_loss: 163.4739\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 166.2774 - val_loss: 160.8216\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.4657 - val_loss: 155.3692\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.5616 - val_loss: 157.7014\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.4105 - val_loss: 154.6561\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.7308 - val_loss: 150.5677\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 153.5480 - val_loss: 150.4605\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.2538 - val_loss: 148.8481\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.6431 - val_loss: 145.3740\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.7196 - val_loss: 147.2134\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.1355 - val_loss: 143.5594\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.5823 - val_loss: 142.6123\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.1350 - val_loss: 140.6452\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.9058 - val_loss: 139.9306\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.9756 - val_loss: 137.1903\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.5721 - val_loss: 139.0733\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.8639 - val_loss: 136.0172\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.7931 - val_loss: 135.2903\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.7001 - val_loss: 135.8858\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.5444 - val_loss: 133.8134\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.0061 - val_loss: 131.4288\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.9192 - val_loss: 132.8400\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.7268 - val_loss: 130.7383\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.6472 - val_loss: 128.6032\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.6899 - val_loss: 128.9964\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.6686 - val_loss: 128.6495\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.9482 - val_loss: 125.5712\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.1177 - val_loss: 126.6102\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.5944 - val_loss: 125.8039\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.4937 - val_loss: 123.2019\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.1396 - val_loss: 125.1995\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.3791 - val_loss: 124.1959\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.0110 - val_loss: 121.2579\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.8143 - val_loss: 123.2906\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.9643 - val_loss: 121.4350\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.6412 - val_loss: 121.1879\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.9514 - val_loss: 120.4419\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.2291 - val_loss: 119.1472\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.0897 - val_loss: 118.0415\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.3529 - val_loss: 117.7195\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.4069 - val_loss: 116.4666\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.0958 - val_loss: 117.0261\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.4950 - val_loss: 114.2518\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.8037 - val_loss: 114.8490\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.4170 - val_loss: 114.8391\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.1073 - val_loss: 113.0088\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.5522 - val_loss: 111.6979\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.7157 - val_loss: 112.3567\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.2545 - val_loss: 112.0493\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.7226 - val_loss: 110.0890\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.8928 - val_loss: 110.0701\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.2688 - val_loss: 110.2267\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.7260 - val_loss: 111.0642\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.7650 - val_loss: 109.2742\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.6929 - val_loss: 107.8746\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.1050 - val_loss: 108.8964\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.8049 - val_loss: 109.0235\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.1874 - val_loss: 108.0982\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.7941 - val_loss: 107.2256\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.5060 - val_loss: 108.2707\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.2135 - val_loss: 107.6422\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.0719 - val_loss: 107.9620\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.7977 - val_loss: 106.1246\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.1785 - val_loss: 105.7176\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.7956 - val_loss: 106.5840\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.3986 - val_loss: 104.3910\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.5193 - val_loss: 105.2792\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.4930 - val_loss: 101.2686\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.4519 - val_loss: 106.8692\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.4973 - val_loss: 100.5536\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.5350 - val_loss: 99.8618\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.6673 - val_loss: 99.8240\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.3798 - val_loss: 102.5488\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.5296 - val_loss: 98.3917\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  39 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 74796.6016 - val_loss: 45255.7188\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 32700.9043 - val_loss: 17300.5273\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 12077.6914 - val_loss: 6008.0566\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4364.4565 - val_loss: 2980.9219\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2570.8494 - val_loss: 2551.4934\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2235.0032 - val_loss: 2348.5613\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1968.9984 - val_loss: 1945.9452\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1636.0265 - val_loss: 1586.9258\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1373.9578 - val_loss: 1341.0643\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1145.5510 - val_loss: 1140.9850\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 971.5504 - val_loss: 974.0217\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 825.7946 - val_loss: 832.1583\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 704.0179 - val_loss: 716.5075\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 604.3316 - val_loss: 616.7200\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 521.5448 - val_loss: 534.2028\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 453.5468 - val_loss: 464.7422\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 398.1055 - val_loss: 408.5804\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 354.3507 - val_loss: 361.2036\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 316.4115 - val_loss: 323.3002\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 288.7678 - val_loss: 289.0735\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 260.0996 - val_loss: 262.3723\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.5877 - val_loss: 241.0098\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.2869 - val_loss: 222.6924\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.9949 - val_loss: 207.7437\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.5184 - val_loss: 195.4489\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 186.3112 - val_loss: 184.8488\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.8863 - val_loss: 177.6046\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 171.9734 - val_loss: 168.7259\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.9983 - val_loss: 162.7558\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.2623 - val_loss: 157.7325\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.3375 - val_loss: 153.0075\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.4286 - val_loss: 149.1595\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.1699 - val_loss: 145.5911\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.2898 - val_loss: 142.6429\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.1125 - val_loss: 142.5208\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.2567 - val_loss: 138.7004\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.7596 - val_loss: 137.2354\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.0138 - val_loss: 135.8730\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.4579 - val_loss: 133.5131\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.1200 - val_loss: 132.3465\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.4618 - val_loss: 131.0302\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.3227 - val_loss: 130.9566\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.5131 - val_loss: 129.3047\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.6649 - val_loss: 128.4693\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.5935 - val_loss: 127.6324\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.8461 - val_loss: 126.9592\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.2411 - val_loss: 128.0690\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.8874 - val_loss: 126.5174\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.2233 - val_loss: 125.9642\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.8842 - val_loss: 125.7106\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.0016 - val_loss: 124.1432\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.7495 - val_loss: 124.7007\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.7251 - val_loss: 127.0494\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.4773 - val_loss: 122.8069\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.1085 - val_loss: 126.1592\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.3881 - val_loss: 122.1007\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.9504 - val_loss: 123.5758\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.2723 - val_loss: 122.2896\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.9107 - val_loss: 121.0621\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.0596 - val_loss: 122.0749\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.2321 - val_loss: 120.6336\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.8699 - val_loss: 121.4898\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.0130 - val_loss: 120.2275\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.3928 - val_loss: 120.2077\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.9133 - val_loss: 120.1796\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.3344 - val_loss: 124.2034\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.3061 - val_loss: 118.9324\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.9590 - val_loss: 120.5981\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.4938 - val_loss: 117.8700\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.5780 - val_loss: 119.6755\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.9921 - val_loss: 117.5000\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.6908 - val_loss: 117.1847\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.3648 - val_loss: 116.6331\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.3523 - val_loss: 116.7161\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.0541 - val_loss: 115.9053\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.7173 - val_loss: 116.8266\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.3845 - val_loss: 115.2961\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.7367 - val_loss: 114.9738\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.0720 - val_loss: 115.1052\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.5767 - val_loss: 114.4583\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.5168 - val_loss: 117.1547\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.7836 - val_loss: 114.1370\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.2298 - val_loss: 114.7377\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.1263 - val_loss: 117.6868\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.5495 - val_loss: 119.4689\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.0531 - val_loss: 112.9752\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.0908 - val_loss: 119.8297\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.0595 - val_loss: 112.0810\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.2951 - val_loss: 116.9451\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.7261 - val_loss: 111.4867\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.7225 - val_loss: 111.8508\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.6698 - val_loss: 111.2130\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.5623 - val_loss: 111.3171\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.4230 - val_loss: 114.0117\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.5431 - val_loss: 109.6421\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.8135 - val_loss: 111.0081\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.0912 - val_loss: 109.4734\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.7785 - val_loss: 109.7176\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.8741 - val_loss: 108.0194\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.2566 - val_loss: 107.7834\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 668us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  40 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 485.6790 - val_loss: 377.6245\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 388.1869 - val_loss: 317.3120\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 344.7367 - val_loss: 290.4443\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 319.4933 - val_loss: 274.8559\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 302.8706 - val_loss: 265.0758\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 292.6208 - val_loss: 254.4276\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 281.0405 - val_loss: 248.1302\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 275.4317 - val_loss: 241.7065\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 266.8762 - val_loss: 238.4916\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 260.4997 - val_loss: 232.8414\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.1163 - val_loss: 227.6267\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 251.7279 - val_loss: 224.4356\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.9680 - val_loss: 221.6692\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.3767 - val_loss: 215.8960\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 234.3809 - val_loss: 211.6526\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 229.9987 - val_loss: 208.0423\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 226.4815 - val_loss: 203.7695\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 223.9064 - val_loss: 200.5779\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 218.6586 - val_loss: 199.7500\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 215.8430 - val_loss: 194.1293\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 207.4717 - val_loss: 195.1464\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.4129 - val_loss: 186.7198\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 200.9427 - val_loss: 185.6944\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.3299 - val_loss: 181.0761\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 190.1412 - val_loss: 181.7076\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.7170 - val_loss: 178.0506\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 184.3675 - val_loss: 174.2249\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.7530 - val_loss: 167.2884\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.7872 - val_loss: 169.8011\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.3747 - val_loss: 164.9369\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 171.6880 - val_loss: 157.4653\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.7939 - val_loss: 160.2592\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.5503 - val_loss: 156.7195\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.3587 - val_loss: 150.1812\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.0455 - val_loss: 147.3341\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.0487 - val_loss: 146.6894\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.3422 - val_loss: 143.4659\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.2960 - val_loss: 141.4576\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.0838 - val_loss: 139.0090\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.2449 - val_loss: 140.4846\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.1170 - val_loss: 131.1440\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.6381 - val_loss: 128.7139\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.3115 - val_loss: 124.5087\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.8486 - val_loss: 123.2753\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.5257 - val_loss: 119.6530\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.5638 - val_loss: 119.4706\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.3403 - val_loss: 115.2637\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.9183 - val_loss: 113.6280\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.8196 - val_loss: 110.9502\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.2447 - val_loss: 108.6563\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.4624 - val_loss: 105.8974\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.9097 - val_loss: 103.6838\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.7854 - val_loss: 113.9319\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.6941 - val_loss: 100.5629\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.0662 - val_loss: 97.9945\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.9228 - val_loss: 101.4107\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.2074 - val_loss: 94.9962\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.0181 - val_loss: 94.8105\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.3521 - val_loss: 99.9315\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.6893 - val_loss: 93.1842\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.4321 - val_loss: 95.6267\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.6711 - val_loss: 91.4797\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.2257 - val_loss: 90.2072\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.2725 - val_loss: 91.8336\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.9530 - val_loss: 88.1685\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.2977 - val_loss: 90.5797\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.6573 - val_loss: 91.3792\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.0657 - val_loss: 87.1137\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.1473 - val_loss: 87.5814\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.9147 - val_loss: 87.2344\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.1331 - val_loss: 87.4462\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.8467 - val_loss: 85.9384\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.3628 - val_loss: 85.6272\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.9726 - val_loss: 85.8861\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.6802 - val_loss: 84.7546\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.3944 - val_loss: 84.2129\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.7996 - val_loss: 84.8826\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.3416 - val_loss: 83.6775\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.3600 - val_loss: 83.0373\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.1678 - val_loss: 84.6469\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.8800 - val_loss: 85.7584\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.9140 - val_loss: 85.7042\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.3347 - val_loss: 81.7722\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 74.8717 - val_loss: 81.5294\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.5227 - val_loss: 81.2521\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 74.5560 - val_loss: 83.8310\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 72.6323 - val_loss: 82.5948\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 72.5276 - val_loss: 80.7271\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 71.4415 - val_loss: 80.8783\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 71.2626 - val_loss: 80.7221\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 73.7374 - val_loss: 82.1288\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 70.9857 - val_loss: 80.2065\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 70.1684 - val_loss: 79.9599\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 69.5278 - val_loss: 79.3520\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 70.1427 - val_loss: 80.0144\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 72.1364 - val_loss: 83.2549\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 69.7799 - val_loss: 79.6684\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 71.3494 - val_loss: 80.7436\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 70.2887 - val_loss: 78.6131\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 67.2109 - val_loss: 79.0819\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  41 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 288321.2188 - val_loss: 238417.6094\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203591.6406 - val_loss: 168541.5000\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143724.5469 - val_loss: 118365.3438\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99135.5469 - val_loss: 77810.6250\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 60702.5312 - val_loss: 41324.0742\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 29856.1172 - val_loss: 17746.6855\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12346.1143 - val_loss: 6682.6533\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5033.4575 - val_loss: 3096.7632\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3001.6914 - val_loss: 2422.2415\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2484.6240 - val_loss: 2142.4316\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2234.6138 - val_loss: 2006.3671\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2078.9980 - val_loss: 1883.7971\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1917.9531 - val_loss: 1760.0936\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1787.0040 - val_loss: 1652.3823\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1663.3348 - val_loss: 1553.2845\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1551.4083 - val_loss: 1464.7601\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1452.2150 - val_loss: 1382.4733\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1365.3018 - val_loss: 1308.1154\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1285.8832 - val_loss: 1242.6934\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1207.5239 - val_loss: 1181.4323\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1144.0095 - val_loss: 1126.5580\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1085.0177 - val_loss: 1082.5052\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1034.5591 - val_loss: 1032.2539\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 985.5207 - val_loss: 1001.0233\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 942.6670 - val_loss: 960.8641\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 902.1179 - val_loss: 928.9518\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 866.5544 - val_loss: 900.7408\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 832.6023 - val_loss: 862.2902\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 806.6517 - val_loss: 845.1847\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 777.9625 - val_loss: 811.6575\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 750.7710 - val_loss: 789.7885\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 727.6422 - val_loss: 767.9355\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 709.0732 - val_loss: 753.2616\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 685.0887 - val_loss: 724.1140\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 662.2838 - val_loss: 707.9031\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 645.1559 - val_loss: 692.8926\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 629.9370 - val_loss: 663.0665\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 607.4699 - val_loss: 658.6193\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 591.0057 - val_loss: 635.1786\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 576.6426 - val_loss: 616.2324\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 562.7971 - val_loss: 606.9094\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 544.8365 - val_loss: 593.4133\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 531.0601 - val_loss: 578.6927\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 517.2304 - val_loss: 559.9104\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 500.1963 - val_loss: 550.2052\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 487.9303 - val_loss: 532.2575\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 478.5956 - val_loss: 516.8311\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 462.2720 - val_loss: 510.2027\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 450.8420 - val_loss: 493.3431\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 438.8053 - val_loss: 484.4391\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 423.8426 - val_loss: 475.2542\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 413.7743 - val_loss: 457.2476\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 406.6844 - val_loss: 464.4473\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 392.9679 - val_loss: 437.8326\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 381.0413 - val_loss: 432.4047\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 371.1230 - val_loss: 416.1404\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 361.0233 - val_loss: 408.0832\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 350.3238 - val_loss: 394.3397\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 341.4283 - val_loss: 384.1613\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 332.5576 - val_loss: 381.4955\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 324.1759 - val_loss: 366.0361\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 317.1142 - val_loss: 364.9016\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 306.7990 - val_loss: 347.1966\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 296.4651 - val_loss: 343.9754\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 289.8837 - val_loss: 329.7230\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 286.5782 - val_loss: 325.1455\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 279.6977 - val_loss: 323.6890\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 273.3837 - val_loss: 308.2881\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.5223 - val_loss: 307.1380\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.5867 - val_loss: 293.9119\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 249.5407 - val_loss: 299.7217\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.4932 - val_loss: 279.8036\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.5753 - val_loss: 273.7484\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 233.1414 - val_loss: 272.2051\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 226.5682 - val_loss: 260.5298\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.6541 - val_loss: 256.1863\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.6180 - val_loss: 248.4505\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 213.9011 - val_loss: 247.0189\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 209.4316 - val_loss: 239.4913\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 206.5473 - val_loss: 240.3880\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 200.7774 - val_loss: 228.6719\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 195.8997 - val_loss: 224.2925\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.6812 - val_loss: 220.4313\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 188.5068 - val_loss: 216.5163\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 185.2803 - val_loss: 212.9857\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 181.8798 - val_loss: 207.5812\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.4140 - val_loss: 205.2444\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 175.5125 - val_loss: 199.6857\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 172.4903 - val_loss: 196.1794\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 169.0844 - val_loss: 198.5038\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 167.1230 - val_loss: 188.9846\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.8936 - val_loss: 185.3672\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.3577 - val_loss: 183.1105\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.8752 - val_loss: 179.1932\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.2999 - val_loss: 177.5083\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.6850 - val_loss: 173.7427\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.2477 - val_loss: 179.7252\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.6913 - val_loss: 174.2121\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.6530 - val_loss: 165.1673\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.1159 - val_loss: 165.1177\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 554us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  42 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 15907.3105 - val_loss: 9685.0938\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 6083.2891 - val_loss: 3383.2693\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2142.8975 - val_loss: 1342.2479\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1173.7177 - val_loss: 1072.2285\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 977.9860 - val_loss: 885.5638\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 811.9840 - val_loss: 770.0789\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 710.0972 - val_loss: 682.8475\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 639.0455 - val_loss: 605.7071\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 572.4601 - val_loss: 546.0394\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 519.9470 - val_loss: 492.0683\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 471.1242 - val_loss: 438.8100\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 424.9808 - val_loss: 393.1804\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 384.0326 - val_loss: 353.0034\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 348.1919 - val_loss: 312.1704\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 312.6269 - val_loss: 286.0600\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 284.0561 - val_loss: 261.8074\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.2654 - val_loss: 235.5296\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 223.3158 - val_loss: 204.9815\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 189.3092 - val_loss: 177.0959\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.6492 - val_loss: 156.6653\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.5312 - val_loss: 145.9518\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.1973 - val_loss: 137.4284\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.4890 - val_loss: 132.5501\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.3389 - val_loss: 129.7135\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.3334 - val_loss: 126.3492\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.0935 - val_loss: 124.0079\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.4204 - val_loss: 120.9046\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.2793 - val_loss: 120.5780\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.7197 - val_loss: 117.0156\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.9222 - val_loss: 116.4634\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.8896 - val_loss: 114.5594\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.3285 - val_loss: 113.0424\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.5110 - val_loss: 114.0107\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.8998 - val_loss: 109.7158\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.4626 - val_loss: 109.6997\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 105.8006 - val_loss: 107.1630\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.0713 - val_loss: 107.3643\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.4058 - val_loss: 105.1650\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.7563 - val_loss: 106.3372\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.5570 - val_loss: 102.7902\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.7671 - val_loss: 101.3588\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.8241 - val_loss: 100.4663\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.5501 - val_loss: 98.9502\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.9937 - val_loss: 101.6880\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.6236 - val_loss: 97.6128\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.2790 - val_loss: 96.2136\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.1804 - val_loss: 97.7115\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.0859 - val_loss: 94.6180\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.3353 - val_loss: 95.1839\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.1851 - val_loss: 95.6297\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.5656 - val_loss: 92.7617\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.8764 - val_loss: 94.7776\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.5900 - val_loss: 92.2293\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.7302 - val_loss: 92.1684\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.8990 - val_loss: 91.8645\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.1526 - val_loss: 91.1464\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.8310 - val_loss: 91.3387\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.0801 - val_loss: 91.6886\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.1456 - val_loss: 87.6861\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.6136 - val_loss: 86.9915\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.7769 - val_loss: 91.3012\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.6163 - val_loss: 85.7455\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.7182 - val_loss: 86.2157\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.6480 - val_loss: 84.8441\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.8062 - val_loss: 86.8210\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.1777 - val_loss: 86.5351\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.2481 - val_loss: 83.5523\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.1901 - val_loss: 83.1742\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.4849 - val_loss: 85.2326\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.6141 - val_loss: 82.3592\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.1136 - val_loss: 81.8656\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.5255 - val_loss: 82.7390\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.1381 - val_loss: 81.5855\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.8624 - val_loss: 82.9080\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.3437 - val_loss: 81.3008\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.1046 - val_loss: 81.5009\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.7198 - val_loss: 80.6524\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.2134 - val_loss: 80.5292\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.8217 - val_loss: 79.2013\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.0436 - val_loss: 79.3415\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.8799 - val_loss: 87.1089\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.0313 - val_loss: 79.5112\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.8734 - val_loss: 78.3784\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.2692 - val_loss: 80.7260\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.2084 - val_loss: 77.1010\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.3470 - val_loss: 77.3177\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.4661 - val_loss: 80.1190\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 74.6025 - val_loss: 76.8345\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 73.8694 - val_loss: 76.1208\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.4809 - val_loss: 78.3094\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.1258 - val_loss: 75.1137\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 72.6504 - val_loss: 78.4398\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 73.2821 - val_loss: 74.9050\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 73.2930 - val_loss: 74.6667\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 72.7089 - val_loss: 74.5352\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 73.6911 - val_loss: 75.2885\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 73.3050 - val_loss: 75.2707\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 70.9931 - val_loss: 74.2231\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 70.1049 - val_loss: 73.8736\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 72.2727 - val_loss: 74.0557\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 557us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  43 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2635.1057 - val_loss: 794.8823\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 650.6443 - val_loss: 719.5662\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 634.6634 - val_loss: 575.2431\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 546.4584 - val_loss: 514.5479\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 519.4304 - val_loss: 495.2111\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 492.9332 - val_loss: 473.6021\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 467.6444 - val_loss: 445.3789\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 447.5600 - val_loss: 420.6525\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 427.7485 - val_loss: 411.5329\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 411.4474 - val_loss: 387.6033\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 395.0075 - val_loss: 371.5215\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 381.8286 - val_loss: 365.1268\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 368.2859 - val_loss: 347.6625\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 357.5749 - val_loss: 342.0939\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 348.8671 - val_loss: 336.6519\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 339.3463 - val_loss: 322.2580\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 332.1230 - val_loss: 316.8199\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 325.7195 - val_loss: 315.0448\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 317.9363 - val_loss: 304.5638\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 315.7476 - val_loss: 313.3308\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 308.3103 - val_loss: 294.0786\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 302.7447 - val_loss: 296.6019\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 300.0150 - val_loss: 287.3686\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 295.5841 - val_loss: 284.3495\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 292.3998 - val_loss: 287.6387\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 286.1893 - val_loss: 277.0666\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 283.1992 - val_loss: 280.9905\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 279.5652 - val_loss: 271.2881\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 276.5235 - val_loss: 276.9365\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 273.1619 - val_loss: 268.0342\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 268.6241 - val_loss: 269.3701\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 266.0040 - val_loss: 263.8416\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 263.8290 - val_loss: 262.6397\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 261.8090 - val_loss: 257.4770\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.4676 - val_loss: 261.0047\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 254.6010 - val_loss: 255.4760\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 249.7601 - val_loss: 257.4822\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 251.6676 - val_loss: 248.6636\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.0444 - val_loss: 247.7990\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 242.6234 - val_loss: 244.6414\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.9811 - val_loss: 254.6323\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.7615 - val_loss: 242.1848\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 233.0152 - val_loss: 239.4079\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 230.0668 - val_loss: 243.7405\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 228.6743 - val_loss: 244.7904\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 226.6293 - val_loss: 231.8538\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 220.5302 - val_loss: 237.9207\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 220.1033 - val_loss: 237.9396\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 218.3040 - val_loss: 224.7985\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 215.0733 - val_loss: 248.5528\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 220.2802 - val_loss: 233.6999\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 212.2000 - val_loss: 236.2625\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 212.4632 - val_loss: 218.3533\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.1906 - val_loss: 215.0108\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.0660 - val_loss: 213.2903\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.4084 - val_loss: 211.0922\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 197.1746 - val_loss: 226.2622\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 197.4608 - val_loss: 210.9631\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 195.5005 - val_loss: 224.4713\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.6375 - val_loss: 208.2968\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 188.3288 - val_loss: 204.7201\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 189.5539 - val_loss: 201.8524\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 183.6019 - val_loss: 205.5700\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 183.6001 - val_loss: 197.5684\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.9421 - val_loss: 194.8499\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 178.9634 - val_loss: 197.4604\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 178.9498 - val_loss: 191.5347\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 175.5715 - val_loss: 191.3255\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.8547 - val_loss: 204.0979\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.7480 - val_loss: 200.0839\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 169.8079 - val_loss: 186.1749\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.5894 - val_loss: 184.2279\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.8347 - val_loss: 209.4910\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.0476 - val_loss: 188.3537\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 164.7057 - val_loss: 179.1013\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.6682 - val_loss: 177.8132\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.4085 - val_loss: 176.4511\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.7815 - val_loss: 174.8516\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.3447 - val_loss: 181.1436\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.2106 - val_loss: 174.4731\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.6959 - val_loss: 176.5870\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.8641 - val_loss: 169.9969\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.1467 - val_loss: 168.3864\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.1477 - val_loss: 167.4135\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.6668 - val_loss: 166.2320\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.3058 - val_loss: 164.2930\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.8116 - val_loss: 164.6296\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.2715 - val_loss: 162.9564\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.3386 - val_loss: 161.2343\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.7411 - val_loss: 166.9650\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.6622 - val_loss: 158.7413\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.9364 - val_loss: 164.6054\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.3077 - val_loss: 157.3964\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.4123 - val_loss: 157.2498\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.8321 - val_loss: 154.4446\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.3302 - val_loss: 153.7076\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 141.6222 - val_loss: 152.8942\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.0287 - val_loss: 153.5917\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.6328 - val_loss: 151.5891\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 138.0942 - val_loss: 150.8598\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 664us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  44 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 16458.6191 - val_loss: 10251.5391\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 7231.7080 - val_loss: 4067.9385\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2854.8921 - val_loss: 1334.2754\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 987.7949 - val_loss: 524.1628\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 519.2262 - val_loss: 473.5576\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 440.1336 - val_loss: 409.5844\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 390.6163 - val_loss: 364.7135\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 364.3395 - val_loss: 344.0933\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 345.8870 - val_loss: 336.6913\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 331.8444 - val_loss: 327.1320\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 321.8663 - val_loss: 317.6681\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 313.0389 - val_loss: 313.8921\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 305.7400 - val_loss: 308.2239\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 300.4295 - val_loss: 304.5125\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 296.1555 - val_loss: 301.8065\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 292.1285 - val_loss: 297.8099\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 289.7999 - val_loss: 297.8009\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 285.0988 - val_loss: 292.7714\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 282.9049 - val_loss: 290.4272\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 280.3688 - val_loss: 287.9728\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 276.9008 - val_loss: 287.5260\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 274.3326 - val_loss: 284.7339\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 271.8325 - val_loss: 284.0698\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 268.8701 - val_loss: 280.4958\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 266.2179 - val_loss: 276.6995\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 266.5639 - val_loss: 279.4379\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 261.9264 - val_loss: 271.8540\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 258.3506 - val_loss: 274.7266\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.1413 - val_loss: 268.4987\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 252.6048 - val_loss: 267.4084\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 249.6449 - val_loss: 266.2610\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 247.1219 - val_loss: 265.4568\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.2543 - val_loss: 260.3418\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.7880 - val_loss: 255.9842\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.7611 - val_loss: 259.4706\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 234.6953 - val_loss: 250.8116\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 231.5923 - val_loss: 250.0515\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 229.7007 - val_loss: 247.0980\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 225.9159 - val_loss: 244.2218\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.3941 - val_loss: 242.9108\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.9847 - val_loss: 236.7078\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 215.2943 - val_loss: 237.1130\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 211.6624 - val_loss: 231.5766\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 208.5364 - val_loss: 231.3035\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 206.0163 - val_loss: 227.9121\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.2696 - val_loss: 224.0925\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 197.9212 - val_loss: 220.0924\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 195.2986 - val_loss: 218.5704\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.0473 - val_loss: 214.3899\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.3505 - val_loss: 212.3813\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 184.5623 - val_loss: 212.1257\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.8178 - val_loss: 205.2520\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.5759 - val_loss: 203.9598\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.9099 - val_loss: 199.2734\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 171.5128 - val_loss: 197.5558\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 169.1205 - val_loss: 192.1287\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 167.6105 - val_loss: 195.3137\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.4203 - val_loss: 187.1778\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.1637 - val_loss: 185.4662\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.0040 - val_loss: 185.7353\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.9772 - val_loss: 179.9977\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 151.7375 - val_loss: 179.3309\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.7283 - val_loss: 177.1337\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.3371 - val_loss: 175.4686\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.3809 - val_loss: 171.3703\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.8053 - val_loss: 170.1869\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.1014 - val_loss: 168.1060\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.4723 - val_loss: 166.2554\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.7076 - val_loss: 164.9363\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.8319 - val_loss: 163.5116\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.2259 - val_loss: 161.2095\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.5680 - val_loss: 160.1457\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.1840 - val_loss: 158.5364\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.8650 - val_loss: 156.4897\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.0848 - val_loss: 158.9785\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.2170 - val_loss: 154.9688\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.2429 - val_loss: 152.7345\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.6960 - val_loss: 158.3640\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.1954 - val_loss: 150.7400\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.6297 - val_loss: 149.2500\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.0363 - val_loss: 150.1101\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.6331 - val_loss: 147.2408\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.6791 - val_loss: 147.1866\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.6875 - val_loss: 144.7635\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.1787 - val_loss: 145.0510\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.6828 - val_loss: 143.9303\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.4372 - val_loss: 142.6660\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.9509 - val_loss: 141.6435\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.4494 - val_loss: 143.2651\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.1456 - val_loss: 143.1651\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.0071 - val_loss: 139.4960\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.9154 - val_loss: 137.0823\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.3172 - val_loss: 135.9970\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.7544 - val_loss: 136.0922\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.7485 - val_loss: 136.6492\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.9830 - val_loss: 134.2568\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.1644 - val_loss: 133.8160\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.3801 - val_loss: 133.1654\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 115.6667 - val_loss: 137.5718\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.8125 - val_loss: 133.6042\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  45 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 787.0346 - val_loss: 453.1909\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 293.2661 - val_loss: 263.4233\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 215.7720 - val_loss: 232.6193\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.6894 - val_loss: 211.2640\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.2542 - val_loss: 191.9030\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.6102 - val_loss: 178.2450\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.4072 - val_loss: 182.2142\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.8226 - val_loss: 191.7032\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.6290 - val_loss: 158.4646\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.8647 - val_loss: 150.8283\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.7679 - val_loss: 157.2997\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.5372 - val_loss: 152.7516\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.8882 - val_loss: 142.5301\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.8385 - val_loss: 145.3356\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.9366 - val_loss: 154.1342\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.3340 - val_loss: 140.9906\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.6891 - val_loss: 134.3048\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.4345 - val_loss: 137.7118\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.2021 - val_loss: 130.9621\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.3672 - val_loss: 130.6964\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.3741 - val_loss: 131.0922\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.2224 - val_loss: 130.1289\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.3866 - val_loss: 137.7045\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.0126 - val_loss: 131.0909\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.7713 - val_loss: 128.0695\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.6478 - val_loss: 146.7660\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.4830 - val_loss: 129.1765\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.9535 - val_loss: 136.0540\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.5180 - val_loss: 127.4387\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.8022 - val_loss: 127.4218\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.8412 - val_loss: 130.5830\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.0062 - val_loss: 127.4618\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.0060 - val_loss: 135.7991\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.2902 - val_loss: 129.3365\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.7210 - val_loss: 128.0838\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.8300 - val_loss: 131.2985\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.4755 - val_loss: 129.8760\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.1817 - val_loss: 132.0246\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.9678 - val_loss: 131.8932\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.0770 - val_loss: 153.4696\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.8578 - val_loss: 129.6324\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.8224 - val_loss: 126.7638\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.9104 - val_loss: 158.8246\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.4362 - val_loss: 158.4704\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.9219 - val_loss: 127.6502\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.3848 - val_loss: 138.9322\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.5792 - val_loss: 126.3130\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.7074 - val_loss: 133.7984\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.3656 - val_loss: 127.2234\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.5289 - val_loss: 126.8769\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.9626 - val_loss: 127.4969\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.0310 - val_loss: 126.3329\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.1014 - val_loss: 126.3573\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.1674 - val_loss: 142.5512\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.2333 - val_loss: 129.1579\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.8068 - val_loss: 126.1450\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.3707 - val_loss: 128.0235\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.4156 - val_loss: 131.1309\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.4430 - val_loss: 127.2283\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.3395 - val_loss: 139.0537\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.9107 - val_loss: 137.9883\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.5541 - val_loss: 129.3984\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.0431 - val_loss: 127.0229\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.5423 - val_loss: 129.3670\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.3246 - val_loss: 137.1339\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.7656 - val_loss: 126.1259\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.9095 - val_loss: 127.3096\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.5642 - val_loss: 125.8527\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.7623 - val_loss: 130.6568\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.3434 - val_loss: 128.4740\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.6726 - val_loss: 127.0469\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.6929 - val_loss: 127.7105\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.8206 - val_loss: 134.7535\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.7645 - val_loss: 140.1329\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.0800 - val_loss: 126.8005\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.9730 - val_loss: 124.3172\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.7306 - val_loss: 127.2776\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.9182 - val_loss: 129.0795\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.3335 - val_loss: 135.1371\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.9889 - val_loss: 131.2543\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.9852 - val_loss: 126.7796\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.7772 - val_loss: 151.4739\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.0184 - val_loss: 126.1555\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.8513 - val_loss: 130.6741\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.4573 - val_loss: 131.1799\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.2940 - val_loss: 130.1400\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.9652 - val_loss: 123.8111\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.0056 - val_loss: 128.7207\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.8883 - val_loss: 122.3323\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.9282 - val_loss: 126.7096\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.5277 - val_loss: 137.6119\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.6559 - val_loss: 122.7101\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.4983 - val_loss: 115.3792\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.9694 - val_loss: 110.0318\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.1751 - val_loss: 111.1369\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.3646 - val_loss: 108.6772\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.5851 - val_loss: 107.9220\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.1065 - val_loss: 113.9194\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.7566 - val_loss: 110.4237\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.9551 - val_loss: 104.7708\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 661us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  46 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4708.4453 - val_loss: 954.6435\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 711.3218 - val_loss: 827.4677\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 688.9140 - val_loss: 595.9979\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 539.0421 - val_loss: 571.1554\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 505.5945 - val_loss: 521.3506\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 472.2283 - val_loss: 488.6160\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 446.2704 - val_loss: 460.2482\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 417.9096 - val_loss: 432.9601\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 394.1654 - val_loss: 408.7310\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 372.6270 - val_loss: 384.8185\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 355.7074 - val_loss: 366.0804\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 335.8614 - val_loss: 345.3579\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 320.3917 - val_loss: 328.4722\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 305.3714 - val_loss: 309.7762\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 293.1592 - val_loss: 295.0424\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 278.7542 - val_loss: 279.8740\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 268.0948 - val_loss: 267.5833\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 257.7580 - val_loss: 254.6237\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 247.7453 - val_loss: 244.5179\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.6464 - val_loss: 233.3289\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 230.9710 - val_loss: 225.0729\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 229.3660 - val_loss: 220.6538\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 217.1776 - val_loss: 207.2591\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 210.5312 - val_loss: 202.4937\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.9637 - val_loss: 192.9089\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 197.6911 - val_loss: 188.0478\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.0831 - val_loss: 181.0969\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.2791 - val_loss: 174.7139\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 183.0351 - val_loss: 169.5319\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.1259 - val_loss: 162.8837\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.2252 - val_loss: 160.2934\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.8147 - val_loss: 149.7906\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.3192 - val_loss: 147.6083\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.2554 - val_loss: 144.3804\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.5680 - val_loss: 137.3282\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.3779 - val_loss: 135.1558\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 148.8553 - val_loss: 132.3988\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.7961 - val_loss: 125.9424\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.9993 - val_loss: 125.6819\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.4758 - val_loss: 119.6697\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.4404 - val_loss: 120.0127\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.7011 - val_loss: 116.2120\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.3181 - val_loss: 119.4246\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.9588 - val_loss: 111.6430\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.1781 - val_loss: 109.4590\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.5596 - val_loss: 109.0619\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.0396 - val_loss: 105.4899\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.7406 - val_loss: 107.2088\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.0040 - val_loss: 103.9358\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.4089 - val_loss: 102.8075\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.9322 - val_loss: 106.7013\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.2915 - val_loss: 100.6353\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.7840 - val_loss: 98.1409\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.2467 - val_loss: 99.3141\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.7755 - val_loss: 98.5510\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.0041 - val_loss: 97.5903\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.4264 - val_loss: 96.9864\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.4537 - val_loss: 97.6876\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.1233 - val_loss: 95.9666\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.5994 - val_loss: 96.8763\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 109.9558 - val_loss: 102.1985\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.6696 - val_loss: 94.9952\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.5789 - val_loss: 90.8757\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.8693 - val_loss: 97.9274\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.1193 - val_loss: 94.4597\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.0614 - val_loss: 97.7295\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.8010 - val_loss: 90.8711\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.0753 - val_loss: 89.8001\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.6021 - val_loss: 90.9049\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.8584 - val_loss: 88.3718\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.7903 - val_loss: 88.5788\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.7274 - val_loss: 89.8051\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.6121 - val_loss: 85.9584\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.6814 - val_loss: 90.6236\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.6165 - val_loss: 87.6207\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.4547 - val_loss: 90.3754\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.7252 - val_loss: 90.0570\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.3540 - val_loss: 85.2833\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.4220 - val_loss: 88.5044\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.8399 - val_loss: 84.9904\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.8006 - val_loss: 85.9938\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.8058 - val_loss: 85.9361\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.2401 - val_loss: 87.8378\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.8020 - val_loss: 87.9237\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.5348 - val_loss: 87.1727\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.9891 - val_loss: 85.1270\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.3598 - val_loss: 84.7336\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.4743 - val_loss: 81.3260\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.1769 - val_loss: 84.8610\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.0274 - val_loss: 81.7701\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.7835 - val_loss: 91.2911\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.0534 - val_loss: 84.9396\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.8477 - val_loss: 81.9809\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.4498 - val_loss: 82.0214\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.6407 - val_loss: 91.2514\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.7006 - val_loss: 81.9944\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.2020 - val_loss: 86.4628\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 97.4294 - val_loss: 82.3712\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.9748 - val_loss: 79.6634\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.7387 - val_loss: 79.7660\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 776us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  47 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 43261.0430 - val_loss: 12245.7412\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 7408.0449 - val_loss: 4083.5171\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4731.3887 - val_loss: 3441.6953\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3609.5806 - val_loss: 2515.1648\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2845.6401 - val_loss: 2002.8921\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2177.3835 - val_loss: 1471.3829\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1593.2716 - val_loss: 1075.5176\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1175.3265 - val_loss: 801.9270\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 913.5650 - val_loss: 645.0059\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 748.6667 - val_loss: 551.8707\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 645.3019 - val_loss: 488.4469\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 580.2093 - val_loss: 446.8075\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 519.6623 - val_loss: 402.7422\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 478.2850 - val_loss: 378.0755\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 442.1994 - val_loss: 351.7362\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 414.5561 - val_loss: 331.9216\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 392.6213 - val_loss: 316.8346\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 375.0379 - val_loss: 297.8129\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 349.8951 - val_loss: 285.1182\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 333.1167 - val_loss: 270.4455\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 317.9729 - val_loss: 256.4118\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 304.5652 - val_loss: 246.2279\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 290.7408 - val_loss: 237.6969\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 278.8733 - val_loss: 228.7591\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 269.0570 - val_loss: 221.4706\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 258.0487 - val_loss: 216.1489\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 249.2780 - val_loss: 206.9961\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 242.6630 - val_loss: 202.3494\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 233.3892 - val_loss: 195.3952\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 225.5220 - val_loss: 191.3183\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 220.4134 - val_loss: 186.5011\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 213.1975 - val_loss: 181.5041\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 207.2405 - val_loss: 177.5224\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.9425 - val_loss: 173.4581\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 197.4661 - val_loss: 168.8676\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 193.0536 - val_loss: 166.5635\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 189.2472 - val_loss: 163.8459\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 184.3201 - val_loss: 159.8658\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 178.7188 - val_loss: 156.1149\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 174.1490 - val_loss: 155.2689\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 171.0313 - val_loss: 152.4082\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 167.0090 - val_loss: 149.5213\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.8811 - val_loss: 148.1444\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 160.9770 - val_loss: 145.7255\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.9198 - val_loss: 143.9844\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.3092 - val_loss: 141.7553\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.0962 - val_loss: 138.9449\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.8089 - val_loss: 138.6131\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.5084 - val_loss: 136.2121\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.5216 - val_loss: 134.8453\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.5871 - val_loss: 133.4131\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.1688 - val_loss: 131.4189\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.6564 - val_loss: 129.9014\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 136.2475 - val_loss: 129.2297\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.5201 - val_loss: 127.9110\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 133.3168 - val_loss: 127.4780\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.1136 - val_loss: 126.2834\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.6940 - val_loss: 125.2469\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 128.9813 - val_loss: 124.6925\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.4242 - val_loss: 122.9314\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.7267 - val_loss: 121.4306\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.4537 - val_loss: 120.7352\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.5232 - val_loss: 119.9183\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.3397 - val_loss: 119.8167\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.9526 - val_loss: 119.3325\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.9810 - val_loss: 118.9666\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.8019 - val_loss: 118.1383\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.1820 - val_loss: 116.7021\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.1123 - val_loss: 116.1740\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.3107 - val_loss: 116.2626\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.9697 - val_loss: 117.2311\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.6890 - val_loss: 116.6661\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 114.0467 - val_loss: 114.9527\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.9283 - val_loss: 114.3814\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 112.4017 - val_loss: 114.0849\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.4271 - val_loss: 114.4052\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.0744 - val_loss: 115.5977\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.6783 - val_loss: 113.2798\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.3305 - val_loss: 114.1203\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.8830 - val_loss: 113.0297\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.7175 - val_loss: 112.5004\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.9297 - val_loss: 112.9680\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.4943 - val_loss: 113.7396\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.6118 - val_loss: 111.7148\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.6593 - val_loss: 111.5025\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 106.8789 - val_loss: 111.9705\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.1619 - val_loss: 111.5899\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.1942 - val_loss: 111.6094\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.9012 - val_loss: 111.9280\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.7828 - val_loss: 112.4484\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.9546 - val_loss: 111.1262\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.2679 - val_loss: 110.1221\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.9191 - val_loss: 112.1459\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.4436 - val_loss: 109.9016\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.1417 - val_loss: 109.4519\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.0375 - val_loss: 108.9765\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.2128 - val_loss: 109.3511\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.3942 - val_loss: 109.8410\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.1902 - val_loss: 108.9055\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.7284 - val_loss: 109.3249\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 662us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  48 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 10407.7910 - val_loss: 2148.1970\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1018.5470 - val_loss: 985.4600\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 919.9913 - val_loss: 701.6982\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 642.6343 - val_loss: 637.9974\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 599.1439 - val_loss: 586.9896\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 556.7908 - val_loss: 555.6435\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 524.0026 - val_loss: 524.2979\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 491.4023 - val_loss: 493.5671\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 457.4246 - val_loss: 456.6905\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 405.9357 - val_loss: 378.6845\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 321.2917 - val_loss: 302.5385\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.6438 - val_loss: 258.4044\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 222.7183 - val_loss: 233.5397\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 200.5188 - val_loss: 219.8929\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 186.0132 - val_loss: 201.4567\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.4277 - val_loss: 187.3709\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 158.0156 - val_loss: 179.2858\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.7750 - val_loss: 172.7975\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.0161 - val_loss: 162.6810\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.1659 - val_loss: 154.7347\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.3044 - val_loss: 151.6247\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.1823 - val_loss: 148.5142\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.2930 - val_loss: 145.5709\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.3131 - val_loss: 143.0132\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.2867 - val_loss: 140.2333\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 114.2841 - val_loss: 138.3465\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 113.7278 - val_loss: 135.9708\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.6932 - val_loss: 134.6308\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.1220 - val_loss: 132.6327\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 108.1288 - val_loss: 130.9075\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.7113 - val_loss: 128.8663\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.6963 - val_loss: 127.2435\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 105.2473 - val_loss: 125.6038\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.3196 - val_loss: 124.0673\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 103.0793 - val_loss: 123.2634\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.6641 - val_loss: 122.4720\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 101.7474 - val_loss: 120.0634\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.4214 - val_loss: 119.4838\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 100.0707 - val_loss: 118.9523\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.8181 - val_loss: 117.4271\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.5474 - val_loss: 116.6344\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.3220 - val_loss: 115.0354\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.4086 - val_loss: 113.9375\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.6192 - val_loss: 112.7743\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.2756 - val_loss: 111.7504\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 96.3985 - val_loss: 110.9975\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 94.1305 - val_loss: 109.7237\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.4522 - val_loss: 109.1670\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.9044 - val_loss: 111.3084\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.7296 - val_loss: 107.6493\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.8241 - val_loss: 106.7913\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.2388 - val_loss: 105.9631\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.9071 - val_loss: 106.6231\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.4959 - val_loss: 104.6888\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.9340 - val_loss: 106.2659\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 91.1896 - val_loss: 103.5398\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.3255 - val_loss: 102.6486\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 89.9123 - val_loss: 101.9463\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.5918 - val_loss: 102.3046\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.7062 - val_loss: 100.9299\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.7551 - val_loss: 100.3081\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.0320 - val_loss: 100.2385\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.4101 - val_loss: 99.7562\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.6866 - val_loss: 100.4817\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.1423 - val_loss: 98.2439\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.5607 - val_loss: 97.7762\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.1536 - val_loss: 98.1630\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.0287 - val_loss: 98.0805\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.2428 - val_loss: 96.5585\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.9943 - val_loss: 95.8039\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.6526 - val_loss: 95.3474\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.6161 - val_loss: 98.9794\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.6623 - val_loss: 94.2131\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.4557 - val_loss: 93.8089\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.9681 - val_loss: 94.1603\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.2974 - val_loss: 92.9092\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.3824 - val_loss: 95.3285\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.0742 - val_loss: 94.7338\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.1060 - val_loss: 91.3837\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.4827 - val_loss: 93.1888\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.1046 - val_loss: 90.3461\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.4265 - val_loss: 93.4944\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.4082 - val_loss: 88.6863\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.7405 - val_loss: 88.5079\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.8764 - val_loss: 90.8905\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.6689 - val_loss: 87.3084\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.5070 - val_loss: 90.1799\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.2526 - val_loss: 86.3836\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.7685 - val_loss: 88.0541\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.6912 - val_loss: 86.3410\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.6906 - val_loss: 85.8491\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.3105 - val_loss: 92.5401\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.1500 - val_loss: 84.5193\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.0587 - val_loss: 85.4523\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 76.5586 - val_loss: 84.7498\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 77.3294 - val_loss: 83.4152\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.9196 - val_loss: 83.2151\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.2656 - val_loss: 85.5660\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 75.2121 - val_loss: 81.9820\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 74.0255 - val_loss: 82.2020\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 668us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  49 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 70231.6094 - val_loss: 41435.3008\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 26472.4316 - val_loss: 13813.7900\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 8272.1582 - val_loss: 3901.3989\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2353.6733 - val_loss: 1288.1250\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 951.3505 - val_loss: 765.8945\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 706.4329 - val_loss: 682.4454\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 663.8474 - val_loss: 656.4052\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 637.8661 - val_loss: 633.3381\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 614.6964 - val_loss: 611.0638\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 592.9095 - val_loss: 591.1746\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 574.1541 - val_loss: 572.4434\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 554.7656 - val_loss: 555.5114\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 537.2790 - val_loss: 539.2905\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 519.4027 - val_loss: 523.6626\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 502.9944 - val_loss: 508.9150\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 486.2410 - val_loss: 493.0346\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 468.8153 - val_loss: 479.0200\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 452.8914 - val_loss: 463.7795\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 436.6755 - val_loss: 449.6019\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 421.7008 - val_loss: 434.7680\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 406.5434 - val_loss: 419.5974\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 389.0925 - val_loss: 403.3498\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 370.3220 - val_loss: 379.9507\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 347.7533 - val_loss: 353.2136\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 323.9543 - val_loss: 332.4448\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 301.9603 - val_loss: 310.0347\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 271.1299 - val_loss: 288.8093\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.8746 - val_loss: 275.0942\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 228.4560 - val_loss: 259.7641\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 220.3972 - val_loss: 249.4754\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 212.6239 - val_loss: 242.8130\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 209.3632 - val_loss: 237.3335\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 204.9926 - val_loss: 233.1044\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 202.9211 - val_loss: 228.7876\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 199.9509 - val_loss: 226.8650\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.5419 - val_loss: 221.9572\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 194.9929 - val_loss: 218.2720\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.9542 - val_loss: 217.3255\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 189.4205 - val_loss: 212.0530\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 187.0741 - val_loss: 208.9254\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 184.7781 - val_loss: 206.4244\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 182.7103 - val_loss: 204.2734\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 180.4545 - val_loss: 200.3983\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 179.9876 - val_loss: 196.1672\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 176.4364 - val_loss: 195.0605\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.0014 - val_loss: 191.1748\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 171.6854 - val_loss: 189.0558\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.6005 - val_loss: 185.5751\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 165.8981 - val_loss: 183.8971\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.6428 - val_loss: 180.6689\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.8901 - val_loss: 177.0601\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.3899 - val_loss: 173.6988\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 155.2937 - val_loss: 170.9506\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 152.4718 - val_loss: 165.8524\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 149.9625 - val_loss: 162.6245\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.8205 - val_loss: 158.9745\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 145.9115 - val_loss: 154.9251\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 142.9331 - val_loss: 150.7207\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.0583 - val_loss: 148.9222\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 137.4826 - val_loss: 142.9564\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 132.7773 - val_loss: 138.9218\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 129.7528 - val_loss: 131.4528\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.7548 - val_loss: 125.0461\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.2361 - val_loss: 119.7815\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.1870 - val_loss: 113.2222\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 110.9043 - val_loss: 108.4754\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 107.5424 - val_loss: 103.2206\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 104.7697 - val_loss: 100.7374\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 102.3635 - val_loss: 98.6466\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 99.4750 - val_loss: 94.7385\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 97.0368 - val_loss: 97.2436\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 95.4247 - val_loss: 95.2528\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 93.9089 - val_loss: 91.0063\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.4325 - val_loss: 92.1771\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 90.2847 - val_loss: 89.3590\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 88.9275 - val_loss: 89.5064\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 87.8923 - val_loss: 90.9936\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 86.1821 - val_loss: 92.8438\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.9618 - val_loss: 86.7705\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 85.2710 - val_loss: 87.0217\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 84.2911 - val_loss: 87.2553\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 83.4752 - val_loss: 86.6152\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.8740 - val_loss: 89.4428\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.7176 - val_loss: 87.2063\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.1905 - val_loss: 88.6708\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.8289 - val_loss: 87.3537\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.8835 - val_loss: 88.0347\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.4818 - val_loss: 85.4574\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.4948 - val_loss: 86.9115\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.1794 - val_loss: 86.0324\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.7230 - val_loss: 86.9247\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.8929 - val_loss: 85.5431\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 81.0919 - val_loss: 91.9329\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 82.0654 - val_loss: 86.9666\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 80.1219 - val_loss: 85.8704\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.7519 - val_loss: 86.9354\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.8121 - val_loss: 85.3193\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.6496 - val_loss: 86.4193\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 79.4365 - val_loss: 86.3287\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.4688 - val_loss: 84.5744\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 665us/step\n",
      "\n",
      "\n",
      "\n",
      "Training Model #  50 \n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 88642.3438 - val_loss: 46024.4531\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 26417.9551 - val_loss: 13327.7256\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 8249.3975 - val_loss: 4824.2471\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4087.7573 - val_loss: 3627.2644\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3497.0742 - val_loss: 3174.6294\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3036.4551 - val_loss: 2756.8879\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2693.1528 - val_loss: 2464.8442\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2447.5808 - val_loss: 2245.5491\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2241.8003 - val_loss: 2059.8672\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2065.4824 - val_loss: 1888.9497\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1910.2511 - val_loss: 1741.4744\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1768.5500 - val_loss: 1612.2670\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1635.8994 - val_loss: 1493.6569\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1521.7020 - val_loss: 1377.7920\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1410.4495 - val_loss: 1274.2654\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1305.0411 - val_loss: 1180.5151\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1210.7142 - val_loss: 1091.7206\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1122.5793 - val_loss: 1010.3259\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1046.1606 - val_loss: 936.7383\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 970.3651 - val_loss: 870.1792\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 895.0456 - val_loss: 803.7200\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 838.2039 - val_loss: 748.7705\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 775.0663 - val_loss: 699.8828\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 722.4042 - val_loss: 648.6008\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 673.1373 - val_loss: 607.1844\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 628.7320 - val_loss: 568.8557\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 588.8172 - val_loss: 533.4895\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 551.6078 - val_loss: 499.6234\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 517.1743 - val_loss: 470.2677\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 486.1142 - val_loss: 444.2008\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 456.6259 - val_loss: 419.4257\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 433.1442 - val_loss: 397.8430\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 410.6583 - val_loss: 376.4766\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 387.4100 - val_loss: 360.2132\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 366.5509 - val_loss: 339.8134\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 345.0711 - val_loss: 324.3918\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 327.4891 - val_loss: 309.3695\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 311.4377 - val_loss: 295.2371\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 296.8559 - val_loss: 283.2583\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 282.6644 - val_loss: 270.5597\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 274.8343 - val_loss: 258.8535\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 258.1565 - val_loss: 248.2448\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 247.0525 - val_loss: 238.2957\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.2963 - val_loss: 229.8510\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 227.2004 - val_loss: 221.2142\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 218.6359 - val_loss: 214.0907\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 210.0020 - val_loss: 206.6211\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 203.2813 - val_loss: 199.3859\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 195.1175 - val_loss: 195.4098\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 192.2753 - val_loss: 186.8419\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 183.2917 - val_loss: 181.5914\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 177.9214 - val_loss: 176.3889\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.0429 - val_loss: 171.8158\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 167.7952 - val_loss: 168.3719\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 163.9130 - val_loss: 163.3869\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 162.4195 - val_loss: 160.1156\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 159.6782 - val_loss: 157.5646\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 154.6190 - val_loss: 153.2169\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 150.3751 - val_loss: 150.5054\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 147.3939 - val_loss: 147.9866\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 144.7706 - val_loss: 145.0449\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 143.3211 - val_loss: 145.3903\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.9929 - val_loss: 140.8162\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.9257 - val_loss: 138.5040\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 139.1444 - val_loss: 138.7056\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 135.3233 - val_loss: 135.5007\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 134.3220 - val_loss: 133.1632\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.7467 - val_loss: 132.7164\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.4965 - val_loss: 130.5994\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 131.3144 - val_loss: 133.0547\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 130.5215 - val_loss: 129.2425\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.9322 - val_loss: 127.1858\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 127.4562 - val_loss: 127.4833\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.1070 - val_loss: 125.2015\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 125.1990 - val_loss: 126.3200\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 124.4366 - val_loss: 124.1693\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.8640 - val_loss: 122.9490\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.7175 - val_loss: 124.3170\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.8267 - val_loss: 122.3366\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 123.9944 - val_loss: 121.6767\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 121.3546 - val_loss: 125.1309\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 126.8262 - val_loss: 123.4063\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.4365 - val_loss: 123.3730\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.5850 - val_loss: 120.0161\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.0962 - val_loss: 119.6151\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.2059 - val_loss: 119.9655\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.5175 - val_loss: 118.9630\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.2987 - val_loss: 122.4084\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 119.3259 - val_loss: 118.3573\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 120.4133 - val_loss: 119.0142\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 122.6277 - val_loss: 122.7289\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 118.9820 - val_loss: 117.9634\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.5069 - val_loss: 117.8740\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.7899 - val_loss: 118.1315\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.7693 - val_loss: 116.7074\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.9638 - val_loss: 116.5400\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 117.4393 - val_loss: 119.5299\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.9919 - val_loss: 117.4048\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 118.0429 - val_loss: 120.7984\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 116.0519 - val_loss: 115.7925\n",
      "\n",
      "\n",
      "10/10 [==============================] - 0s 668us/step\n",
      "\n",
      "\n",
      "Total Execution Time :  0:02:54.402513\n"
     ]
    }
   ],
   "source": [
    "# Create the for loop to split the data, create, compile & fit model, evaluate & nake predictions, caluclate mse and store\n",
    "# in list_of_mse\n",
    "\n",
    "start_time = datetime.now() # Starting time of the for loop execution\n",
    "\n",
    "for i in range(50) :\n",
    "    # Split the data into train and test set\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\n",
    "    \n",
    "    # Create and compile the regression model using the function regression_model as defined in TASK 1\n",
    "    model = regression_model()\n",
    "\n",
    "    # Fit the model on the train set\n",
    "    print('\\n\\n\\nTraining Model # ' , i+1 , '\\n\\n') # Print the Model Number that is being trained\n",
    "    model.fit(X_train, Y_train, validation_split=0.3, epochs=100)\n",
    "    print('\\n')\n",
    "    \n",
    "    # Make prediction on the test set\n",
    "    Y_predicted = model.predict(X_test)\n",
    "    \n",
    "    # Calculate the mean square error\n",
    "    mse = mean_squared_error(Y_test, Y_predicted)\n",
    "    \n",
    "    # Add the mse to the list_of_mse list\n",
    "    list_of_mse.append(mse)\n",
    "\n",
    "end_time = datetime.now() # Ending time of the for loop execution\n",
    "\n",
    "# Print time taken for fitting 50 models and calucating the Mean and Standard Deviation of MSE of 50 models\n",
    "print('\\n\\nTotal Execution Time : ' , format(end_time - start_time))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Mean of the MSE of 50 Models :  148.0049474659527\n",
      "Standard Deviation of MSE of 50 Models :  213.9519003781904\n"
     ]
    }
   ],
   "source": [
    "# Calculate the Mean of the MSE of 50 models\n",
    "mean_of_mse = stats.mean(list_of_mse)\n",
    "\n",
    "# Calculate the Standard Deviation of the MSE of 50 models\n",
    "std_of_mse = stats.stdev(list_of_mse)\n",
    "\n",
    "# Print the Mean and Standard Deviation of MSE of 50 models\n",
    "print('\\n\\nMean of the MSE of 50 Models : ' , str(mean_of_mse))\n",
    "print('Standard Deviation of MSE of 50 Models : ' , str(std_of_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = green> Comparision of Mean of MSE with Mean of MSE with PART C </font>\n",
    "<table style=\"width:30%\">\n",
    "  <tr>\n",
    "    <th>Mean of MSE of PART A</th>\n",
    "    <th>Mean of MSE of PART B</th>\n",
    "    <th>Mean of MSE of PART C</th>\n",
    "    <th>Mean of MSE of PART D</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>244.77</td>\n",
    "    <td>126.13</td>\n",
    "    <td>149.31</td>\n",
    "    <td>121.49</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "The table above compares the Mean of **MSE for PART A**, **Mean of MSE for PART B**, **Mean of MSE for PART C** and **Mean of MSE for PART D**. As can be seen, the value of Mean of MSE of PART D is marginally smaller than that of PART C and is the smallest value obtained. This shows that the effect of **normalizing the features** as well as **increasing the number of epochs by 2** yield the best results in terms of the performance of the regression model and helps it in finding the line of best fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = blue> END OF PART D</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = red> END OF NOTEBOOK </font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
